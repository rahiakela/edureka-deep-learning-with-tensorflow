{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-beginners_guide_to_autoencoders.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMIKkw2J6HLtJlmEPEv31eK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/edureka-deep-learning-with-tensorflow/blob/module-7-restricted-boltzmann-machine-and-autoencoders/1_beginners_guide_to_autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc9lIVs_1iVE",
        "colab_type": "text"
      },
      "source": [
        "# A Beginner’s Guide to Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDbO1ScS1vbK",
        "colab_type": "text"
      },
      "source": [
        "Artificial Intelligence encircles a wide range of technologies and techniques that enable computer systems to solve problems like Data Compression which is used in computer vision, computer networks, computer architecture, and many other fields. Autoencoders are unsupervised neural networks that use machine learning to do this compression for us. \n",
        "\n",
        "Let’s begin with the most fundamental and essential question, What are autoencoders?\n",
        "\n",
        "Reference: https://www.edureka.co/blog/autoencoders-tutorial/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3YO7szQ125-",
        "colab_type": "text"
      },
      "source": [
        "## What are Autoencoders?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okMOFmGH17eo",
        "colab_type": "text"
      },
      "source": [
        "An autoencoder neural network is an Unsupervised Machine learning algorithm that applies backpropagation, setting the target values to be equal to the inputs. Autoencoders are used to reduce the size of our inputs into a smaller representation. If anyone needs the original data, they can reconstruct it from the compressed data.\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/1_8ixTe1VHLsmKB3AquWdxpQ-528x177.png?raw=1' width='800'/>\n",
        "\n",
        "We have a similar machine learning algorithm ie. PCA which does the same task. So you might be thinking why do we need Autoencoders then? Let’s continue this Autoencoders Tutorial and find out the reason behind using Autoencoders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icvj5Ehv2q6N",
        "colab_type": "text"
      },
      "source": [
        "## Autoencoders Tutorial: Its Emergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCcp0KCJ2t1I",
        "colab_type": "text"
      },
      "source": [
        "Autoencoders are preferred over PCA because:\n",
        "\n",
        "* An autoencoder can learn non-linear transformations with a non-linear activation function and multiple layers.\n",
        "* It doesn’t have to learn dense layers. It can use convolutional layers to learn which is better for video, image and series data.\n",
        "* It is more efficient to learn several layers with an autoencoder rather than learn one huge transformation with PCA.\n",
        "* An autoencoder provides a representation of each layer as the output.\n",
        "* It can make use of pre-trained layers from another model to apply transfer learning to enhance the encoder/decoder.\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/pca-vs-346x300.png?raw=1' width='800'/>\n",
        "\n",
        "Now let’s have a look at a few Industrial Applications of Autoencoders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef-jEj9F3wKz",
        "colab_type": "text"
      },
      "source": [
        "## Applications of Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igm2VwtV3zUZ",
        "colab_type": "text"
      },
      "source": [
        "### Image Coloring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxT2aDtb317R",
        "colab_type": "text"
      },
      "source": [
        "Autoencoders are used for converting any black and white picture into a colored image. Depending on what is in the picture, it is possible to tell what the color should be.\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/4-6-528x176.png?raw=1' width='800'/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQRwEZJ-4Ncy",
        "colab_type": "text"
      },
      "source": [
        "### Feature variation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4kpcqJf4QjI",
        "colab_type": "text"
      },
      "source": [
        "It extracts only the required features of an image and generates the output by removing any noise or unnecessary interruption.\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/1_has2O8b3HAUqvcqqLrlBQA-528x193.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEcEm0pf4XJH",
        "colab_type": "text"
      },
      "source": [
        "### Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYJ4l-v94XXY",
        "colab_type": "text"
      },
      "source": [
        "The reconstructed image is the same as our input but with reduced dimensions. It helps in providing the similar image with a reduced pixel value.\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/raisr-info-width-2000-528x297.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw8FqQWO4xx8",
        "colab_type": "text"
      },
      "source": [
        "### Denoising Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmDmCkTi4yWV",
        "colab_type": "text"
      },
      "source": [
        "The input seen by the autoencoder is not the raw input but a stochastically corrupted version. A denoising autoencoder is thus trained to reconstruct the original input from the noisy version.\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/1_G0V4dz4RKTKGpebeoSWB0A-528x254.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWsE1wi45Rka",
        "colab_type": "text"
      },
      "source": [
        "### Watermark Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlyIlBtN5SM3",
        "colab_type": "text"
      },
      "source": [
        "It is also used for removing watermarks from images or to remove any object while filming a video or a movie.\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/f4as-1-528x120.jpg?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpwE6sXs5v2W",
        "colab_type": "text"
      },
      "source": [
        "Now that you have an idea of the different industrial applications of Autoencoders, let’s continue our Autoencoders Tutorial Blog and understand the complex architecture of Autoencoders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEiljVYQ5wmW",
        "colab_type": "text"
      },
      "source": [
        "## Architecture of Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbaBYy326tKs",
        "colab_type": "text"
      },
      "source": [
        "An Autoencoder consist of three layers:\n",
        "\n",
        "1. **Encoder**\n",
        "2. **Code**\n",
        "3. **Decoder**\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/1_44eDEuZBEsmG_TCAKRI3Kw@2x-1-528x300.png?raw=1' width='800'/>\n",
        "\n",
        "* **Encoder**: This part of the network compresses the input into a latent space representation. The encoder layer encodes the input image as a compressed representation in a reduced dimension. The compressed image is the distorted version of the original image.\n",
        "\n",
        "* **Code**: This part of the network represents the compressed input which is fed to the decoder.\n",
        "\n",
        "* **Decoder**: This layer decodes the encoded image back to the original dimension. The decoded image is a lossy reconstruction of the original image and it is reconstructed from the latent space representation.\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/Input-Output-layers-313x300.png?raw=1' width='800'/>\n",
        "\n",
        "The layer between the encoder and decoder, ie. the code is also known as Bottleneck. This is a well-designed approach to decide which aspects of observed data are relevant information and what aspects can be discarded. It does this by balancing two criteria :\n",
        "\n",
        "* Compactness of representation, measured as the compressibility.\n",
        "* It retains some behaviourally relevant variables from the input. \n",
        "\n",
        "Now that you have an idea of the architecture of an Autoencoder. Let’s continue our Autoencoders Tutorial and understand the different properties and the Hyperparameters involved while training Autoencoders.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL85qkpA7_Xe",
        "colab_type": "text"
      },
      "source": [
        "## Properties and Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gjK3Ry88Aeb",
        "colab_type": "text"
      },
      "source": [
        "### Properties of Autoencoders:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHybYJmH8IiI",
        "colab_type": "text"
      },
      "source": [
        "* **Data-specific**: Autoencoders are only able to compress data similar to what they have been trained on.\n",
        "* **Lossy**: The decompressed outputs will be degraded compared to the original inputs.\n",
        "* **Learned automatically from examples**: It is easy to train specialized instances of the algorithm that will perform well on a specific type of input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_KgaSim8bCP",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters of Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xcQAa8i8d1U",
        "colab_type": "text"
      },
      "source": [
        "There are 4 hyperparameters that we need to set before training an autoencoder:\n",
        "\n",
        "* **Code size**: It represents the number of nodes in the middle layer. Smaller size results in more compression.\n",
        "* **Number of layers**: The autoencoder can consist of as many layers as we want.\n",
        "* **Number of nodes per layer**: The number of nodes per layer decreases with each subsequent layer of the encoder, and increases back in the decoder. The decoder is symmetric to the encoder in terms of the layer structure.\n",
        "* **Loss function**: We either use mean squared error or binary cross-entropy. If the input values are in the range [0, 1] then we typically use cross-entropy, otherwise, we use the mean squared error.\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/hyperparameters-332x300.png?raw=1' width='800'/>\n",
        "\n",
        "Now that you know the properties and hyperparameters involved in the training of Autoencoders. Let’s move forward with our Autoencoders Tutorial and understand the different types of autoencoders and how they differ from each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azWwL81O9Whq",
        "colab_type": "text"
      },
      "source": [
        "## Types of Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoNod_Qo9Xo8",
        "colab_type": "text"
      },
      "source": [
        "### Convolution Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvjbDbmpRQal",
        "colab_type": "text"
      },
      "source": [
        "Autoencoders in their traditional formulation does not take into account the fact that a signal can be seen as a sum of other signals. Convolutional Autoencoders use the convolution operator to exploit this observation. They learn to encode the input in a set of simple signals and then try to reconstruct the input from them, modify the geometry or the reflectance of the image.\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/4-Figure1-1-528x192.png?raw=1' width='800'/>\n",
        "\n",
        "Use cases of CAE:\n",
        "\n",
        "* Image Reconstruction\n",
        "* Image Colorization\n",
        "* latent space clustering\n",
        "* generating higher resolution images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVjNsAT8SAKx",
        "colab_type": "text"
      },
      "source": [
        "### Sparse Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS9iroOTSUpR",
        "colab_type": "text"
      },
      "source": [
        "Sparse autoencoders offer us an alternative method for introducing an information bottleneck without requiring a reduction in the number of nodes at our hidden layers. Instead, we’ll construct our loss function such that we penalize activations within a layer.\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/Screen-Shot-2018-03-07-at-1.50.55-PM-423x300.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozuOZvFXTBmA",
        "colab_type": "text"
      },
      "source": [
        "### Deep Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ggV9YG2TD9z",
        "colab_type": "text"
      },
      "source": [
        "The extension of the simple Autoencoder is the Deep Autoencoder. The first layer of the Deep Autoencoder is used for first-order features in the raw input. The second layer is used for second-order features corresponding to patterns in the appearance of first-order features. Deeper layers of the Deep Autoencoder tend to learn even higher-order features.\n",
        "\n",
        "A deep autoencoder is composed of two, symmetrical deep-belief networks-\n",
        "\n",
        "1. First four or five shallow layers representing the encoding half of the net.\n",
        "2. The second set of four or five layers that make up the decoding half.\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/deep_autoencoder-444x300.png?raw=1' width='800'/>\n",
        "\n",
        "Use cases of Deep Autoencoders\n",
        "\n",
        "* Image Search\n",
        "* Data Compression\n",
        "* Topic Modeling & Information Retrieval (IR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX9NBV1UUWKa",
        "colab_type": "text"
      },
      "source": [
        "### Contractive Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mJZtQjzUXQv",
        "colab_type": "text"
      },
      "source": [
        "A contractive autoencoder is an unsupervised deep learning technique that helps a neural network encode unlabeled training data. This is accomplished by constructing a loss term which penalizes large derivatives of our hidden layer activations with respect to the input training examples, essentially penalizing instances where a small change in the input leads to a large change in the encoding space.\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/Screen-Shot-2018-03-10-at-12.25.43-PM-528x238.png?raw=1' width='800'/>\n",
        "\n",
        "Now that you have an idea of what Autoencoders is, it’s different types and it’s properties. Let’s move ahead with our Autoencoders Tutorial and understand a simple implementation of it using TensorFlow in Python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJt2PfNyWese",
        "colab_type": "text"
      },
      "source": [
        "## Data Compression using Autoencoders(Demo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMvaHMfcWgbq",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/Autoencoders-528x265.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY6HD9FpWusv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "38dceed5-79a0-4976-8a2a-939d98d04451"
      },
      "source": [
        "import numpy as np\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DCG4DYTfkwE",
        "colab_type": "text"
      },
      "source": [
        "### Declaration of Hidden Layers and Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP4pCXXjW-u-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "1a5358e9-fa1b-4503-be9a-fa6c0bb060f5"
      },
      "source": [
        "# this is the size of our encoded representations\n",
        "encoding_dim = 32 # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
        "\n",
        "# this is our input placeholder\n",
        "input_img = Input(shape=(784, ))\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
        "\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "# this model maps an input to its encoded representation\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "# create a placeholder for an encoded (32-dimensional) input\n",
        "encoded_input = Input(shape=(encoding_dim, ))\n",
        "\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "\n",
        "# create the decoder model\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "# configure our model to use a per-pixel binary crossentropy loss, and the Adadelta optimizer:\n",
        "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwQo5vBMfnyO",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the input data (MNIST Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDzr9sehfSMv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "d41c3d62-e16e-467a-8b0d-f67778e70ae2"
      },
      "source": [
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "# normalize all values between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "# flatten the 28x28 images into vectors of size 784.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jDvWbdphdVQ",
        "colab_type": "text"
      },
      "source": [
        "### Training Autoencoders for 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmx1wmGtgC6S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ff3fed1-e672-468b-f5e7-63e353518db7"
      },
      "source": [
        "autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3564 - val_loss: 0.2698\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.2616 - val_loss: 0.2500\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.2404 - val_loss: 0.2284\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.2209 - val_loss: 0.2112\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.2065 - val_loss: 0.1994\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1962 - val_loss: 0.1905\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1882 - val_loss: 0.1832\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1816 - val_loss: 0.1771\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1759 - val_loss: 0.1719\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1709 - val_loss: 0.1671\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1663 - val_loss: 0.1626\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1620 - val_loss: 0.1586\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1580 - val_loss: 0.1548\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1544 - val_loss: 0.1513\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1510 - val_loss: 0.1481\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1480 - val_loss: 0.1451\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1451 - val_loss: 0.1424\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1426 - val_loss: 0.1399\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1402 - val_loss: 0.1376\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1379 - val_loss: 0.1354\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1358 - val_loss: 0.1333\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1338 - val_loss: 0.1314\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1319 - val_loss: 0.1295\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1301 - val_loss: 0.1277\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1283 - val_loss: 0.1260\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1267 - val_loss: 0.1243\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1251 - val_loss: 0.1228\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1236 - val_loss: 0.1213\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1221 - val_loss: 0.1198\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1208 - val_loss: 0.1185\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1195 - val_loss: 0.1172\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1183 - val_loss: 0.1161\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1171 - val_loss: 0.1150\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1160 - val_loss: 0.1139\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1150 - val_loss: 0.1129\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1141 - val_loss: 0.1120\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1132 - val_loss: 0.1111\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1124 - val_loss: 0.1103\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1116 - val_loss: 0.1095\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1108 - val_loss: 0.1088\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1101 - val_loss: 0.1081\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1094 - val_loss: 0.1074\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1088 - val_loss: 0.1068\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1082 - val_loss: 0.1062\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1076 - val_loss: 0.1056\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1070 - val_loss: 0.1051\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1065 - val_loss: 0.1045\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1060 - val_loss: 0.1040\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1055 - val_loss: 0.1036\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1051 - val_loss: 0.1031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6c4ab8d4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFTlz-3Zh4wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode and decode some digits note that we take them from the *test* set\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HFFtzx-iOyf",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing the reconstructed inputs and the encoded representations using Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSEt_PWbiQGN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "82cbdd5b-3017-44c9-996b-2fbd8da7ed4a"
      },
      "source": [
        "n = 10 # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "  # display original\n",
        "  ax = plt.subplot(2, n, i + 1)\n",
        "  plt.imshow(x_test[i].reshape(28, 28))\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # display reconstruction\n",
        "  ax = plt.subplot(2, n, i + 1 + n)\n",
        "  plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAADnCAYAAACkCqtqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}