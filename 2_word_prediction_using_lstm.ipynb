{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-word-prediction-using-lstm.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOdtJWW5+RihGpmobkLYnMT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/edureka-deep-learning-with-tensorflow/blob/module-6-recurrent-neural-networks/2_word_prediction_using_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C55ZgTk6prvt",
        "colab_type": "text"
      },
      "source": [
        "# Word prediction using LSTM in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJoewcxipssL",
        "colab_type": "text"
      },
      "source": [
        "We implement an LSTM in TensorFlow to predict word from texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW7IcB10pvHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "outputId": "39bafc9c-208d-46ca-dad9-015e27598f48"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "import random\n",
        "import collections\n",
        "import time\n",
        "\n",
        "start_time = time.time()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8t4B3Imp5vO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def elapsed(sec):\n",
        "    if sec < 60:\n",
        "        return str(sec) + \" sec\"\n",
        "    elif sec < (60 * 60):\n",
        "        return str(sec / 60) + \" min\"\n",
        "    else:\n",
        "        return str(sec / (60 * 60)) + \" hr\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5ax-eeXp6ZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Target log path\n",
        "logs_path = '/tmp/tensorflow/rnn_words'\n",
        "writer = tf.summary.FileWriter(logs_path)\n",
        "\n",
        "# Text file containing words for training\n",
        "training_file = 'harry_porter.txt'\n",
        "\n",
        "# read data\n",
        "def reading_text(fname):\n",
        "    with open(fname) as f:\n",
        "        text = f.readlines()\n",
        "\n",
        "    print(text)\n",
        "\n",
        "    text = [x.strip() for x in text]\n",
        "    print(text)\n",
        "    text = [text[i].split() for i in range(len(text))]\n",
        "    print(text)\n",
        "    text = np.array(text)\n",
        "    print(text)\n",
        "    text = np.reshape(text, [-1, ])\n",
        "\n",
        "    return text\n",
        "\n",
        "training_data = reading_text(training_file)\n",
        "print(\"Training Data Loaded\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P_GU83p5Vd-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e0fc8941-2353-4a35-bcab-f02d24d496ce"
      },
      "source": [
        "training_data[:2]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Harry', 'had'], dtype='<U11')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU7GPvLVqDHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building the dictionary and  reverse dictionary\n",
        "def build_dataset_dictionary(words):\n",
        "    count = collections.Counter(words).most_common()\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return dictionary, reverse_dictionary\n",
        "\n",
        "\n",
        "dictionary, reverse_dictionary = build_dataset_dictionary(training_data)\n",
        "vocabulary_size = len(dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDF1zc2PqH9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 50000\n",
        "display_step = 1000\n",
        "n_input = 3\n",
        "\n",
        "# number of units in RNN cell\n",
        "n_hidden = 512\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
        "y = tf.placeholder(\"float\", [None, vocabulary_size])\n",
        "\n",
        "# RNN output node weights and biases\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([vocabulary_size]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnjoilf4qNlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RNN(x, weights, biases):\n",
        "    # reshape to [1, n_input]\n",
        "    x = tf.reshape(x, [-1, n_input])\n",
        "\n",
        "    # Generate a n_input-element sequence of inputs\n",
        "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
        "    x = tf.split(x, n_input, 1)\n",
        "\n",
        "    # 2-layer LSTM, each layer has n_hidden units.\n",
        "    # Average Accuracy= 95.20% at 50k iter\n",
        "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden), rnn.BasicLSTMCell(n_hidden)])\n",
        "\n",
        "    # 1-layer LSTM with n_hidden units but with lower accuracy.\n",
        "    # Average Accuracy= 90.60% 50k iter\n",
        "    # Uncomment line below to test but comment out the 2-layer rnn.MultiRNNCell above\n",
        "    # rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
        "\n",
        "    # generate prediction\n",
        "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "\n",
        "    # there are n_input outputs but\n",
        "    # we only want the last output\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "\n",
        "pred = RNN(x, weights, biases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjW3xjS1qXbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss and optimizer\n",
        "cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost_function)\n",
        "\n",
        "# Model evaluation\n",
        "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah5D6BY4qg55",
        "colab_type": "code",
        "outputId": "439a01b9-93e2-4640-b60c-40150afc951a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Launch the graph\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    step = 0\n",
        "    offset = random.randint(0, n_input + 1)\n",
        "    end_offset = n_input + 1\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "\n",
        "    writer.add_graph(session.graph)\n",
        "\n",
        "    while step < training_iters:\n",
        "        # Generate a minibatch. Add some randomness on selection process.\n",
        "        if offset > (len(training_data) - end_offset):\n",
        "            offset = random.randint(0, n_input + 1)\n",
        "\n",
        "        symbols_in_keys = [[dictionary[str(training_data[i])]] for i in range(offset, offset + n_input)]\n",
        "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
        "\n",
        "        symbols_out_onehot = np.zeros([vocabulary_size], dtype=float)\n",
        "        symbols_out_onehot[dictionary[str(training_data[offset + n_input])]] = 1.0\n",
        "        symbols_out_onehot = np.reshape(symbols_out_onehot, [1, -1])\n",
        "\n",
        "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost_function, pred], \\\n",
        "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
        "        loss_total += loss\n",
        "        acc_total += acc\n",
        "        if (step + 1) % display_step == 0:\n",
        "            print(\"Iter= \" + str(step + 1) + \", Average Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss_total / display_step) + \", Average Accuracy= \" + \\\n",
        "                  \"{:.2f}%\".format(100 * acc_total / display_step))\n",
        "            acc_total = 0\n",
        "            loss_total = 0\n",
        "            symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
        "            symbols_out = training_data[offset + n_input]\n",
        "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
        "            print(\"%s - [%s] vs [%s]\" % (symbols_in, symbols_out, symbols_out_pred))\n",
        "        step += 1\n",
        "        offset += (n_input + 1)\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
        "    print(\"Run on command line.\")\n",
        "    print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
        "    print(\"Point your web browser to: http://localhost:6006/\")\n",
        "    while True:\n",
        "        prompt = \"%s words: \" % n_input\n",
        "        sentence = input(prompt)\n",
        "        sentence = sentence.strip()\n",
        "        words = sentence.split(' ')\n",
        "        if len(words) != n_input:\n",
        "            continue\n",
        "        try:\n",
        "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
        "            for i in range(32):\n",
        "                keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
        "                onehot_pred = session.run(pred, feed_dict={x: keys})\n",
        "                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
        "                sentence = \"%s %s\" % (sentence, reverse_dictionary[onehot_pred_index])\n",
        "                symbols_in_keys = symbols_in_keys[1:]\n",
        "                symbols_in_keys.append(onehot_pred_index)\n",
        "            print(sentence)\n",
        "        except:\n",
        "            print(\"Word not in dictionary\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter= 1000, Average Loss= 3.868219, Average Accuracy= 9.50%\n",
            "['on', 'the', 'wall'] - [behind] vs [hung]\n",
            "Iter= 2000, Average Loss= 2.072295, Average Accuracy= 38.50%\n",
            "['every', 'pupil', 'Filch'] - [had] vs [of]\n",
            "Iter= 3000, Average Loss= 1.719230, Average Accuracy= 50.60%\n",
            "['Wooden', 'filing', 'cabinets'] - [stood] vs [the]\n",
            "Iter= 4000, Average Loss= 1.591745, Average Accuracy= 53.90%\n",
            "['most', 'students', 'avoided.'] - [The] vs [The]\n",
            "Iter= 5000, Average Loss= 1.389498, Average Accuracy= 59.80%\n",
            "['was', 'always', 'begging'] - [Dumbledore] vs [that]\n",
            "Iter= 6000, Average Loss= 1.127574, Average Accuracy= 69.50%\n",
            "['A', 'highly', 'polished'] - [collection] vs [collection]\n",
            "Iter= 7000, Average Loss= 1.045205, Average Accuracy= 71.20%\n",
            "['cabinets', 'stood', 'around'] - [the] vs [the]\n",
            "Iter= 8000, Average Loss= 0.978674, Average Accuracy= 73.30%\n",
            "['and', 'windowless,', 'lit'] - [by] vs [by]\n",
            "Iter= 9000, Average Loss= 0.802429, Average Accuracy= 75.80%\n",
            "['was', 'always', 'begging'] - [Dumbledore] vs [that]\n",
            "Iter= 10000, Average Loss= 0.935526, Average Accuracy= 72.60%\n",
            "['punished.', 'Fred', 'and'] - [George] vs [chains]\n",
            "Iter= 11000, Average Loss= 0.817018, Average Accuracy= 76.30%\n",
            "['see', 'that', 'they'] - [contained] vs [contained]\n",
            "Iter= 12000, Average Loss= 0.775770, Average Accuracy= 78.20%\n",
            "['low', 'ceiling.', 'A'] - [faint] vs [single]\n",
            "Iter= 13000, Average Loss= 0.792413, Average Accuracy= 78.30%\n",
            "['he', 'was', 'always'] - [begging] vs [begging]\n",
            "Iter= 14000, Average Loss= 0.908086, Average Accuracy= 75.10%\n",
            "['themselves.', 'A', 'highly'] - [polished] vs [contained]\n",
            "Iter= 15000, Average Loss= 0.777777, Average Accuracy= 77.80%\n",
            "['and', 'windowless,', 'lit'] - [by] vs [by]\n",
            "Iter= 16000, Average Loss= 0.917843, Average Accuracy= 75.00%\n",
            "[\"Filch's\", 'office', 'before;'] - [it] vs [it]\n",
            "Iter= 17000, Average Loss= 0.799780, Average Accuracy= 78.30%\n",
            "['A', 'highly', 'polished'] - [collection] vs [collection]\n",
            "Iter= 18000, Average Loss= 0.774898, Average Accuracy= 79.30%\n",
            "['Harry', 'could', 'see'] - [that] vs [that]\n",
            "Iter= 19000, Average Loss= 0.688590, Average Accuracy= 80.10%\n",
            "['a', 'single', 'oil'] - [lamp] vs [lamp]\n",
            "Iter= 20000, Average Loss= 0.670007, Average Accuracy= 81.50%\n",
            "['was', 'common', 'knowledge'] - [that] vs [that]\n",
            "Iter= 21000, Average Loss= 0.682000, Average Accuracy= 81.60%\n",
            "['had', 'an', 'entire'] - [drawer] vs [Fred]\n",
            "Iter= 22000, Average Loss= 0.689407, Average Accuracy= 80.90%\n",
            "['the', 'walls;', 'from'] - [their] vs [their]\n",
            "Iter= 23000, Average Loss= 0.625215, Average Accuracy= 82.80%\n",
            "['a', 'place', 'most'] - [students] vs [students]\n",
            "Iter= 24000, Average Loss= 0.637599, Average Accuracy= 82.10%\n",
            "['common', 'knowledge', 'that'] - [he] vs [he]\n",
            "Iter= 25000, Average Loss= 0.667986, Average Accuracy= 81.60%\n",
            "['collection', 'of', 'chains'] - [and] vs [and]\n",
            "Iter= 26000, Average Loss= 0.688373, Average Accuracy= 80.10%\n",
            "['from', 'their', 'labels,'] - [Harry] vs [Harry]\n",
            "Iter= 27000, Average Loss= 0.672023, Average Accuracy= 82.20%\n",
            "['fried', 'fish', 'lingered'] - [about] vs [about]\n",
            "Iter= 28000, Average Loss= 0.695057, Average Accuracy= 78.70%\n",
            "['ankles', 'from', 'the'] - [ceiling.] vs [ceiling.]\n",
            "Iter= 29000, Average Loss= 0.674916, Average Accuracy= 80.80%\n",
            "['behind', \"Filch's\", 'desk.'] - [It] vs [It]\n",
            "Iter= 30000, Average Loss= 0.532892, Average Accuracy= 84.40%\n",
            "['Harry', 'could', 'see'] - [that] vs [that]\n",
            "Iter= 31000, Average Loss= 0.678421, Average Accuracy= 80.20%\n",
            "['from', 'the', 'low'] - [ceiling.] vs [ceiling.]\n",
            "Iter= 32000, Average Loss= 0.627591, Average Accuracy= 82.40%\n",
            "['Harry', 'had', 'never'] - [been] vs [been]\n",
            "Iter= 33000, Average Loss= 0.659253, Average Accuracy= 81.10%\n",
            "['manacles', 'hung', 'on'] - [the] vs [to]\n",
            "Iter= 34000, Average Loss= 0.677886, Average Accuracy= 82.00%\n",
            "['every', 'pupil', 'Filch'] - [had] vs [of]\n",
            "Iter= 35000, Average Loss= 0.582714, Average Accuracy= 84.50%\n",
            "['fried', 'fish', 'lingered'] - [about] vs [about]\n",
            "Iter= 36000, Average Loss= 0.563358, Average Accuracy= 84.70%\n",
            "['never', 'been', 'inside'] - [Filch's] vs [Filch's]\n",
            "Iter= 37000, Average Loss= 0.638574, Average Accuracy= 82.20%\n",
            "['that', 'he', 'was'] - [always] vs [always]\n",
            "Iter= 38000, Average Loss= 0.541098, Average Accuracy= 84.50%\n",
            "['entire', 'drawer', 'to'] - [themselves.] vs [themselves.]\n",
            "Iter= 39000, Average Loss= 0.509374, Average Accuracy= 85.30%\n",
            "['a', 'single', 'oil'] - [lamp] vs [lamp]\n",
            "Iter= 40000, Average Loss= 0.644866, Average Accuracy= 81.70%\n",
            "['never', 'been', 'inside'] - [Filch's] vs [Filch's]\n",
            "Iter= 41000, Average Loss= 0.539746, Average Accuracy= 84.10%\n",
            "['on', 'the', 'wall'] - [behind] vs [behind]\n",
            "Iter= 42000, Average Loss= 0.612106, Average Accuracy= 82.50%\n",
            "['entire', 'drawer', 'to'] - [themselves.] vs [themselves.]\n",
            "Iter= 43000, Average Loss= 0.508508, Average Accuracy= 85.90%\n",
            "['the', 'low', 'ceiling.'] - [A] vs [A]\n",
            "Iter= 44000, Average Loss= 0.529992, Average Accuracy= 86.70%\n",
            "['office', 'before;', 'it'] - [was] vs [was]\n",
            "Iter= 45000, Average Loss= 0.553084, Average Accuracy= 82.60%\n",
            "['hung', 'on', 'the'] - [wall] vs [wall]\n",
            "Iter= 46000, Average Loss= 0.501022, Average Accuracy= 85.30%\n",
            "['cabinets', 'stood', 'around'] - [the] vs [the]\n",
            "Iter= 47000, Average Loss= 0.475755, Average Accuracy= 86.10%\n",
            "['Harry', 'had', 'never'] - [been] vs [been]\n",
            "Iter= 48000, Average Loss= 0.522736, Average Accuracy= 86.30%\n",
            "['collection', 'of', 'chains'] - [and] vs [and]\n",
            "Iter= 49000, Average Loss= 0.591047, Average Accuracy= 84.20%\n",
            "['see', 'that', 'they'] - [contained] vs [contained]\n",
            "Iter= 50000, Average Loss= 0.575456, Average Accuracy= 84.30%\n",
            "['lit', 'by', 'a'] - [single] vs [single]\n",
            "Optimization Finished!\n",
            "Elapsed time:  26.618329536914825 min\n",
            "Run on command line.\n",
            "\ttensorboard --logdir=/tmp/tensorflow/rnn_words\n",
            "Point your web browser to: http://localhost:6006/\n",
            "3 words: the they them\n",
            "Word not in dictionary\n",
            "3 words: look watch notice\n",
            "Word not in dictionary\n",
            "3 words: love\n",
            "3 words: love heart fear\n",
            "Word not in dictionary\n",
            "3 words: fear\n",
            "3 words: \n",
            "3 words: \n",
            "3 words: false\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0d181a7588d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s words: \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}