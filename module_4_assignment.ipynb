{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "module-4-assignment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMidAM+DAgh7CqxBkcULu+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/edureka-deep-learning-with-tensorflow/blob/module-4-master-deep-networks/module_4_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHRs6swoGkVa",
        "colab_type": "text"
      },
      "source": [
        "# Module 4 Assignment: Deep dive into Neural Networks with Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp1LPMtEGl4y",
        "colab_type": "text"
      },
      "source": [
        "**Analyse the information given in the following ‘Wines’ dataset and classify the wine based on their place of origin.**\n",
        "\n",
        "This data contains the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n",
        "\n",
        "Data variables:\n",
        "* Alcohol\n",
        "* Malic acid\n",
        "* Ash\n",
        "* Alkalinity of ash\n",
        "* Magnesium\n",
        "* Total phenols\n",
        "* Flavanoids\n",
        "* Nonflavanoid phenols\n",
        "* Proanthocyanins\n",
        "* Color intensity\n",
        "* Hue\n",
        "* OD280/OD315 of diluted wines\n",
        "* Proline\n",
        "\n",
        "The dataset for the ‘Wines’ looks like this:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/wine-dataset.png?raw=1' width='800'/>\n",
        "\n",
        "\n",
        "Dataset download link:\n",
        "https://www.edureka.co/medias/giu1bur0kw/download?media_file_id=254929809\n",
        "\n",
        "## Task\n",
        "\n",
        "1.   Create a model with minimum three hidden layers\n",
        "2.   Compare the accuracy of the model with the previous model built in module 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry2_svlGGsln",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-P5bxusGtvy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "7ea2f651-e560-4ad8-b54f-7d4bb8279a90"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RlkAzwpG6KC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "03e8a074-f7fa-450b-a3fc-fc9e731fde17"
      },
      "source": [
        "df = pd.read_csv('wine.csv')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Wine</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic.acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Acl</th>\n",
              "      <th>Mg</th>\n",
              "      <th>Phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid.phenols</th>\n",
              "      <th>Proanth</th>\n",
              "      <th>Color.int</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD</th>\n",
              "      <th>Proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Wine  Alcohol  Malic.acid   Ash  ...  Color.int   Hue    OD  Proline\n",
              "0     1    14.23        1.71  2.43  ...       5.64  1.04  3.92     1065\n",
              "1     1    13.20        1.78  2.14  ...       4.38  1.05  3.40     1050\n",
              "2     1    13.16        2.36  2.67  ...       5.68  1.03  3.17     1185\n",
              "3     1    14.37        1.95  2.50  ...       7.80  0.86  3.45     1480\n",
              "4     1    13.24        2.59  2.87  ...       4.32  1.04  2.93      735\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9qWO3AZHnfB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e52e5d81-6078-4d83-ae31-598988f88f06"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVY68a_xHpiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "63406db1-bfaf-4632-d960-ebac20e192da"
      },
      "source": [
        "# slice out featuers and labels from dataset\n",
        "X = df[df.columns[1:13]]\n",
        "y = df[df.columns[0]]\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(178, 12) (178,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGlI3BL6H-nt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "596c810d-2f23-4f53-9469-305ccf9bced9"
      },
      "source": [
        "# normize the feature column\n",
        "def feature_normalize(dataset):\n",
        "  mu = np.mean(dataset,axis=0)\n",
        "  sigma = np.std(dataset,axis=0)\n",
        "  return (dataset - mu)/sigma\n",
        "X = feature_normalize(X)\n",
        "X.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic.acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Acl</th>\n",
              "      <th>Mg</th>\n",
              "      <th>Phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid.phenols</th>\n",
              "      <th>Proanth</th>\n",
              "      <th>Color.int</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.518613</td>\n",
              "      <td>-0.562250</td>\n",
              "      <td>0.232053</td>\n",
              "      <td>-1.169593</td>\n",
              "      <td>1.913905</td>\n",
              "      <td>0.808997</td>\n",
              "      <td>1.034819</td>\n",
              "      <td>-0.659563</td>\n",
              "      <td>1.224884</td>\n",
              "      <td>0.251717</td>\n",
              "      <td>0.362177</td>\n",
              "      <td>1.847920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.246290</td>\n",
              "      <td>-0.499413</td>\n",
              "      <td>-0.827996</td>\n",
              "      <td>-2.490847</td>\n",
              "      <td>0.018145</td>\n",
              "      <td>0.568648</td>\n",
              "      <td>0.733629</td>\n",
              "      <td>-0.820719</td>\n",
              "      <td>-0.544721</td>\n",
              "      <td>-0.293321</td>\n",
              "      <td>0.406051</td>\n",
              "      <td>1.113449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.196879</td>\n",
              "      <td>0.021231</td>\n",
              "      <td>1.109334</td>\n",
              "      <td>-0.268738</td>\n",
              "      <td>0.088358</td>\n",
              "      <td>0.808997</td>\n",
              "      <td>1.215533</td>\n",
              "      <td>-0.498407</td>\n",
              "      <td>2.135968</td>\n",
              "      <td>0.269020</td>\n",
              "      <td>0.318304</td>\n",
              "      <td>0.788587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.691550</td>\n",
              "      <td>-0.346811</td>\n",
              "      <td>0.487926</td>\n",
              "      <td>-0.809251</td>\n",
              "      <td>0.930918</td>\n",
              "      <td>2.491446</td>\n",
              "      <td>1.466525</td>\n",
              "      <td>-0.981875</td>\n",
              "      <td>1.032155</td>\n",
              "      <td>1.186068</td>\n",
              "      <td>-0.427544</td>\n",
              "      <td>1.184071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.295700</td>\n",
              "      <td>0.227694</td>\n",
              "      <td>1.840403</td>\n",
              "      <td>0.451946</td>\n",
              "      <td>1.281985</td>\n",
              "      <td>0.808997</td>\n",
              "      <td>0.663351</td>\n",
              "      <td>0.226796</td>\n",
              "      <td>0.401404</td>\n",
              "      <td>-0.319276</td>\n",
              "      <td>0.362177</td>\n",
              "      <td>0.449601</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Alcohol  Malic.acid       Ash  ...  Color.int       Hue        OD\n",
              "0  1.518613   -0.562250  0.232053  ...   0.251717  0.362177  1.847920\n",
              "1  0.246290   -0.499413 -0.827996  ...  -0.293321  0.406051  1.113449\n",
              "2  0.196879    0.021231  1.109334  ...   0.269020  0.318304  0.788587\n",
              "3  1.691550   -0.346811  0.487926  ...   1.186068 -0.427544  1.184071\n",
              "4  0.295700    0.227694  1.840403  ...  -0.319276  0.362177  0.449601\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h902ujRBIJgd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "a3038c23-e3b7-4de3-a39b-a5e70cbc9137"
      },
      "source": [
        "# do one-hot encoding for label\n",
        "print(len(np.unique(y)))\n",
        "\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(y)\n",
        "print(integer_encoded)\n",
        "\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded[:10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWz3jzgMIirV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "3ecfdb9c-4041-4844-8586-178b699f0200"
      },
      "source": [
        "# Shuffle the dataset to mix up the rows.\n",
        "X, Y = shuffle(X, onehot_encoded, random_state=1)\n",
        "\n",
        "# Convert the dataset into train and test part\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.20, random_state=415)\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(142, 12)\n",
            "(142, 3)\n",
            "(36, 12)\n",
            "(36, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn1KpImrInva",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f9c8f987-befc-4310-be2f-a9c28c39004a"
      },
      "source": [
        "# Define the important parameters and variable to work with the tensors\n",
        "learning_rate = 0.3\n",
        "training_epochs = 100\n",
        "cost_history = np.empty(shape=[1], dtype=float)\n",
        "n_dim = X.shape[1]\n",
        "print(\"n_dim\", n_dim)\n",
        "n_class = 3\n",
        "\n",
        "# Define the number of hidden layers and number of neurons for each layer\n",
        "n_hidden_0 = 4\n",
        "n_hidden_1 = 4\n",
        "n_hidden_2 = 4\n",
        "n_hidden_3 = 4\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, n_dim])\n",
        "W = tf.Variable(tf.zeros([n_dim, n_class]))\n",
        "b = tf.Variable(tf.zeros([n_class]))\n",
        "y_hat = tf.placeholder(tf.float32, [None, n_class])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_dim 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_FKIBMOKSdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the weights and the biases for each layer\n",
        "weights = {\n",
        "    'h0': tf.Variable(tf.truncated_normal([n_dim, n_hidden_0])),\n",
        "    'h1': tf.Variable(tf.truncated_normal([n_hidden_0, n_hidden_1])),\n",
        "    'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2])),\n",
        "    'h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3])),\n",
        "    'out': tf.Variable(tf.truncated_normal([n_hidden_3, n_class]))\n",
        "}\n",
        "biases = {\n",
        "    'b0': tf.Variable(tf.truncated_normal([n_hidden_0])),\n",
        "    'b1': tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
        "    'b3': tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
        "    'out': tf.Variable(tf.truncated_normal([n_class]))\n",
        "}\n",
        "\n",
        "# Define the model\n",
        "def create_model(x, weights, biases):\n",
        "\n",
        "  # 0- Input layer with RELU activationed\n",
        "  layer_0 = tf.nn.relu(tf.add(tf.matmul(x, weights['h0']), biases['b0']))\n",
        "\n",
        "  # 1- Hidden layer with RELU activationed\n",
        "  layer_1 = tf.nn.relu(tf.add(tf.matmul(layer_0, weights['h1']), biases['b1']))\n",
        "\n",
        "  # 2- Hidden layer with RELU activationed\n",
        "  layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n",
        "\n",
        "  # 3- Hidden layer with softmax activationed\n",
        "  layer_3 = tf.nn.softmax(tf.add(tf.matmul(layer_2, weights['h3']), biases['b3']))\n",
        "\n",
        "  # Output layer with linear activation\n",
        "  output_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
        "\n",
        "  return output_layer\n",
        "\n",
        "  \n",
        "# Initialize all the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# create the model\n",
        "y = create_model(x, weights, biases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhAJuRX5N0E8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the cost function and optimizer\n",
        "cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y, labels=y))\n",
        "training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
        "\n",
        "# create session\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXqm5Q7dPR1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a5b1aee-b921-41a9-849d-8e6624459a62"
      },
      "source": [
        "# Calculate the cost and the accuracy for each epoch\n",
        "mse_history = []\n",
        "accuracy_history = []\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "  sess.run(training_step, feed_dict={x: train_x, y_hat: train_y})\n",
        "  cost = sess.run(cost_function, feed_dict={x: train_x, y_hat: train_y})\n",
        "\n",
        "  cost_history = np.append(cost_history, cost)\n",
        "  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_hat, 1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "  pred_y = sess.run(y, feed_dict={x: test_x})\n",
        "  mse = tf.reduce_mean(tf.square(pred_y - test_y))\n",
        "  mse_ = sess.run(mse)\n",
        "  mse_history.append(mse_)\n",
        "\n",
        "  accuracy = (sess.run(accuracy, feed_dict={x: train_x, y_hat: train_y}))\n",
        "  accuracy_history.append(accuracy)\n",
        "\n",
        "  print(f'epoch: {str(epoch)} - cost: {str(cost)} - MSE: {str(mse_)} - Train Accuracy: {str(accuracy)}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 - cost: -42.613537 - MSE: 13.683124104733857 - Train Accuracy: 0.26056337\n",
            "epoch: 1 - cost: -143.69301 - MSE: 45.515401646798196 - Train Accuracy: 0.26056337\n",
            "epoch: 2 - cost: -577.20844 - MSE: 178.9610891626658 - Train Accuracy: 0.26056337\n",
            "epoch: 3 - cost: -2775.388 - MSE: 872.6638840699464 - Train Accuracy: 0.26056337\n",
            "epoch: 4 - cost: -13647.438 - MSE: 4364.802849994914 - Train Accuracy: 0.26056337\n",
            "epoch: 5 - cost: -66790.76 - MSE: 21618.02985176721 - Train Accuracy: 0.26056337\n",
            "epoch: 6 - cost: -325836.16 - MSE: 106348.68976381265 - Train Accuracy: 0.26056337\n",
            "epoch: 7 - cost: -1586051.5 - MSE: 520734.21163976775 - Train Accuracy: 0.26056337\n",
            "epoch: 8 - cost: -7708120.5 - MSE: 2541415.480430316 - Train Accuracy: 0.26056337\n",
            "epoch: 9 - cost: -37418532.0 - MSE: 12374437.178676872 - Train Accuracy: 0.26056337\n",
            "epoch: 10 - cost: -181497070.0 - MSE: 60152478.83736488 - Train Accuracy: 0.26056337\n",
            "epoch: 11 - cost: -879823940.0 - MSE: 292054183.5155289 - Train Accuracy: 0.26056337\n",
            "epoch: 12 - cost: -4263195600.0 - MSE: 1416766814.1555645 - Train Accuracy: 0.26056337\n",
            "epoch: 13 - cost: -20650928000.0 - MSE: 6868504197.490711 - Train Accuracy: 0.26056337\n",
            "epoch: 14 - cost: -100010600000.0 - MSE: 33283559143.8142 - Train Accuracy: 0.26056337\n",
            "epoch: 15 - cost: -484263000000.0 - MSE: 161233269883.2525 - Train Accuracy: 0.26056337\n",
            "epoch: 16 - cost: -2344578000000.0 - MSE: 780864849933.9165 - Train Accuracy: 0.26056337\n",
            "epoch: 17 - cost: -11350374000000.0 - MSE: 3781130142156.1445 - Train Accuracy: 0.26056337\n",
            "epoch: 18 - cost: -54945040000000.0 - MSE: 18306814151362.5 - Train Accuracy: 0.26056337\n",
            "epoch: 19 - cost: -265966640000000.0 - MSE: 88626681328082.67 - Train Accuracy: 0.26056337\n",
            "epoch: 20 - cost: -1287391300000000.0 - MSE: 429028941581615.56 - Train Accuracy: 0.26056337\n",
            "epoch: 21 - cost: -6231385000000000.0 - MSE: 2076770289475316.2 - Train Accuracy: 0.26056337\n",
            "epoch: 22 - cost: -3.0161313e+16 - MSE: 1.005251186769e+16 - Train Accuracy: 0.26056337\n",
            "epoch: 23 - cost: -1.4598561e+17 - MSE: 4.865744500864899e+16 - Train Accuracy: 0.26056337\n",
            "epoch: 24 - cost: -7.06588e+17 - MSE: 2.3551375818718026e+17 - Train Accuracy: 0.26056337\n",
            "epoch: 25 - cost: -3.4199504e+18 - MSE: 1.1399283672910143e+18 - Train Accuracy: 0.26056337\n",
            "epoch: 26 - cost: -1.6552766e+19 - MSE: 5.517394715584557e+18 - Train Accuracy: 0.26056337\n",
            "epoch: 27 - cost: -8.011617e+19 - MSE: 2.670471044157628e+19 - Train Accuracy: 0.26056337\n",
            "epoch: 28 - cost: -3.877652e+20 - MSE: 1.2925269391970748e+20 - Train Accuracy: 0.26056337\n",
            "epoch: 29 - cost: -1.8767935e+21 - MSE: 6.255893394590398e+20 - Train Accuracy: 0.26056337\n",
            "epoch: 30 - cost: -9.083708e+21 - MSE: 3.027873503603951e+21 - Train Accuracy: 0.26056337\n",
            "epoch: 31 - cost: -4.3965265e+22 - MSE: 1.4654983783219747e+22 - Train Accuracy: 0.26056337\n",
            "epoch: 32 - cost: -2.1279222e+23 - MSE: 7.0930373981392414e+22 - Train Accuracy: 0.26056337\n",
            "epoch: 33 - cost: -1.02991544e+24 - MSE: 3.4330390486486185e+23 - Train Accuracy: 0.26056337\n",
            "epoch: 34 - cost: -4.9847985e+24 - MSE: 1.6615948797335356e+24 - Train Accuracy: 0.26056337\n",
            "epoch: 35 - cost: -2.4126423e+25 - MSE: 8.042124879316388e+24 - Train Accuracy: 0.26056337\n",
            "epoch: 36 - cost: -1.1677197e+26 - MSE: 3.892392602478146e+25 - Train Accuracy: 0.26056337\n",
            "epoch: 37 - cost: -5.651763e+26 - MSE: 1.8839185695386925e+26 - Train Accuracy: 0.26056337\n",
            "epoch: 38 - cost: -2.7354544e+27 - MSE: 9.118174280955165e+26 - Train Accuracy: 0.26056337\n",
            "epoch: 39 - cost: -1.3239608e+28 - MSE: 4.413200384228586e+27 - Train Accuracy: 0.26056337\n",
            "epoch: 40 - cost: -6.40797e+28 - MSE: 2.13598867818661e+28 - Train Accuracy: 0.26056337\n",
            "epoch: 41 - cost: -3.1014573e+29 - MSE: 1.0338188619895559e+29 - Train Accuracy: 0.26056337\n",
            "epoch: 42 - cost: -1.501105e+30 - MSE: 5.0036824160914946e+29 - Train Accuracy: 0.26056337\n",
            "epoch: 43 - cost: -7.265345e+30 - MSE: 2.4217814012565858e+30 - Train Accuracy: 0.26056337\n",
            "epoch: 44 - cost: -3.5164265e+31 - MSE: 1.1721420128927125e+31 - Train Accuracy: 0.26056337\n",
            "epoch: 45 - cost: -1.7019511e+32 - MSE: 5.673170144805226e+31 - Train Accuracy: 0.26056337\n",
            "epoch: 46 - cost: -8.2374395e+32 - MSE: 2.7458128337371155e+32 - Train Accuracy: 0.26056337\n",
            "epoch: 47 - cost: -3.986918e+33 - MSE: 1.328972269286999e+33 - Train Accuracy: 0.26056337\n",
            "epoch: 48 - cost: -1.929667e+34 - MSE: 6.432223342578915e+33 - Train Accuracy: 0.26056337\n",
            "epoch: 49 - cost: -9.3396e+34 - MSE: 3.113199813766015e+34 - Train Accuracy: 0.26056337\n",
            "epoch: 50 - cost: -4.520368e+35 - MSE: 1.5067895704199563e+35 - Train Accuracy: 0.26056337\n",
            "epoch: 51 - cost: -2.1878558e+36 - MSE: 7.292852230188269e+35 - Train Accuracy: 0.26056337\n",
            "epoch: 52 - cost: -inf - MSE: 3.5297422930059176e+36 - Train Accuracy: 0.26056337\n",
            "epoch: 53 - cost: -inf - MSE: 1.7083948534524273e+37 - Train Accuracy: 0.26056337\n",
            "epoch: 54 - cost: -inf - MSE: 8.268628429401203e+37 - Train Accuracy: 0.26056337\n",
            "epoch: 55 - cost: -inf - MSE: 4.002018743481945e+38 - Train Accuracy: 0.26056337\n",
            "epoch: 56 - cost: -inf - MSE: 1.936977013309552e+39 - Train Accuracy: 0.26056337\n",
            "epoch: 57 - cost: -inf - MSE: 9.374972990508546e+39 - Train Accuracy: 0.26056337\n",
            "epoch: 58 - cost: -inf - MSE: 4.537485191657004e+40 - Train Accuracy: 0.26056337\n",
            "epoch: 59 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 60 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 61 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 62 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 63 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 64 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 65 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 66 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 67 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 68 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 69 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 70 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 71 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 72 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 73 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 74 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 75 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 76 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 77 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 78 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 79 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 80 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 81 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 82 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 83 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 84 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 85 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 86 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 87 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 88 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 89 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 90 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 91 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 92 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 93 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 94 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 95 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 96 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 97 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 98 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n",
            "epoch: 99 - cost: nan - MSE: nan - Train Accuracy: 0.3309859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c9a8S3uRmM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "8fc19424-0725-4bb2-f500-4dfe5eb8b6b0"
      },
      "source": [
        "# Plot Accuracy Graph\n",
        "plt.plot(accuracy_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIgY6P8DR1F6rbrNu84AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7FwFIpKSFtS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fce5dfe0-408d-4216-b836-b2bd2e0e937c"
      },
      "source": [
        "# final accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_hat, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print(\"Test Accuracy: \", (sess.run(accuracy, feed_dict={x: test_x, y_hat: test_y})))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.33333334\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}