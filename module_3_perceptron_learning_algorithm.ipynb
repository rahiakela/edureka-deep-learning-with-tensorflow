{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "module-3-perceptron-learning-algorithm.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNgqlg821r2ZmrpJEQqZDQV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/edureka-deep-learning-with-tensorflow/blob/module-3-deep-dive-into-neural-networks-with-tensorFlow/module_3_perceptron_learning_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X87MA0ZfoAA",
        "colab_type": "text"
      },
      "source": [
        "# Perceptron Learning Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV0J4p1wfo39",
        "colab_type": "text"
      },
      "source": [
        "As you know a perceptron serves as a basic building block for creating a deep neural network therefore, it is quite obvious that we should begin our journey of mastering Deep Learning with perceptron and learn how to implement it using TensorFlow to solve different problems. \n",
        "\n",
        "Following are the topics that will be covered in this blog on Perceptron Learning Algorithm:\n",
        "\n",
        "* Perceptron as a Linear Classifier\n",
        "* Implementation of a Perceptron using TensorFlow Library\n",
        "* SONAR Data Classification Using a Single Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsWX2LjigNcN",
        "colab_type": "text"
      },
      "source": [
        "## Types of Classification Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCK8qKpqgOcn",
        "colab_type": "text"
      },
      "source": [
        "One can categorize all kinds of classification problems that can be solved using neural networks into two broad categories:\n",
        "* Linearly Separable Problems\n",
        "* Non-Linearly Separable Problems\n",
        "\n",
        "Basically, a problem is said to be linearly separable if you can classify the data set into two categories or classes using a single line. For example, separating cats from a group of cats and dogs. On the contrary, in case of a non-linearly separable problems, the data set contains multiple classes and requires non-linear line for separating them into their respective classes. For example, classification of handwritten digits. Let us visualize the difference between the two by plotting the graph of a linearly separable problem and non-linearly problem data set:\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/07/Linear-528x264.jpg?raw=1' width='800'/>\n",
        "\n",
        "Since, you all are familiar with AND Gates, I will be using it as an example to explain how a perceptron works as a linear classifier.\n",
        "\n",
        "**Note**: As you move onto much more complex problems such as Image Recognition, which I covered briefly in the previous blog, the relationship in the data that you want to capture becomes highly non-linear and therefore, requires a network which consists of multiple artificial neurons, called as artificial neural network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l38AVIwCgt7O",
        "colab_type": "text"
      },
      "source": [
        "## Perceptron as AND Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw_i1kKjgxfj",
        "colab_type": "text"
      },
      "source": [
        "As you know that AND gate produces an output as 1 if both the inputs are 1 and 0 in all other cases. Therefore, a perceptron can be used as a separator or a decision line that divides the input set of AND Gate, into two classes:\n",
        "\n",
        "* **Class 1**: Inputs having output as 0 that lies below the decision line.\n",
        "* **Class 2**: Inputs having output as 1 that lies above the decision line or separator. \n",
        "\n",
        "The below diagram shows the above idea of classifying the inputs of AND Gate using a perceptron:\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/07/AND-Gate-Classifier-Deep-Learning-Tutorial-Edureka-528x194.png?raw=1' width='800'/>\n",
        "\n",
        "Till now, you understood that a linear perceptron can be used to classify the input data set into two classes. But, how does it actually classify the data? \n",
        "\n",
        "Mathematically, one can represent a perceptron as a function of weights, inputs and bias (vertical offset):\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/06/Transfer-Function-Deep-Learning-Tutorial-Edureka-300x152.png?raw=1' width='800'/>\n",
        "\n",
        "* Each of the input received by the perceptron has been weighted based on the amount of its contribution for obtaining the final output. \n",
        "* Bias allows us to shift the decision line so that it can best separate the inputs into two classes.\n",
        "\n",
        "Enough of the theory, let us look at the first example of this blog on Perceptron Learning Algorithm where I will implement AND Gate using a perceptron from scratch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e47qA-LzhaQ0",
        "colab_type": "text"
      },
      "source": [
        "## Perceptron Learning Algorithm: Implementation of AND Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2NY31fRhbuV",
        "colab_type": "text"
      },
      "source": [
        "### 1. Import all the required library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdZ1hlxIhw9D",
        "colab_type": "code",
        "outputId": "f5f1b074-231e-42e3-ab97-a9c7f4f7de91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p9bgabRh1Ez",
        "colab_type": "text"
      },
      "source": [
        "### Define Vector Variables for Input and Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OraQuPBh4id",
        "colab_type": "text"
      },
      "source": [
        "Now, I will create variables for storing the input, output and bias for my perceptron:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beBxtabgh7BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input1, input2 and bias\n",
        "train_in = [\n",
        "   [1., 1., 1],\n",
        "   [1., 0, 1],\n",
        "   [0, 1., 1],\n",
        "   [0, 0, 1]         \n",
        "]\n",
        "\n",
        "# target\n",
        "train_out = [[1.], [0], [0], [0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiYRxW21iXNx",
        "colab_type": "text"
      },
      "source": [
        "### 3. Define Weight Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HT8JTLbiYIz",
        "colab_type": "text"
      },
      "source": [
        "Now, I need to define the weight variable and assign some random values to it initially. Since, I have three inputs over here (input 1, input 2 & bias), I will require 3 weight values for each input. So, I will define a tensor variable of shape 3×1 for our weights that will be initialized with random values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLrI59cwiWS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weight variable initialized with random values using random_normal()\n",
        "w = tf.Variable(tf.random_normal([3, 1], seed=12))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihBZ3xhgivUs",
        "colab_type": "text"
      },
      "source": [
        "### 4. Define placeholders for Input and Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euygNDrwiw8f",
        "colab_type": "text"
      },
      "source": [
        "In TensorFlow, you can specify placeholders that can accept external inputs on the runtime. So, I will define two placeholders –  x for input and y for output. Later on, you will understand how to feed inputs to a placeholder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2dkHF0Nisg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Placeholder for input and output\n",
        "x = tf.placeholder(tf.float32, [None, 3])\n",
        "y = tf.placeholder(tf.float32, [None, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CRIPl6zjGwr",
        "colab_type": "text"
      },
      "source": [
        "### 5. Calculate Output and Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ekemDTjKnC",
        "colab_type": "text"
      },
      "source": [
        "As discussed earlier, the input received by a perceptron is first multiplied by the respective weights and then, all these weighted inputs are summed together. This summed value is then fed to activation for obtaining the final result as shown in the image below followed by the the code:\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/07/AND-Gate-Perceptron-Perceptron-Learning-Algorithm-Edureka-528x207.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42FvadTqje1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate output\n",
        "output = tf.nn.relu(tf.matmul(x, w))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWmTCpX2jqgc",
        "colab_type": "text"
      },
      "source": [
        "### 6. Calculate the Cost or Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqXyxiCSjrf4",
        "colab_type": "text"
      },
      "source": [
        "Now, I need to calculate the error value w.r.t perceptron output and the desired output. Generally, this error is calculated as Mean Squared Error which is nothing but the square of difference of perceptron output and desired output as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KtUfM24joYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mean Squared Loss or Error\n",
        "loss = tf.reduce_sum(tf.square(output - y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDY6ep0Aj7At",
        "colab_type": "text"
      },
      "source": [
        "### 7. Minimize Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk8x1Qqcj-qH",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow provides optimizers that slowly change each variable (weight and bias) in order to minimize the loss in successive iterations. The simplest optimizer is gradient descent which I will be using in this case. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpqcJPVVkEHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Minimize loss using GradientDescentOptimizer with a learning rate of 0.01\n",
        "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
        "train = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5Q1mJlokUV3",
        "colab_type": "text"
      },
      "source": [
        "### 8. Initialize all the variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya66I47IkVRd",
        "colab_type": "text"
      },
      "source": [
        "Variables are not initialized when you call tf.Variable. So, I need to explicitly initialize all the variables in a TensorFlow program using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ0VBZCmkRo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize all the global variables\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eomg17bdknyW",
        "colab_type": "text"
      },
      "source": [
        "### 9. Training Perceptron in Iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvC7MemGkoxW",
        "colab_type": "text"
      },
      "source": [
        "Now, I need to train our perceptron i.e. update values of weights and bias in successive iteration to minimize the error or loss. Here, I will train our perceptron in 1000 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBiykzKDklOi",
        "colab_type": "code",
        "outputId": "4abcd348-5d0c-4ff4-a0fb-10d67f3aaf11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# Compute output and cost w.r.t to input vector\n",
        "for i in range(10):\n",
        "  sess.run(train, {x: train_in, y: train_out})\n",
        "  cost = sess.run(loss, feed_dict={x: train_in, y: train_out})\n",
        "  print(f'Epoch-- {str(i)} -- loss -- {str(cost)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch-- 0 -- loss -- 1.0213106\n",
            "Epoch-- 1 -- loss -- 1.0033305\n",
            "Epoch-- 2 -- loss -- 0.9856898\n",
            "Epoch-- 3 -- loss -- 0.96837854\n",
            "Epoch-- 4 -- loss -- 0.95138687\n",
            "Epoch-- 5 -- loss -- 0.93470633\n",
            "Epoch-- 6 -- loss -- 0.9183289\n",
            "Epoch-- 7 -- loss -- 0.90224737\n",
            "Epoch-- 8 -- loss -- 0.8864547\n",
            "Epoch-- 9 -- loss -- 0.8709445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C3b5uEylSbn",
        "colab_type": "text"
      },
      "source": [
        "In  the above code, you can observe how I am feeding train_in (input set of AND Gate) and train_out (output set of AND gate) to placeholders x and y respectively using feed_dict for calculating the cost or error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooz07MrGl2Ko",
        "colab_type": "text"
      },
      "source": [
        "## Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he9bvf-el3w8",
        "colab_type": "text"
      },
      "source": [
        "As discussed earlier, the activation function is applied to the output of a perceptron as shown in the image below:\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/06/Activation-Function-Deep-Learning-Tutorial-Edureka-300x121.png?raw=1' width='800'/>\n",
        "\n",
        "In the previous example, I have shown you how to use a linear perceptron with relu activation function for performing linear classification on the input set of AND Gate. But, what if the classification that you wish to perform is non-linear in nature. In that case, you will be using one of the non-linear activation functions. Some of the prominent non-linear activation functions have been shown below:\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/06/Activation-Functions-Deep-Learning-Tutorial-Edureka-528x177.png?raw=1' width='800'/>\n",
        "\n",
        "TensorFlow library provides built-in functions for applying activation functions. The built-in functions w.r.t. above stated activation functions are listed below:\n",
        "\n",
        "* **tf.sigmoid(x, name=None)**\n",
        "  * Computes sigmoid of x element-wise\n",
        "  * For an element x, sigmoid is calculated as –  y = 1 / (1 + exp(-x))\n",
        "* **tf.nn.relu(features, name=None)**\n",
        "  * Computes rectified linear as – max(features, 0)\n",
        "* **tf.tanh(x, name=None)**\n",
        "  * Computes hyperbolic tangent of x element wise\n",
        "\n",
        "So far, you have learned how a perceptron works and how you can program it using TensorFlow. So, it’s time to move ahead and apply our understanding of a perceptron to solve an interesting use case on SONAR Data Classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5F-B1XMmqmq",
        "colab_type": "text"
      },
      "source": [
        "## SONAR Data Classification Using Single Layer Perceptrons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b60csAxmuQc",
        "colab_type": "text"
      },
      "source": [
        "In this use case, I have been provided with a SONAR data set which contains the data about 208 patterns obtained by bouncing sonar signals off a metal cylinder (naval mine) and a rock at various angles and under various conditions. Now, as you know, a naval mine is a self-contained explosive device placed in water to damage or destroy surface ships or submarines. So, our goal is to build a model that can predict whether the object is a naval mine or rock based on our data set. \n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/07/SONAR-Use-Case-Single-Layer-Perceptron-Edureka-528x220.png?raw=1' width='800'/>\n",
        "\n",
        "Now, let us have a look at our SONAR data set:\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/07/SONAR-Data-Set-Single-Layer-Perceptron-Edureka-4.png?raw=1' width='800'/>\n",
        "\n",
        "Here, the overall fundamental procedure will be same as that of AND gate with few difference which will be discussed to avoid any confusion. Let me provide you a walk-through of all the steps to perform linear classification on SONAR data set using Single Layer Perceptron:\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/07/SONAR-Data-Use-Case-Work-Flow-Single-Layer-Perceptron-Edureka.png?raw=1' width='800'/>\n",
        "\n",
        "Now that you have a good idea about all the steps involved in this use case, let us go ahead and program the model using TensorFlow:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoMRlsfjeoqa",
        "colab_type": "text"
      },
      "source": [
        "### 1. Import all the required Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJHGHX6nesrD",
        "colab_type": "text"
      },
      "source": [
        "At first, I will begin with all the required libraries as listed below:\n",
        "\n",
        "* **matplotlib library**: It provides functions for plotting graph.\n",
        "* **tensorflow library**: It provides functions for implementing Deep Learning Model. \n",
        "* **pandas, numpy and sklearn library**: It provides functions for pre-processing the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A2b21jffW0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "e85d6960-d706-4aef-ecdc-4919ee696700"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr_NizBMe5iD",
        "colab_type": "text"
      },
      "source": [
        "### 2. Read and Pre-process the data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4wk1Sne8eg",
        "colab_type": "text"
      },
      "source": [
        "In the previous example, I defined the input and the output variable w.r.t. AND Gate and explicitly assigned the required values to it. But, in real-life use cases like SONAR, you will be provided with the raw data set which you need to read and pre-process so that you can train your model around it.\n",
        "\n",
        "* At first I will read the CSV file (input data set) using read_csv() function\n",
        "* Then, I will segregate the feature columns (independent variables) and the output column (dependent variable) as X and y respectively\n",
        "* The output column consists of string categorical values as ‘M’ and ‘R’, signifying Rock and Mine respectively. So, I will label them them as 0 and 1 w.r.t. ‘M’ and ‘R’\n",
        "* After I have converted these categorical values into integer labels, I will apply one hot encoding using one_hot_encode() function that is discussed in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGBFUb7Hd9Lh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "9ff58aa2-cc53-48d8-b00d-4bd6dd091863"
      },
      "source": [
        "# Read the sonar dataset\n",
        "df = pd.read_csv('sonar.csv')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.02</th>\n",
              "      <th>0.0371</th>\n",
              "      <th>0.0428</th>\n",
              "      <th>0.0207</th>\n",
              "      <th>0.0954</th>\n",
              "      <th>0.0986</th>\n",
              "      <th>0.1539</th>\n",
              "      <th>0.1601</th>\n",
              "      <th>0.3109</th>\n",
              "      <th>0.2111</th>\n",
              "      <th>0.1609</th>\n",
              "      <th>0.1582</th>\n",
              "      <th>0.2238</th>\n",
              "      <th>0.0645</th>\n",
              "      <th>0.066</th>\n",
              "      <th>0.2273</th>\n",
              "      <th>0.31</th>\n",
              "      <th>0.2999</th>\n",
              "      <th>0.5078</th>\n",
              "      <th>0.4797</th>\n",
              "      <th>0.5783</th>\n",
              "      <th>0.5071</th>\n",
              "      <th>0.4328</th>\n",
              "      <th>0.555</th>\n",
              "      <th>0.6711</th>\n",
              "      <th>0.6415</th>\n",
              "      <th>0.7104</th>\n",
              "      <th>0.808</th>\n",
              "      <th>0.6791</th>\n",
              "      <th>0.3857</th>\n",
              "      <th>0.1307</th>\n",
              "      <th>0.2604</th>\n",
              "      <th>0.5121</th>\n",
              "      <th>0.7547</th>\n",
              "      <th>0.8537</th>\n",
              "      <th>0.8507</th>\n",
              "      <th>0.6692</th>\n",
              "      <th>0.6097</th>\n",
              "      <th>0.4943</th>\n",
              "      <th>0.2744</th>\n",
              "      <th>0.051</th>\n",
              "      <th>0.2834</th>\n",
              "      <th>0.2825</th>\n",
              "      <th>0.4256</th>\n",
              "      <th>0.2641</th>\n",
              "      <th>0.1386</th>\n",
              "      <th>0.1051</th>\n",
              "      <th>0.1343</th>\n",
              "      <th>0.0383</th>\n",
              "      <th>0.0324</th>\n",
              "      <th>0.0232</th>\n",
              "      <th>0.0027</th>\n",
              "      <th>0.0065</th>\n",
              "      <th>0.0159</th>\n",
              "      <th>0.0072</th>\n",
              "      <th>0.0167</th>\n",
              "      <th>0.018</th>\n",
              "      <th>0.0084</th>\n",
              "      <th>0.009</th>\n",
              "      <th>0.0032</th>\n",
              "      <th>R</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.4918</td>\n",
              "      <td>0.6552</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>0.7797</td>\n",
              "      <td>0.7464</td>\n",
              "      <td>0.9444</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8874</td>\n",
              "      <td>0.8024</td>\n",
              "      <td>0.7818</td>\n",
              "      <td>0.5212</td>\n",
              "      <td>0.4052</td>\n",
              "      <td>0.3957</td>\n",
              "      <td>0.3914</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.3200</td>\n",
              "      <td>0.3271</td>\n",
              "      <td>0.2767</td>\n",
              "      <td>0.4423</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.3788</td>\n",
              "      <td>0.2947</td>\n",
              "      <td>0.1984</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>0.4182</td>\n",
              "      <td>0.3835</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.1840</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.0583</td>\n",
              "      <td>0.1401</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.0621</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0530</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>0.6333</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.5544</td>\n",
              "      <td>0.5320</td>\n",
              "      <td>0.6479</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>0.6759</td>\n",
              "      <td>0.7551</td>\n",
              "      <td>0.8929</td>\n",
              "      <td>0.8619</td>\n",
              "      <td>0.7974</td>\n",
              "      <td>0.6737</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.3648</td>\n",
              "      <td>0.5331</td>\n",
              "      <td>0.2413</td>\n",
              "      <td>0.5070</td>\n",
              "      <td>0.8533</td>\n",
              "      <td>0.6036</td>\n",
              "      <td>0.8514</td>\n",
              "      <td>0.8512</td>\n",
              "      <td>0.5045</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.2709</td>\n",
              "      <td>0.4232</td>\n",
              "      <td>0.3043</td>\n",
              "      <td>0.6116</td>\n",
              "      <td>0.6756</td>\n",
              "      <td>0.5375</td>\n",
              "      <td>0.4719</td>\n",
              "      <td>0.4647</td>\n",
              "      <td>0.2587</td>\n",
              "      <td>0.2129</td>\n",
              "      <td>0.2222</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.1348</td>\n",
              "      <td>0.0744</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>0.0106</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>0.1992</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.2261</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0693</td>\n",
              "      <td>0.2281</td>\n",
              "      <td>0.4060</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.2741</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.5556</td>\n",
              "      <td>0.4846</td>\n",
              "      <td>0.3140</td>\n",
              "      <td>0.5334</td>\n",
              "      <td>0.5256</td>\n",
              "      <td>0.2520</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>0.3559</td>\n",
              "      <td>0.6260</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.6120</td>\n",
              "      <td>0.3497</td>\n",
              "      <td>0.3953</td>\n",
              "      <td>0.3012</td>\n",
              "      <td>0.5408</td>\n",
              "      <td>0.8814</td>\n",
              "      <td>0.9857</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.6121</td>\n",
              "      <td>0.5006</td>\n",
              "      <td>0.3210</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.3654</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1576</td>\n",
              "      <td>0.0681</td>\n",
              "      <td>0.0294</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>0.4152</td>\n",
              "      <td>0.3952</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.4135</td>\n",
              "      <td>0.4528</td>\n",
              "      <td>0.5326</td>\n",
              "      <td>0.7306</td>\n",
              "      <td>0.6193</td>\n",
              "      <td>0.2032</td>\n",
              "      <td>0.4636</td>\n",
              "      <td>0.4148</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.5730</td>\n",
              "      <td>0.5399</td>\n",
              "      <td>0.3161</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>0.6995</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7262</td>\n",
              "      <td>0.4724</td>\n",
              "      <td>0.5103</td>\n",
              "      <td>0.5459</td>\n",
              "      <td>0.2881</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.1951</td>\n",
              "      <td>0.4181</td>\n",
              "      <td>0.4604</td>\n",
              "      <td>0.3217</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.1979</td>\n",
              "      <td>0.2444</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.0841</td>\n",
              "      <td>0.0692</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0286</td>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0277</td>\n",
              "      <td>0.0174</td>\n",
              "      <td>0.0384</td>\n",
              "      <td>0.0990</td>\n",
              "      <td>0.1201</td>\n",
              "      <td>0.1833</td>\n",
              "      <td>0.2105</td>\n",
              "      <td>0.3039</td>\n",
              "      <td>0.2988</td>\n",
              "      <td>0.4250</td>\n",
              "      <td>0.6343</td>\n",
              "      <td>0.8198</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9988</td>\n",
              "      <td>0.9508</td>\n",
              "      <td>0.9025</td>\n",
              "      <td>0.7234</td>\n",
              "      <td>0.5122</td>\n",
              "      <td>0.2074</td>\n",
              "      <td>0.3985</td>\n",
              "      <td>0.5890</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.2043</td>\n",
              "      <td>0.5782</td>\n",
              "      <td>0.5389</td>\n",
              "      <td>0.3750</td>\n",
              "      <td>0.3411</td>\n",
              "      <td>0.5067</td>\n",
              "      <td>0.5580</td>\n",
              "      <td>0.4778</td>\n",
              "      <td>0.3299</td>\n",
              "      <td>0.2198</td>\n",
              "      <td>0.1407</td>\n",
              "      <td>0.2856</td>\n",
              "      <td>0.3807</td>\n",
              "      <td>0.4158</td>\n",
              "      <td>0.4054</td>\n",
              "      <td>0.3296</td>\n",
              "      <td>0.2707</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.0723</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1192</td>\n",
              "      <td>0.1089</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0494</td>\n",
              "      <td>0.0264</td>\n",
              "      <td>0.0081</td>\n",
              "      <td>0.0104</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0.02  0.0371  0.0428  0.0207  0.0954  ...   0.018  0.0084   0.009  0.0032  R\n",
              "0  0.0453  0.0523  0.0843  0.0689  0.1183  ...  0.0140  0.0049  0.0052  0.0044  R\n",
              "1  0.0262  0.0582  0.1099  0.1083  0.0974  ...  0.0316  0.0164  0.0095  0.0078  R\n",
              "2  0.0100  0.0171  0.0623  0.0205  0.0205  ...  0.0050  0.0044  0.0040  0.0117  R\n",
              "3  0.0762  0.0666  0.0481  0.0394  0.0590  ...  0.0072  0.0048  0.0107  0.0094  R\n",
              "4  0.0286  0.0453  0.0277  0.0174  0.0384  ...  0.0057  0.0027  0.0051  0.0062  R\n",
              "\n",
              "[5 rows x 61 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBPRK_Lsfwwb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "746ddf8e-3400-4a6f-a478-417932075a66"
      },
      "source": [
        "# check how many columns?\n",
        "len(df.columns)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om2XO_KVgCvI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ce05e8c-5757-44c5-ce13-06ef10db3828"
      },
      "source": [
        "# divide dataset into features and lables\n",
        "X = df[df.columns[0:60]].values\n",
        "y = df[df.columns[60]]\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(207, 60) (207,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxTUk7Higdgy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8787ff1f-035b-4052-fe14-51ef11142d25"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# encode the dependent variable as it has two categorical values\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "y = encoder.transform(y)\n",
        "y[:5]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2kvj7s7hcio",
        "colab_type": "text"
      },
      "source": [
        "### Function for One Hot Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2vydorlhdeK",
        "colab_type": "text"
      },
      "source": [
        "One Hot Encoder adds extra columns based on number of labels present in the column. In this case, I have two labels 0 and 1 (for Rock and Mine). Therefore, two extra columns will be added corresponding to each categorical value as shown in the image below:\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/07/One-Hot-Encoder-Perceptron-Learning-Algorithm-Edureka-528x182.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK1x7GbmhMbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "176be5c1-3edd-419b-8b30-645fcefdb999"
      },
      "source": [
        "# function for applying one_hot_encoder\n",
        "def one_hot_encode(labels):\n",
        "  n_labels = len(labels)\n",
        "  n_unique_labels = len(np.unique(labels, ))\n",
        "  one_hot_encode = np.zeros((n_labels, n_unique_labels))\n",
        "  one_hot_encode[np.arange(n_labels), labels] = 1\n",
        "\n",
        "  return one_hot_encode\n",
        "\n",
        "Y = one_hot_encode(y)\n",
        "Y[:5]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4beKqd_HkaqB",
        "colab_type": "text"
      },
      "source": [
        "### Dividing data set into Training and Test Subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t9E3hG-kb0J",
        "colab_type": "text"
      },
      "source": [
        "While working on any deep learning project, you need to divide your data set into two parts where one of the parts is used for training your deep learning model and the other is used for validating the model once it has been trained. Therefore, in this step I will also divide the data set into two subsets:\n",
        "\n",
        "* **Training Subset**: It is used for training the model\n",
        "* **Test Subset**: It is used for validating our trained model \n",
        "\n",
        "I will be use train_test_split() function from the sklearn library for dividing the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdE6EY-KitdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4ba17eee-1c0b-4dc6-f6ba-c7fd04f91934"
      },
      "source": [
        "from sklearn.utils import  shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Divide the data in training and test subset\n",
        "X, Y = shuffle(X, Y, random_state=1)\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(165, 60)\n",
            "(165, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yBYroAVxijP",
        "colab_type": "text"
      },
      "source": [
        "### 5. Define Variables and Placeholders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQu5O__Mxjc1",
        "colab_type": "text"
      },
      "source": [
        "Here, I will be define variables for following entities:\n",
        "\n",
        "* **Learning Rate**: The amount by which the weight will be adjusted.\n",
        "* **Training Epochs**: No. of iterations\n",
        "* **Cost History**: An array that stores the cost values in successive epochs.\n",
        "* **Weight**: Tensor variable for storing weight values\n",
        "* **Bias**: Tensor variable for storing bias values\n",
        "\n",
        "Apart from variable, I will also need placeholders that can take input. So, I will create place holder for my input and feed it with the data set later on. At last, I will call global_variable_initializer() to initialize all the variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N31d2TPlbgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define all the variables to work with the tensors\n",
        "learning_rate = 0.1\n",
        "training_epochs = 1000\n",
        "\n",
        "cost_history = np.empty(shape=[1], dtype=float)\n",
        "n_dim = X.shape[1]\n",
        "n_class = 2\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, n_dim])\n",
        "W = tf.Variable(tf.zeros([n_dim, n_class]))\n",
        "b = tf.Variable(tf.zeros([n_class]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoqbsHZXzG81",
        "colab_type": "text"
      },
      "source": [
        "### 6. Calculate the Cost or Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsTbb-VdzHze",
        "colab_type": "text"
      },
      "source": [
        "Similar to AND Gate implementation, I will calculate the cost or error produced by our model. Instead of Mean Squared Error, I will use cross entropy to calculate the error in this case. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pA_ouTXy8Pq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat = tf.placeholder(tf.float32, [None, n_class])\n",
        "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
        "\n",
        "cost_function = tf.reduce_mean( - tf.reduce_sum((y_hat * tf.log(y)), reduction_indices=[1]))\n",
        "training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdFS_i3jz-RG",
        "colab_type": "text"
      },
      "source": [
        "### 7. Training the Perceptron Model in Successive Epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EgRH90Fz_Qp",
        "colab_type": "text"
      },
      "source": [
        "Now, I will train my model in successive epochs. In each of the epochs, the cost is calculated and then, based on this cost the optimizer modifies the weight and bias variables in order to minimize the error. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB042K2Pz8Dk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22f17b2d-b95c-484a-99d6-46f1dd44318a"
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "# Minimizing the cost for each epoch\n",
        "for epoch in range(training_epochs):\n",
        "  sess.run(training_step, feed_dict={x: train_x, y_hat: train_y})\n",
        "  cost = sess.run(cost_function, feed_dict={x: train_x, y_hat: train_y})\n",
        "  cost_history = np.append(cost_history, cost)\n",
        "  print(f'epoch: {str(epoch)}  - cost: {str(cost)}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0  - cost: 0.6890586\n",
            "epoch: 1  - cost: 0.6861614\n",
            "epoch: 2  - cost: 0.68367136\n",
            "epoch: 3  - cost: 0.6813467\n",
            "epoch: 4  - cost: 0.6791113\n",
            "epoch: 5  - cost: 0.6769402\n",
            "epoch: 6  - cost: 0.6748241\n",
            "epoch: 7  - cost: 0.6727587\n",
            "epoch: 8  - cost: 0.67074114\n",
            "epoch: 9  - cost: 0.6687693\n",
            "epoch: 10  - cost: 0.6668413\n",
            "epoch: 11  - cost: 0.6649552\n",
            "epoch: 12  - cost: 0.6631094\n",
            "epoch: 13  - cost: 0.6613022\n",
            "epoch: 14  - cost: 0.65953195\n",
            "epoch: 15  - cost: 0.65779734\n",
            "epoch: 16  - cost: 0.6560968\n",
            "epoch: 17  - cost: 0.6544292\n",
            "epoch: 18  - cost: 0.6527932\n",
            "epoch: 19  - cost: 0.6511875\n",
            "epoch: 20  - cost: 0.6496111\n",
            "epoch: 21  - cost: 0.64806294\n",
            "epoch: 22  - cost: 0.6465418\n",
            "epoch: 23  - cost: 0.64504665\n",
            "epoch: 24  - cost: 0.6435769\n",
            "epoch: 25  - cost: 0.6421313\n",
            "epoch: 26  - cost: 0.6407093\n",
            "epoch: 27  - cost: 0.63930976\n",
            "epoch: 28  - cost: 0.63793224\n",
            "epoch: 29  - cost: 0.6365758\n",
            "epoch: 30  - cost: 0.63523984\n",
            "epoch: 31  - cost: 0.6339236\n",
            "epoch: 32  - cost: 0.6326266\n",
            "epoch: 33  - cost: 0.6313482\n",
            "epoch: 34  - cost: 0.63008755\n",
            "epoch: 35  - cost: 0.6288445\n",
            "epoch: 36  - cost: 0.62761825\n",
            "epoch: 37  - cost: 0.6264085\n",
            "epoch: 38  - cost: 0.6252146\n",
            "epoch: 39  - cost: 0.6240361\n",
            "epoch: 40  - cost: 0.6228727\n",
            "epoch: 41  - cost: 0.62172383\n",
            "epoch: 42  - cost: 0.62058926\n",
            "epoch: 43  - cost: 0.6194685\n",
            "epoch: 44  - cost: 0.6183612\n",
            "epoch: 45  - cost: 0.617267\n",
            "epoch: 46  - cost: 0.6161856\n",
            "epoch: 47  - cost: 0.6151166\n",
            "epoch: 48  - cost: 0.6140598\n",
            "epoch: 49  - cost: 0.6130148\n",
            "epoch: 50  - cost: 0.61198145\n",
            "epoch: 51  - cost: 0.61095935\n",
            "epoch: 52  - cost: 0.60994834\n",
            "epoch: 53  - cost: 0.60894805\n",
            "epoch: 54  - cost: 0.6079583\n",
            "epoch: 55  - cost: 0.606979\n",
            "epoch: 56  - cost: 0.6060097\n",
            "epoch: 57  - cost: 0.6050503\n",
            "epoch: 58  - cost: 0.6041005\n",
            "epoch: 59  - cost: 0.60316026\n",
            "epoch: 60  - cost: 0.60222936\n",
            "epoch: 61  - cost: 0.60130745\n",
            "epoch: 62  - cost: 0.6003945\n",
            "epoch: 63  - cost: 0.5994903\n",
            "epoch: 64  - cost: 0.5985946\n",
            "epoch: 65  - cost: 0.5977075\n",
            "epoch: 66  - cost: 0.5968285\n",
            "epoch: 67  - cost: 0.5959577\n",
            "epoch: 68  - cost: 0.5950949\n",
            "epoch: 69  - cost: 0.59423983\n",
            "epoch: 70  - cost: 0.59339255\n",
            "epoch: 71  - cost: 0.59255284\n",
            "epoch: 72  - cost: 0.5917205\n",
            "epoch: 73  - cost: 0.59089553\n",
            "epoch: 74  - cost: 0.59007776\n",
            "epoch: 75  - cost: 0.5892671\n",
            "epoch: 76  - cost: 0.58846337\n",
            "epoch: 77  - cost: 0.58766645\n",
            "epoch: 78  - cost: 0.58687633\n",
            "epoch: 79  - cost: 0.5860928\n",
            "epoch: 80  - cost: 0.5853159\n",
            "epoch: 81  - cost: 0.58454543\n",
            "epoch: 82  - cost: 0.58378136\n",
            "epoch: 83  - cost: 0.5830235\n",
            "epoch: 84  - cost: 0.58227175\n",
            "epoch: 85  - cost: 0.58152616\n",
            "epoch: 86  - cost: 0.5807865\n",
            "epoch: 87  - cost: 0.5800528\n",
            "epoch: 88  - cost: 0.57932496\n",
            "epoch: 89  - cost: 0.5786029\n",
            "epoch: 90  - cost: 0.5778864\n",
            "epoch: 91  - cost: 0.57717556\n",
            "epoch: 92  - cost: 0.5764702\n",
            "epoch: 93  - cost: 0.5757704\n",
            "epoch: 94  - cost: 0.57507586\n",
            "epoch: 95  - cost: 0.5743867\n",
            "epoch: 96  - cost: 0.57370275\n",
            "epoch: 97  - cost: 0.57302403\n",
            "epoch: 98  - cost: 0.5723504\n",
            "epoch: 99  - cost: 0.5716818\n",
            "epoch: 100  - cost: 0.5710182\n",
            "epoch: 101  - cost: 0.57035947\n",
            "epoch: 102  - cost: 0.5697056\n",
            "epoch: 103  - cost: 0.56905663\n",
            "epoch: 104  - cost: 0.5684125\n",
            "epoch: 105  - cost: 0.56777287\n",
            "epoch: 106  - cost: 0.56713796\n",
            "epoch: 107  - cost: 0.5665076\n",
            "epoch: 108  - cost: 0.5658818\n",
            "epoch: 109  - cost: 0.5652605\n",
            "epoch: 110  - cost: 0.56464356\n",
            "epoch: 111  - cost: 0.56403106\n",
            "epoch: 112  - cost: 0.56342286\n",
            "epoch: 113  - cost: 0.5628189\n",
            "epoch: 114  - cost: 0.56221926\n",
            "epoch: 115  - cost: 0.56162375\n",
            "epoch: 116  - cost: 0.56103235\n",
            "epoch: 117  - cost: 0.56044513\n",
            "epoch: 118  - cost: 0.5598619\n",
            "epoch: 119  - cost: 0.5592828\n",
            "epoch: 120  - cost: 0.5587075\n",
            "epoch: 121  - cost: 0.5581361\n",
            "epoch: 122  - cost: 0.5575686\n",
            "epoch: 123  - cost: 0.557005\n",
            "epoch: 124  - cost: 0.55644524\n",
            "epoch: 125  - cost: 0.5558891\n",
            "epoch: 126  - cost: 0.55533683\n",
            "epoch: 127  - cost: 0.5547881\n",
            "epoch: 128  - cost: 0.5542431\n",
            "epoch: 129  - cost: 0.55370164\n",
            "epoch: 130  - cost: 0.55316377\n",
            "epoch: 131  - cost: 0.5526294\n",
            "epoch: 132  - cost: 0.5520986\n",
            "epoch: 133  - cost: 0.5515712\n",
            "epoch: 134  - cost: 0.5510472\n",
            "epoch: 135  - cost: 0.5505266\n",
            "epoch: 136  - cost: 0.5500095\n",
            "epoch: 137  - cost: 0.5494956\n",
            "epoch: 138  - cost: 0.5489849\n",
            "epoch: 139  - cost: 0.5484776\n",
            "epoch: 140  - cost: 0.54797345\n",
            "epoch: 141  - cost: 0.5474725\n",
            "epoch: 142  - cost: 0.5469747\n",
            "epoch: 143  - cost: 0.54648\n",
            "epoch: 144  - cost: 0.5459885\n",
            "epoch: 145  - cost: 0.5455\n",
            "epoch: 146  - cost: 0.54501456\n",
            "epoch: 147  - cost: 0.5445321\n",
            "epoch: 148  - cost: 0.54405266\n",
            "epoch: 149  - cost: 0.5435761\n",
            "epoch: 150  - cost: 0.5431026\n",
            "epoch: 151  - cost: 0.5426319\n",
            "epoch: 152  - cost: 0.5421641\n",
            "epoch: 153  - cost: 0.54169923\n",
            "epoch: 154  - cost: 0.541237\n",
            "epoch: 155  - cost: 0.5407777\n",
            "epoch: 156  - cost: 0.5403211\n",
            "epoch: 157  - cost: 0.5398672\n",
            "epoch: 158  - cost: 0.5394161\n",
            "epoch: 159  - cost: 0.5389676\n",
            "epoch: 160  - cost: 0.5385219\n",
            "epoch: 161  - cost: 0.5380787\n",
            "epoch: 162  - cost: 0.53763825\n",
            "epoch: 163  - cost: 0.5372003\n",
            "epoch: 164  - cost: 0.53676486\n",
            "epoch: 165  - cost: 0.5363321\n",
            "epoch: 166  - cost: 0.5359017\n",
            "epoch: 167  - cost: 0.53547394\n",
            "epoch: 168  - cost: 0.53504866\n",
            "epoch: 169  - cost: 0.5346257\n",
            "epoch: 170  - cost: 0.53420526\n",
            "epoch: 171  - cost: 0.53378725\n",
            "epoch: 172  - cost: 0.5333716\n",
            "epoch: 173  - cost: 0.5329583\n",
            "epoch: 174  - cost: 0.53254735\n",
            "epoch: 175  - cost: 0.53213876\n",
            "epoch: 176  - cost: 0.5317325\n",
            "epoch: 177  - cost: 0.53132844\n",
            "epoch: 178  - cost: 0.5309267\n",
            "epoch: 179  - cost: 0.53052723\n",
            "epoch: 180  - cost: 0.53012997\n",
            "epoch: 181  - cost: 0.5297349\n",
            "epoch: 182  - cost: 0.529342\n",
            "epoch: 183  - cost: 0.5289513\n",
            "epoch: 184  - cost: 0.5285627\n",
            "epoch: 185  - cost: 0.5281763\n",
            "epoch: 186  - cost: 0.527792\n",
            "epoch: 187  - cost: 0.5274098\n",
            "epoch: 188  - cost: 0.52702963\n",
            "epoch: 189  - cost: 0.5266515\n",
            "epoch: 190  - cost: 0.5262755\n",
            "epoch: 191  - cost: 0.5259015\n",
            "epoch: 192  - cost: 0.5255295\n",
            "epoch: 193  - cost: 0.5251595\n",
            "epoch: 194  - cost: 0.5247914\n",
            "epoch: 195  - cost: 0.5244253\n",
            "epoch: 196  - cost: 0.5240612\n",
            "epoch: 197  - cost: 0.523699\n",
            "epoch: 198  - cost: 0.5233387\n",
            "epoch: 199  - cost: 0.5229803\n",
            "epoch: 200  - cost: 0.5226238\n",
            "epoch: 201  - cost: 0.52226907\n",
            "epoch: 202  - cost: 0.5219163\n",
            "epoch: 203  - cost: 0.52156526\n",
            "epoch: 204  - cost: 0.5212161\n",
            "epoch: 205  - cost: 0.5208687\n",
            "epoch: 206  - cost: 0.52052313\n",
            "epoch: 207  - cost: 0.52017933\n",
            "epoch: 208  - cost: 0.51983726\n",
            "epoch: 209  - cost: 0.5194969\n",
            "epoch: 210  - cost: 0.51915836\n",
            "epoch: 211  - cost: 0.51882154\n",
            "epoch: 212  - cost: 0.5184863\n",
            "epoch: 213  - cost: 0.5181529\n",
            "epoch: 214  - cost: 0.5178211\n",
            "epoch: 215  - cost: 0.517491\n",
            "epoch: 216  - cost: 0.5171625\n",
            "epoch: 217  - cost: 0.5168357\n",
            "epoch: 218  - cost: 0.5165104\n",
            "epoch: 219  - cost: 0.5161868\n",
            "epoch: 220  - cost: 0.5158648\n",
            "epoch: 221  - cost: 0.5155444\n",
            "epoch: 222  - cost: 0.51522547\n",
            "epoch: 223  - cost: 0.5149082\n",
            "epoch: 224  - cost: 0.51459247\n",
            "epoch: 225  - cost: 0.51427823\n",
            "epoch: 226  - cost: 0.5139656\n",
            "epoch: 227  - cost: 0.5136545\n",
            "epoch: 228  - cost: 0.51334476\n",
            "epoch: 229  - cost: 0.51303655\n",
            "epoch: 230  - cost: 0.5127298\n",
            "epoch: 231  - cost: 0.51242465\n",
            "epoch: 232  - cost: 0.51212084\n",
            "epoch: 233  - cost: 0.51181847\n",
            "epoch: 234  - cost: 0.5115175\n",
            "epoch: 235  - cost: 0.51121813\n",
            "epoch: 236  - cost: 0.51092\n",
            "epoch: 237  - cost: 0.51062334\n",
            "epoch: 238  - cost: 0.51032805\n",
            "epoch: 239  - cost: 0.5100342\n",
            "epoch: 240  - cost: 0.50974166\n",
            "epoch: 241  - cost: 0.50945044\n",
            "epoch: 242  - cost: 0.5091606\n",
            "epoch: 243  - cost: 0.5088721\n",
            "epoch: 244  - cost: 0.508585\n",
            "epoch: 245  - cost: 0.5082991\n",
            "epoch: 246  - cost: 0.50801456\n",
            "epoch: 247  - cost: 0.5077314\n",
            "epoch: 248  - cost: 0.50744945\n",
            "epoch: 249  - cost: 0.5071688\n",
            "epoch: 250  - cost: 0.5068894\n",
            "epoch: 251  - cost: 0.5066113\n",
            "epoch: 252  - cost: 0.50633436\n",
            "epoch: 253  - cost: 0.50605875\n",
            "epoch: 254  - cost: 0.5057844\n",
            "epoch: 255  - cost: 0.5055112\n",
            "epoch: 256  - cost: 0.50523925\n",
            "epoch: 257  - cost: 0.50496846\n",
            "epoch: 258  - cost: 0.50469893\n",
            "epoch: 259  - cost: 0.5044306\n",
            "epoch: 260  - cost: 0.5041634\n",
            "epoch: 261  - cost: 0.5038974\n",
            "epoch: 262  - cost: 0.50363255\n",
            "epoch: 263  - cost: 0.50336885\n",
            "epoch: 264  - cost: 0.50310636\n",
            "epoch: 265  - cost: 0.50284487\n",
            "epoch: 266  - cost: 0.50258464\n",
            "epoch: 267  - cost: 0.50232553\n",
            "epoch: 268  - cost: 0.5020675\n",
            "epoch: 269  - cost: 0.50181055\n",
            "epoch: 270  - cost: 0.50155485\n",
            "epoch: 271  - cost: 0.50130004\n",
            "epoch: 272  - cost: 0.50104636\n",
            "epoch: 273  - cost: 0.50079393\n",
            "epoch: 274  - cost: 0.5005424\n",
            "epoch: 275  - cost: 0.50029194\n",
            "epoch: 276  - cost: 0.50004256\n",
            "epoch: 277  - cost: 0.49979427\n",
            "epoch: 278  - cost: 0.49954703\n",
            "epoch: 279  - cost: 0.49930072\n",
            "epoch: 280  - cost: 0.49905553\n",
            "epoch: 281  - cost: 0.49881133\n",
            "epoch: 282  - cost: 0.49856812\n",
            "epoch: 283  - cost: 0.49832597\n",
            "epoch: 284  - cost: 0.49808475\n",
            "epoch: 285  - cost: 0.49784458\n",
            "epoch: 286  - cost: 0.49760535\n",
            "epoch: 287  - cost: 0.4973671\n",
            "epoch: 288  - cost: 0.49712986\n",
            "epoch: 289  - cost: 0.49689358\n",
            "epoch: 290  - cost: 0.49665824\n",
            "epoch: 291  - cost: 0.49642384\n",
            "epoch: 292  - cost: 0.4961904\n",
            "epoch: 293  - cost: 0.495958\n",
            "epoch: 294  - cost: 0.49572638\n",
            "epoch: 295  - cost: 0.4954958\n",
            "epoch: 296  - cost: 0.49526602\n",
            "epoch: 297  - cost: 0.4950373\n",
            "epoch: 298  - cost: 0.49480933\n",
            "epoch: 299  - cost: 0.49458236\n",
            "epoch: 300  - cost: 0.49435627\n",
            "epoch: 301  - cost: 0.4941311\n",
            "epoch: 302  - cost: 0.4939068\n",
            "epoch: 303  - cost: 0.49368337\n",
            "epoch: 304  - cost: 0.49346074\n",
            "epoch: 305  - cost: 0.49323907\n",
            "epoch: 306  - cost: 0.49301827\n",
            "epoch: 307  - cost: 0.4927982\n",
            "epoch: 308  - cost: 0.4925792\n",
            "epoch: 309  - cost: 0.49236095\n",
            "epoch: 310  - cost: 0.49214342\n",
            "epoch: 311  - cost: 0.49192685\n",
            "epoch: 312  - cost: 0.4917111\n",
            "epoch: 313  - cost: 0.49149615\n",
            "epoch: 314  - cost: 0.49128202\n",
            "epoch: 315  - cost: 0.49106875\n",
            "epoch: 316  - cost: 0.49085623\n",
            "epoch: 317  - cost: 0.4906445\n",
            "epoch: 318  - cost: 0.4904336\n",
            "epoch: 319  - cost: 0.4902235\n",
            "epoch: 320  - cost: 0.49001423\n",
            "epoch: 321  - cost: 0.48980564\n",
            "epoch: 322  - cost: 0.48959795\n",
            "epoch: 323  - cost: 0.48939088\n",
            "epoch: 324  - cost: 0.4891847\n",
            "epoch: 325  - cost: 0.48897922\n",
            "epoch: 326  - cost: 0.48877463\n",
            "epoch: 327  - cost: 0.48857066\n",
            "epoch: 328  - cost: 0.48836744\n",
            "epoch: 329  - cost: 0.48816505\n",
            "epoch: 330  - cost: 0.48796326\n",
            "epoch: 331  - cost: 0.4877623\n",
            "epoch: 332  - cost: 0.4875621\n",
            "epoch: 333  - cost: 0.48736262\n",
            "epoch: 334  - cost: 0.4871638\n",
            "epoch: 335  - cost: 0.48696575\n",
            "epoch: 336  - cost: 0.48676842\n",
            "epoch: 337  - cost: 0.48657176\n",
            "epoch: 338  - cost: 0.48637584\n",
            "epoch: 339  - cost: 0.48618054\n",
            "epoch: 340  - cost: 0.48598605\n",
            "epoch: 341  - cost: 0.48579222\n",
            "epoch: 342  - cost: 0.48559904\n",
            "epoch: 343  - cost: 0.48540655\n",
            "epoch: 344  - cost: 0.48521474\n",
            "epoch: 345  - cost: 0.48502365\n",
            "epoch: 346  - cost: 0.48483324\n",
            "epoch: 347  - cost: 0.48464337\n",
            "epoch: 348  - cost: 0.4844543\n",
            "epoch: 349  - cost: 0.48426586\n",
            "epoch: 350  - cost: 0.48407805\n",
            "epoch: 351  - cost: 0.4838909\n",
            "epoch: 352  - cost: 0.48370436\n",
            "epoch: 353  - cost: 0.4835185\n",
            "epoch: 354  - cost: 0.48333332\n",
            "epoch: 355  - cost: 0.48314875\n",
            "epoch: 356  - cost: 0.48296475\n",
            "epoch: 357  - cost: 0.48278153\n",
            "epoch: 358  - cost: 0.48259878\n",
            "epoch: 359  - cost: 0.48241675\n",
            "epoch: 360  - cost: 0.4822353\n",
            "epoch: 361  - cost: 0.48205447\n",
            "epoch: 362  - cost: 0.48187423\n",
            "epoch: 363  - cost: 0.48169458\n",
            "epoch: 364  - cost: 0.4815156\n",
            "epoch: 365  - cost: 0.48133722\n",
            "epoch: 366  - cost: 0.48115933\n",
            "epoch: 367  - cost: 0.48098218\n",
            "epoch: 368  - cost: 0.48080555\n",
            "epoch: 369  - cost: 0.48062953\n",
            "epoch: 370  - cost: 0.48045406\n",
            "epoch: 371  - cost: 0.48027918\n",
            "epoch: 372  - cost: 0.48010486\n",
            "epoch: 373  - cost: 0.47993115\n",
            "epoch: 374  - cost: 0.47975796\n",
            "epoch: 375  - cost: 0.4795854\n",
            "epoch: 376  - cost: 0.4794134\n",
            "epoch: 377  - cost: 0.47924194\n",
            "epoch: 378  - cost: 0.479071\n",
            "epoch: 379  - cost: 0.47890064\n",
            "epoch: 380  - cost: 0.47873086\n",
            "epoch: 381  - cost: 0.47856164\n",
            "epoch: 382  - cost: 0.4783929\n",
            "epoch: 383  - cost: 0.47822472\n",
            "epoch: 384  - cost: 0.47805718\n",
            "epoch: 385  - cost: 0.47789\n",
            "epoch: 386  - cost: 0.4777235\n",
            "epoch: 387  - cost: 0.47755748\n",
            "epoch: 388  - cost: 0.477392\n",
            "epoch: 389  - cost: 0.477227\n",
            "epoch: 390  - cost: 0.4770626\n",
            "epoch: 391  - cost: 0.4768987\n",
            "epoch: 392  - cost: 0.47673523\n",
            "epoch: 393  - cost: 0.4765723\n",
            "epoch: 394  - cost: 0.47640997\n",
            "epoch: 395  - cost: 0.4762481\n",
            "epoch: 396  - cost: 0.4760867\n",
            "epoch: 397  - cost: 0.4759259\n",
            "epoch: 398  - cost: 0.4757655\n",
            "epoch: 399  - cost: 0.47560564\n",
            "epoch: 400  - cost: 0.47544625\n",
            "epoch: 401  - cost: 0.47528738\n",
            "epoch: 402  - cost: 0.47512892\n",
            "epoch: 403  - cost: 0.474971\n",
            "epoch: 404  - cost: 0.4748136\n",
            "epoch: 405  - cost: 0.47465667\n",
            "epoch: 406  - cost: 0.4745002\n",
            "epoch: 407  - cost: 0.47434428\n",
            "epoch: 408  - cost: 0.4741887\n",
            "epoch: 409  - cost: 0.4740337\n",
            "epoch: 410  - cost: 0.47387913\n",
            "epoch: 411  - cost: 0.47372502\n",
            "epoch: 412  - cost: 0.4735713\n",
            "epoch: 413  - cost: 0.47341812\n",
            "epoch: 414  - cost: 0.4732654\n",
            "epoch: 415  - cost: 0.47311315\n",
            "epoch: 416  - cost: 0.47296134\n",
            "epoch: 417  - cost: 0.47281\n",
            "epoch: 418  - cost: 0.47265902\n",
            "epoch: 419  - cost: 0.4725086\n",
            "epoch: 420  - cost: 0.47235852\n",
            "epoch: 421  - cost: 0.47220904\n",
            "epoch: 422  - cost: 0.47205982\n",
            "epoch: 423  - cost: 0.47191116\n",
            "epoch: 424  - cost: 0.47176284\n",
            "epoch: 425  - cost: 0.47161505\n",
            "epoch: 426  - cost: 0.47146767\n",
            "epoch: 427  - cost: 0.4713207\n",
            "epoch: 428  - cost: 0.47117412\n",
            "epoch: 429  - cost: 0.471028\n",
            "epoch: 430  - cost: 0.4708823\n",
            "epoch: 431  - cost: 0.470737\n",
            "epoch: 432  - cost: 0.4705922\n",
            "epoch: 433  - cost: 0.4704477\n",
            "epoch: 434  - cost: 0.4703037\n",
            "epoch: 435  - cost: 0.4701601\n",
            "epoch: 436  - cost: 0.4700169\n",
            "epoch: 437  - cost: 0.46987405\n",
            "epoch: 438  - cost: 0.46973175\n",
            "epoch: 439  - cost: 0.4695897\n",
            "epoch: 440  - cost: 0.46944812\n",
            "epoch: 441  - cost: 0.4693069\n",
            "epoch: 442  - cost: 0.46916616\n",
            "epoch: 443  - cost: 0.46902576\n",
            "epoch: 444  - cost: 0.46888575\n",
            "epoch: 445  - cost: 0.46874613\n",
            "epoch: 446  - cost: 0.46860695\n",
            "epoch: 447  - cost: 0.46846807\n",
            "epoch: 448  - cost: 0.4683296\n",
            "epoch: 449  - cost: 0.46819156\n",
            "epoch: 450  - cost: 0.46805388\n",
            "epoch: 451  - cost: 0.46791658\n",
            "epoch: 452  - cost: 0.46777964\n",
            "epoch: 453  - cost: 0.46764314\n",
            "epoch: 454  - cost: 0.467507\n",
            "epoch: 455  - cost: 0.4673712\n",
            "epoch: 456  - cost: 0.4672357\n",
            "epoch: 457  - cost: 0.4671007\n",
            "epoch: 458  - cost: 0.46696606\n",
            "epoch: 459  - cost: 0.46683168\n",
            "epoch: 460  - cost: 0.46669775\n",
            "epoch: 461  - cost: 0.46656415\n",
            "epoch: 462  - cost: 0.46643093\n",
            "epoch: 463  - cost: 0.46629804\n",
            "epoch: 464  - cost: 0.46616554\n",
            "epoch: 465  - cost: 0.46603334\n",
            "epoch: 466  - cost: 0.46590155\n",
            "epoch: 467  - cost: 0.4657701\n",
            "epoch: 468  - cost: 0.46563897\n",
            "epoch: 469  - cost: 0.4655082\n",
            "epoch: 470  - cost: 0.4653778\n",
            "epoch: 471  - cost: 0.46524784\n",
            "epoch: 472  - cost: 0.46511808\n",
            "epoch: 473  - cost: 0.4649887\n",
            "epoch: 474  - cost: 0.46485966\n",
            "epoch: 475  - cost: 0.46473092\n",
            "epoch: 476  - cost: 0.46460256\n",
            "epoch: 477  - cost: 0.46447453\n",
            "epoch: 478  - cost: 0.46434683\n",
            "epoch: 479  - cost: 0.46421954\n",
            "epoch: 480  - cost: 0.46409246\n",
            "epoch: 481  - cost: 0.4639658\n",
            "epoch: 482  - cost: 0.4638394\n",
            "epoch: 483  - cost: 0.4637134\n",
            "epoch: 484  - cost: 0.46358767\n",
            "epoch: 485  - cost: 0.46346223\n",
            "epoch: 486  - cost: 0.46333715\n",
            "epoch: 487  - cost: 0.4632125\n",
            "epoch: 488  - cost: 0.46308798\n",
            "epoch: 489  - cost: 0.4629639\n",
            "epoch: 490  - cost: 0.46284014\n",
            "epoch: 491  - cost: 0.46271664\n",
            "epoch: 492  - cost: 0.46259344\n",
            "epoch: 493  - cost: 0.46247065\n",
            "epoch: 494  - cost: 0.4623481\n",
            "epoch: 495  - cost: 0.46222585\n",
            "epoch: 496  - cost: 0.46210393\n",
            "epoch: 497  - cost: 0.46198225\n",
            "epoch: 498  - cost: 0.46186092\n",
            "epoch: 499  - cost: 0.46173993\n",
            "epoch: 500  - cost: 0.46161926\n",
            "epoch: 501  - cost: 0.46149883\n",
            "epoch: 502  - cost: 0.46137872\n",
            "epoch: 503  - cost: 0.46125886\n",
            "epoch: 504  - cost: 0.46113938\n",
            "epoch: 505  - cost: 0.46102017\n",
            "epoch: 506  - cost: 0.4609012\n",
            "epoch: 507  - cost: 0.46078265\n",
            "epoch: 508  - cost: 0.46066427\n",
            "epoch: 509  - cost: 0.4605462\n",
            "epoch: 510  - cost: 0.46042845\n",
            "epoch: 511  - cost: 0.46031097\n",
            "epoch: 512  - cost: 0.46019378\n",
            "epoch: 513  - cost: 0.4600769\n",
            "epoch: 514  - cost: 0.45996028\n",
            "epoch: 515  - cost: 0.45984396\n",
            "epoch: 516  - cost: 0.45972785\n",
            "epoch: 517  - cost: 0.45961207\n",
            "epoch: 518  - cost: 0.45949662\n",
            "epoch: 519  - cost: 0.45938137\n",
            "epoch: 520  - cost: 0.45926648\n",
            "epoch: 521  - cost: 0.4591518\n",
            "epoch: 522  - cost: 0.45903742\n",
            "epoch: 523  - cost: 0.45892325\n",
            "epoch: 524  - cost: 0.45880947\n",
            "epoch: 525  - cost: 0.4586959\n",
            "epoch: 526  - cost: 0.45858255\n",
            "epoch: 527  - cost: 0.45846954\n",
            "epoch: 528  - cost: 0.45835677\n",
            "epoch: 529  - cost: 0.45824426\n",
            "epoch: 530  - cost: 0.458132\n",
            "epoch: 531  - cost: 0.45802006\n",
            "epoch: 532  - cost: 0.4579084\n",
            "epoch: 533  - cost: 0.45779693\n",
            "epoch: 534  - cost: 0.45768568\n",
            "epoch: 535  - cost: 0.45757475\n",
            "epoch: 536  - cost: 0.45746413\n",
            "epoch: 537  - cost: 0.4573537\n",
            "epoch: 538  - cost: 0.45724353\n",
            "epoch: 539  - cost: 0.45713362\n",
            "epoch: 540  - cost: 0.45702395\n",
            "epoch: 541  - cost: 0.4569146\n",
            "epoch: 542  - cost: 0.45680547\n",
            "epoch: 543  - cost: 0.45669657\n",
            "epoch: 544  - cost: 0.4565879\n",
            "epoch: 545  - cost: 0.45647952\n",
            "epoch: 546  - cost: 0.45637137\n",
            "epoch: 547  - cost: 0.45626345\n",
            "epoch: 548  - cost: 0.45615587\n",
            "epoch: 549  - cost: 0.4560484\n",
            "epoch: 550  - cost: 0.45594123\n",
            "epoch: 551  - cost: 0.4558343\n",
            "epoch: 552  - cost: 0.45572764\n",
            "epoch: 553  - cost: 0.45562124\n",
            "epoch: 554  - cost: 0.455515\n",
            "epoch: 555  - cost: 0.45540902\n",
            "epoch: 556  - cost: 0.45530334\n",
            "epoch: 557  - cost: 0.45519778\n",
            "epoch: 558  - cost: 0.4550926\n",
            "epoch: 559  - cost: 0.45498756\n",
            "epoch: 560  - cost: 0.4548828\n",
            "epoch: 561  - cost: 0.45477822\n",
            "epoch: 562  - cost: 0.45467395\n",
            "epoch: 563  - cost: 0.45456988\n",
            "epoch: 564  - cost: 0.45446596\n",
            "epoch: 565  - cost: 0.45436236\n",
            "epoch: 566  - cost: 0.45425895\n",
            "epoch: 567  - cost: 0.45415574\n",
            "epoch: 568  - cost: 0.45405287\n",
            "epoch: 569  - cost: 0.45395008\n",
            "epoch: 570  - cost: 0.45384762\n",
            "epoch: 571  - cost: 0.4537453\n",
            "epoch: 572  - cost: 0.45364323\n",
            "epoch: 573  - cost: 0.45354146\n",
            "epoch: 574  - cost: 0.4534398\n",
            "epoch: 575  - cost: 0.45333847\n",
            "epoch: 576  - cost: 0.45323727\n",
            "epoch: 577  - cost: 0.45313638\n",
            "epoch: 578  - cost: 0.45303565\n",
            "epoch: 579  - cost: 0.4529351\n",
            "epoch: 580  - cost: 0.45283484\n",
            "epoch: 581  - cost: 0.45273474\n",
            "epoch: 582  - cost: 0.45263487\n",
            "epoch: 583  - cost: 0.45253527\n",
            "epoch: 584  - cost: 0.45243573\n",
            "epoch: 585  - cost: 0.4523365\n",
            "epoch: 586  - cost: 0.4522375\n",
            "epoch: 587  - cost: 0.45213872\n",
            "epoch: 588  - cost: 0.4520401\n",
            "epoch: 589  - cost: 0.4519416\n",
            "epoch: 590  - cost: 0.4518435\n",
            "epoch: 591  - cost: 0.45174557\n",
            "epoch: 592  - cost: 0.45164776\n",
            "epoch: 593  - cost: 0.4515502\n",
            "epoch: 594  - cost: 0.45145282\n",
            "epoch: 595  - cost: 0.45135564\n",
            "epoch: 596  - cost: 0.45125872\n",
            "epoch: 597  - cost: 0.45116198\n",
            "epoch: 598  - cost: 0.4510654\n",
            "epoch: 599  - cost: 0.45096904\n",
            "epoch: 600  - cost: 0.4508729\n",
            "epoch: 601  - cost: 0.4507769\n",
            "epoch: 602  - cost: 0.45068118\n",
            "epoch: 603  - cost: 0.4505856\n",
            "epoch: 604  - cost: 0.45049027\n",
            "epoch: 605  - cost: 0.4503951\n",
            "epoch: 606  - cost: 0.45030013\n",
            "epoch: 607  - cost: 0.45020536\n",
            "epoch: 608  - cost: 0.4501107\n",
            "epoch: 609  - cost: 0.45001632\n",
            "epoch: 610  - cost: 0.44992214\n",
            "epoch: 611  - cost: 0.44982818\n",
            "epoch: 612  - cost: 0.4497343\n",
            "epoch: 613  - cost: 0.44964072\n",
            "epoch: 614  - cost: 0.44954723\n",
            "epoch: 615  - cost: 0.449454\n",
            "epoch: 616  - cost: 0.44936088\n",
            "epoch: 617  - cost: 0.44926807\n",
            "epoch: 618  - cost: 0.44917542\n",
            "epoch: 619  - cost: 0.44908294\n",
            "epoch: 620  - cost: 0.44899064\n",
            "epoch: 621  - cost: 0.44889846\n",
            "epoch: 622  - cost: 0.44880652\n",
            "epoch: 623  - cost: 0.4487148\n",
            "epoch: 624  - cost: 0.44862315\n",
            "epoch: 625  - cost: 0.44853178\n",
            "epoch: 626  - cost: 0.44844055\n",
            "epoch: 627  - cost: 0.44834957\n",
            "epoch: 628  - cost: 0.44825864\n",
            "epoch: 629  - cost: 0.44816798\n",
            "epoch: 630  - cost: 0.4480775\n",
            "epoch: 631  - cost: 0.44798714\n",
            "epoch: 632  - cost: 0.44789705\n",
            "epoch: 633  - cost: 0.44780704\n",
            "epoch: 634  - cost: 0.44771725\n",
            "epoch: 635  - cost: 0.44762763\n",
            "epoch: 636  - cost: 0.44753817\n",
            "epoch: 637  - cost: 0.4474489\n",
            "epoch: 638  - cost: 0.4473598\n",
            "epoch: 639  - cost: 0.4472709\n",
            "epoch: 640  - cost: 0.44718212\n",
            "epoch: 641  - cost: 0.44709358\n",
            "epoch: 642  - cost: 0.44700515\n",
            "epoch: 643  - cost: 0.44691694\n",
            "epoch: 644  - cost: 0.44682884\n",
            "epoch: 645  - cost: 0.44674096\n",
            "epoch: 646  - cost: 0.4466532\n",
            "epoch: 647  - cost: 0.44656572\n",
            "epoch: 648  - cost: 0.44647828\n",
            "epoch: 649  - cost: 0.44639102\n",
            "epoch: 650  - cost: 0.446304\n",
            "epoch: 651  - cost: 0.44621712\n",
            "epoch: 652  - cost: 0.44613034\n",
            "epoch: 653  - cost: 0.44604376\n",
            "epoch: 654  - cost: 0.4459574\n",
            "epoch: 655  - cost: 0.44587114\n",
            "epoch: 656  - cost: 0.44578505\n",
            "epoch: 657  - cost: 0.44569916\n",
            "epoch: 658  - cost: 0.44561338\n",
            "epoch: 659  - cost: 0.4455278\n",
            "epoch: 660  - cost: 0.44544238\n",
            "epoch: 661  - cost: 0.4453571\n",
            "epoch: 662  - cost: 0.44527203\n",
            "epoch: 663  - cost: 0.445187\n",
            "epoch: 664  - cost: 0.4451022\n",
            "epoch: 665  - cost: 0.44501758\n",
            "epoch: 666  - cost: 0.44493315\n",
            "epoch: 667  - cost: 0.44484878\n",
            "epoch: 668  - cost: 0.44476467\n",
            "epoch: 669  - cost: 0.4446806\n",
            "epoch: 670  - cost: 0.44459674\n",
            "epoch: 671  - cost: 0.44451302\n",
            "epoch: 672  - cost: 0.44442952\n",
            "epoch: 673  - cost: 0.44434607\n",
            "epoch: 674  - cost: 0.4442628\n",
            "epoch: 675  - cost: 0.4441798\n",
            "epoch: 676  - cost: 0.44409683\n",
            "epoch: 677  - cost: 0.444014\n",
            "epoch: 678  - cost: 0.4439314\n",
            "epoch: 679  - cost: 0.4438489\n",
            "epoch: 680  - cost: 0.44376656\n",
            "epoch: 681  - cost: 0.44368434\n",
            "epoch: 682  - cost: 0.44360235\n",
            "epoch: 683  - cost: 0.44352037\n",
            "epoch: 684  - cost: 0.4434387\n",
            "epoch: 685  - cost: 0.4433571\n",
            "epoch: 686  - cost: 0.44327563\n",
            "epoch: 687  - cost: 0.44319436\n",
            "epoch: 688  - cost: 0.4431132\n",
            "epoch: 689  - cost: 0.44303215\n",
            "epoch: 690  - cost: 0.44295132\n",
            "epoch: 691  - cost: 0.44287053\n",
            "epoch: 692  - cost: 0.44279\n",
            "epoch: 693  - cost: 0.44270954\n",
            "epoch: 694  - cost: 0.44262928\n",
            "epoch: 695  - cost: 0.44254908\n",
            "epoch: 696  - cost: 0.4424691\n",
            "epoch: 697  - cost: 0.4423892\n",
            "epoch: 698  - cost: 0.44230947\n",
            "epoch: 699  - cost: 0.44222987\n",
            "epoch: 700  - cost: 0.4421504\n",
            "epoch: 701  - cost: 0.4420711\n",
            "epoch: 702  - cost: 0.44199196\n",
            "epoch: 703  - cost: 0.44191295\n",
            "epoch: 704  - cost: 0.441834\n",
            "epoch: 705  - cost: 0.4417552\n",
            "epoch: 706  - cost: 0.44167665\n",
            "epoch: 707  - cost: 0.44159818\n",
            "epoch: 708  - cost: 0.44151992\n",
            "epoch: 709  - cost: 0.44144163\n",
            "epoch: 710  - cost: 0.4413635\n",
            "epoch: 711  - cost: 0.4412856\n",
            "epoch: 712  - cost: 0.4412078\n",
            "epoch: 713  - cost: 0.44113007\n",
            "epoch: 714  - cost: 0.44105262\n",
            "epoch: 715  - cost: 0.44097513\n",
            "epoch: 716  - cost: 0.4408979\n",
            "epoch: 717  - cost: 0.44082078\n",
            "epoch: 718  - cost: 0.44074377\n",
            "epoch: 719  - cost: 0.44066685\n",
            "epoch: 720  - cost: 0.44059014\n",
            "epoch: 721  - cost: 0.44051355\n",
            "epoch: 722  - cost: 0.44043708\n",
            "epoch: 723  - cost: 0.4403607\n",
            "epoch: 724  - cost: 0.4402845\n",
            "epoch: 725  - cost: 0.44020838\n",
            "epoch: 726  - cost: 0.4401324\n",
            "epoch: 727  - cost: 0.4400566\n",
            "epoch: 728  - cost: 0.4399809\n",
            "epoch: 729  - cost: 0.4399053\n",
            "epoch: 730  - cost: 0.4398299\n",
            "epoch: 731  - cost: 0.43975446\n",
            "epoch: 732  - cost: 0.43967927\n",
            "epoch: 733  - cost: 0.43960422\n",
            "epoch: 734  - cost: 0.43952927\n",
            "epoch: 735  - cost: 0.4394544\n",
            "epoch: 736  - cost: 0.43937975\n",
            "epoch: 737  - cost: 0.43930516\n",
            "epoch: 738  - cost: 0.4392307\n",
            "epoch: 739  - cost: 0.43915644\n",
            "epoch: 740  - cost: 0.4390822\n",
            "epoch: 741  - cost: 0.43900812\n",
            "epoch: 742  - cost: 0.43893415\n",
            "epoch: 743  - cost: 0.43886036\n",
            "epoch: 744  - cost: 0.4387866\n",
            "epoch: 745  - cost: 0.43871298\n",
            "epoch: 746  - cost: 0.43863955\n",
            "epoch: 747  - cost: 0.43856612\n",
            "epoch: 748  - cost: 0.43849292\n",
            "epoch: 749  - cost: 0.43841982\n",
            "epoch: 750  - cost: 0.43834677\n",
            "epoch: 751  - cost: 0.43827394\n",
            "epoch: 752  - cost: 0.43820122\n",
            "epoch: 753  - cost: 0.43812853\n",
            "epoch: 754  - cost: 0.43805608\n",
            "epoch: 755  - cost: 0.43798357\n",
            "epoch: 756  - cost: 0.43791133\n",
            "epoch: 757  - cost: 0.43783915\n",
            "epoch: 758  - cost: 0.4377671\n",
            "epoch: 759  - cost: 0.43769518\n",
            "epoch: 760  - cost: 0.43762332\n",
            "epoch: 761  - cost: 0.43755165\n",
            "epoch: 762  - cost: 0.43748006\n",
            "epoch: 763  - cost: 0.43740854\n",
            "epoch: 764  - cost: 0.4373372\n",
            "epoch: 765  - cost: 0.437266\n",
            "epoch: 766  - cost: 0.43719482\n",
            "epoch: 767  - cost: 0.43712375\n",
            "epoch: 768  - cost: 0.43705288\n",
            "epoch: 769  - cost: 0.43698207\n",
            "epoch: 770  - cost: 0.43691143\n",
            "epoch: 771  - cost: 0.43684083\n",
            "epoch: 772  - cost: 0.43677035\n",
            "epoch: 773  - cost: 0.4367\n",
            "epoch: 774  - cost: 0.43662977\n",
            "epoch: 775  - cost: 0.43655968\n",
            "epoch: 776  - cost: 0.43648964\n",
            "epoch: 777  - cost: 0.43641967\n",
            "epoch: 778  - cost: 0.43634996\n",
            "epoch: 779  - cost: 0.43628022\n",
            "epoch: 780  - cost: 0.4362107\n",
            "epoch: 781  - cost: 0.43614122\n",
            "epoch: 782  - cost: 0.4360718\n",
            "epoch: 783  - cost: 0.43600252\n",
            "epoch: 784  - cost: 0.43593338\n",
            "epoch: 785  - cost: 0.43586436\n",
            "epoch: 786  - cost: 0.4357954\n",
            "epoch: 787  - cost: 0.43572664\n",
            "epoch: 788  - cost: 0.4356579\n",
            "epoch: 789  - cost: 0.43558928\n",
            "epoch: 790  - cost: 0.4355208\n",
            "epoch: 791  - cost: 0.43545237\n",
            "epoch: 792  - cost: 0.43538412\n",
            "epoch: 793  - cost: 0.4353159\n",
            "epoch: 794  - cost: 0.4352478\n",
            "epoch: 795  - cost: 0.4351798\n",
            "epoch: 796  - cost: 0.4351119\n",
            "epoch: 797  - cost: 0.43504417\n",
            "epoch: 798  - cost: 0.43497652\n",
            "epoch: 799  - cost: 0.43490896\n",
            "epoch: 800  - cost: 0.4348415\n",
            "epoch: 801  - cost: 0.4347741\n",
            "epoch: 802  - cost: 0.4347068\n",
            "epoch: 803  - cost: 0.43463963\n",
            "epoch: 804  - cost: 0.43457264\n",
            "epoch: 805  - cost: 0.43450567\n",
            "epoch: 806  - cost: 0.4344388\n",
            "epoch: 807  - cost: 0.43437204\n",
            "epoch: 808  - cost: 0.4343054\n",
            "epoch: 809  - cost: 0.43423888\n",
            "epoch: 810  - cost: 0.43417242\n",
            "epoch: 811  - cost: 0.43410602\n",
            "epoch: 812  - cost: 0.43403977\n",
            "epoch: 813  - cost: 0.4339736\n",
            "epoch: 814  - cost: 0.43390757\n",
            "epoch: 815  - cost: 0.43384165\n",
            "epoch: 816  - cost: 0.43377575\n",
            "epoch: 817  - cost: 0.43371004\n",
            "epoch: 818  - cost: 0.4336443\n",
            "epoch: 819  - cost: 0.43357876\n",
            "epoch: 820  - cost: 0.43351325\n",
            "epoch: 821  - cost: 0.43344793\n",
            "epoch: 822  - cost: 0.43338266\n",
            "epoch: 823  - cost: 0.4333174\n",
            "epoch: 824  - cost: 0.43325233\n",
            "epoch: 825  - cost: 0.43318737\n",
            "epoch: 826  - cost: 0.4331225\n",
            "epoch: 827  - cost: 0.4330577\n",
            "epoch: 828  - cost: 0.43299294\n",
            "epoch: 829  - cost: 0.43292832\n",
            "epoch: 830  - cost: 0.43286386\n",
            "epoch: 831  - cost: 0.43279946\n",
            "epoch: 832  - cost: 0.4327351\n",
            "epoch: 833  - cost: 0.43267086\n",
            "epoch: 834  - cost: 0.4326067\n",
            "epoch: 835  - cost: 0.4325427\n",
            "epoch: 836  - cost: 0.4324787\n",
            "epoch: 837  - cost: 0.4324149\n",
            "epoch: 838  - cost: 0.43235108\n",
            "epoch: 839  - cost: 0.43228742\n",
            "epoch: 840  - cost: 0.43222383\n",
            "epoch: 841  - cost: 0.43216035\n",
            "epoch: 842  - cost: 0.4320969\n",
            "epoch: 843  - cost: 0.4320336\n",
            "epoch: 844  - cost: 0.43197045\n",
            "epoch: 845  - cost: 0.4319073\n",
            "epoch: 846  - cost: 0.43184426\n",
            "epoch: 847  - cost: 0.43178123\n",
            "epoch: 848  - cost: 0.43171844\n",
            "epoch: 849  - cost: 0.4316557\n",
            "epoch: 850  - cost: 0.43159294\n",
            "epoch: 851  - cost: 0.43153036\n",
            "epoch: 852  - cost: 0.4314679\n",
            "epoch: 853  - cost: 0.4314055\n",
            "epoch: 854  - cost: 0.43134314\n",
            "epoch: 855  - cost: 0.43128088\n",
            "epoch: 856  - cost: 0.43121874\n",
            "epoch: 857  - cost: 0.4311567\n",
            "epoch: 858  - cost: 0.43109474\n",
            "epoch: 859  - cost: 0.4310328\n",
            "epoch: 860  - cost: 0.43097106\n",
            "epoch: 861  - cost: 0.43090937\n",
            "epoch: 862  - cost: 0.43084773\n",
            "epoch: 863  - cost: 0.43078613\n",
            "epoch: 864  - cost: 0.43072468\n",
            "epoch: 865  - cost: 0.43066332\n",
            "epoch: 866  - cost: 0.43060204\n",
            "epoch: 867  - cost: 0.43054083\n",
            "epoch: 868  - cost: 0.43047976\n",
            "epoch: 869  - cost: 0.43041876\n",
            "epoch: 870  - cost: 0.4303577\n",
            "epoch: 871  - cost: 0.43029687\n",
            "epoch: 872  - cost: 0.43023613\n",
            "epoch: 873  - cost: 0.43017545\n",
            "epoch: 874  - cost: 0.43011484\n",
            "epoch: 875  - cost: 0.43005428\n",
            "epoch: 876  - cost: 0.42999387\n",
            "epoch: 877  - cost: 0.4299335\n",
            "epoch: 878  - cost: 0.42987323\n",
            "epoch: 879  - cost: 0.4298131\n",
            "epoch: 880  - cost: 0.4297529\n",
            "epoch: 881  - cost: 0.42969292\n",
            "epoch: 882  - cost: 0.429633\n",
            "epoch: 883  - cost: 0.42957312\n",
            "epoch: 884  - cost: 0.4295133\n",
            "epoch: 885  - cost: 0.4294536\n",
            "epoch: 886  - cost: 0.42939404\n",
            "epoch: 887  - cost: 0.42933446\n",
            "epoch: 888  - cost: 0.42927504\n",
            "epoch: 889  - cost: 0.42921564\n",
            "epoch: 890  - cost: 0.42915636\n",
            "epoch: 891  - cost: 0.42909712\n",
            "epoch: 892  - cost: 0.429038\n",
            "epoch: 893  - cost: 0.42897895\n",
            "epoch: 894  - cost: 0.42891997\n",
            "epoch: 895  - cost: 0.42886102\n",
            "epoch: 896  - cost: 0.42880222\n",
            "epoch: 897  - cost: 0.42874348\n",
            "epoch: 898  - cost: 0.4286848\n",
            "epoch: 899  - cost: 0.42862627\n",
            "epoch: 900  - cost: 0.4285678\n",
            "epoch: 901  - cost: 0.42850932\n",
            "epoch: 902  - cost: 0.42845103\n",
            "epoch: 903  - cost: 0.4283927\n",
            "epoch: 904  - cost: 0.4283345\n",
            "epoch: 905  - cost: 0.4282764\n",
            "epoch: 906  - cost: 0.42821836\n",
            "epoch: 907  - cost: 0.42816043\n",
            "epoch: 908  - cost: 0.42810252\n",
            "epoch: 909  - cost: 0.42804468\n",
            "epoch: 910  - cost: 0.427987\n",
            "epoch: 911  - cost: 0.4279293\n",
            "epoch: 912  - cost: 0.42787176\n",
            "epoch: 913  - cost: 0.42781428\n",
            "epoch: 914  - cost: 0.4277568\n",
            "epoch: 915  - cost: 0.42769948\n",
            "epoch: 916  - cost: 0.42764223\n",
            "epoch: 917  - cost: 0.42758504\n",
            "epoch: 918  - cost: 0.42752787\n",
            "epoch: 919  - cost: 0.4274708\n",
            "epoch: 920  - cost: 0.42741385\n",
            "epoch: 921  - cost: 0.42735687\n",
            "epoch: 922  - cost: 0.4273001\n",
            "epoch: 923  - cost: 0.42724326\n",
            "epoch: 924  - cost: 0.42718664\n",
            "epoch: 925  - cost: 0.42713004\n",
            "epoch: 926  - cost: 0.42707348\n",
            "epoch: 927  - cost: 0.42701697\n",
            "epoch: 928  - cost: 0.42696065\n",
            "epoch: 929  - cost: 0.42690438\n",
            "epoch: 930  - cost: 0.42684808\n",
            "epoch: 931  - cost: 0.42679194\n",
            "epoch: 932  - cost: 0.4267358\n",
            "epoch: 933  - cost: 0.42667982\n",
            "epoch: 934  - cost: 0.42662382\n",
            "epoch: 935  - cost: 0.42656794\n",
            "epoch: 936  - cost: 0.4265121\n",
            "epoch: 937  - cost: 0.4264564\n",
            "epoch: 938  - cost: 0.42640072\n",
            "epoch: 939  - cost: 0.42634514\n",
            "epoch: 940  - cost: 0.4262896\n",
            "epoch: 941  - cost: 0.42623416\n",
            "epoch: 942  - cost: 0.42617875\n",
            "epoch: 943  - cost: 0.4261234\n",
            "epoch: 944  - cost: 0.42606816\n",
            "epoch: 945  - cost: 0.42601305\n",
            "epoch: 946  - cost: 0.42595792\n",
            "epoch: 947  - cost: 0.4259028\n",
            "epoch: 948  - cost: 0.4258479\n",
            "epoch: 949  - cost: 0.425793\n",
            "epoch: 950  - cost: 0.42573822\n",
            "epoch: 951  - cost: 0.42568335\n",
            "epoch: 952  - cost: 0.42562872\n",
            "epoch: 953  - cost: 0.4255741\n",
            "epoch: 954  - cost: 0.42551953\n",
            "epoch: 955  - cost: 0.42546508\n",
            "epoch: 956  - cost: 0.4254106\n",
            "epoch: 957  - cost: 0.42535627\n",
            "epoch: 958  - cost: 0.425302\n",
            "epoch: 959  - cost: 0.4252478\n",
            "epoch: 960  - cost: 0.4251936\n",
            "epoch: 961  - cost: 0.4251396\n",
            "epoch: 962  - cost: 0.42508554\n",
            "epoch: 963  - cost: 0.42503163\n",
            "epoch: 964  - cost: 0.42497775\n",
            "epoch: 965  - cost: 0.42492393\n",
            "epoch: 966  - cost: 0.42487016\n",
            "epoch: 967  - cost: 0.42481652\n",
            "epoch: 968  - cost: 0.42476287\n",
            "epoch: 969  - cost: 0.42470938\n",
            "epoch: 970  - cost: 0.42465585\n",
            "epoch: 971  - cost: 0.42460245\n",
            "epoch: 972  - cost: 0.42454913\n",
            "epoch: 973  - cost: 0.42449582\n",
            "epoch: 974  - cost: 0.42444256\n",
            "epoch: 975  - cost: 0.42438942\n",
            "epoch: 976  - cost: 0.42433634\n",
            "epoch: 977  - cost: 0.4242833\n",
            "epoch: 978  - cost: 0.4242304\n",
            "epoch: 979  - cost: 0.4241775\n",
            "epoch: 980  - cost: 0.42412466\n",
            "epoch: 981  - cost: 0.42407185\n",
            "epoch: 982  - cost: 0.4240192\n",
            "epoch: 983  - cost: 0.42396653\n",
            "epoch: 984  - cost: 0.423914\n",
            "epoch: 985  - cost: 0.42386147\n",
            "epoch: 986  - cost: 0.423809\n",
            "epoch: 987  - cost: 0.42375663\n",
            "epoch: 988  - cost: 0.4237043\n",
            "epoch: 989  - cost: 0.423652\n",
            "epoch: 990  - cost: 0.4235999\n",
            "epoch: 991  - cost: 0.42354774\n",
            "epoch: 992  - cost: 0.42349568\n",
            "epoch: 993  - cost: 0.42344365\n",
            "epoch: 994  - cost: 0.42339173\n",
            "epoch: 995  - cost: 0.42333984\n",
            "epoch: 996  - cost: 0.42328802\n",
            "epoch: 997  - cost: 0.42323622\n",
            "epoch: 998  - cost: 0.4231845\n",
            "epoch: 999  - cost: 0.42313287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRG8fHsU1ZB0",
        "colab_type": "text"
      },
      "source": [
        "### 8. Validation of the Model based on Test Subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjC_MkLg1aG9",
        "colab_type": "text"
      },
      "source": [
        "As discussed earlier, the accuracy of a trained model is calculated based on Test Subset. Therefore, at first, I will feed the test subset to my model and get the output (labels). Then, I will compare the output obtained from the model with that of the actual or desired output and finally, will calculate the accuracy as percentage of correct predictions out of total predictions made on test subset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8GbwhuZ1Ewc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16babdd5-9b03-405c-999d-6f425724646a"
      },
      "source": [
        "# Run the trained model on test subset\n",
        "pred_y = sess.run(y, feed_dict={x: test_x})\n",
        "\n",
        "# calculate the correct predictions\n",
        "correct_prediction = tf.equal(tf.argmax(pred_y, 1), tf.argmax(test_y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print(f'Accuracy: {sess.run(accuracy)}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8333333134651184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLOTyDI02axM",
        "colab_type": "text"
      },
      "source": [
        "As you can see, we got an accuracy of 83.34% which is descent enough. Now, let us observe how the cost or error has been reduced in successive epochs by plotting a graph of Cost vs No. of Epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmrt3Udb2TBj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "1fba4f74-aed9-4261-f7e7-cb8cb78f5407"
      },
      "source": [
        "plt.plot(range(len(cost_history)),cost_history)\n",
        "plt.axis([0,training_epochs,0,np.max(cost_history)])\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPcvGLup2osG",
        "colab_type": "text"
      },
      "source": [
        "## Putting all together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjv_8ejd2fGE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a823b54-a57a-4bcd-f237-76ace94d7f98"
      },
      "source": [
        "#define the one hot encode function\n",
        "def one_hot_encode(labels):\n",
        "    n_labels = len(labels)\n",
        "    n_unique_labels = len(np.unique(labels))\n",
        "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
        "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
        "    return one_hot_encode\n",
        " \n",
        "#Read the sonar dataset\n",
        "df = pd.read_csv('sonar.csv')\n",
        "print(len(df.columns))\n",
        "X = df[df.columns[0:60]].values\n",
        "y=df[df.columns[60]]\n",
        "\n",
        "#encode the dependent variable containing categorical values\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "y = encoder.transform(y)\n",
        "Y = one_hot_encode(y)\n",
        " \n",
        "#Transform the data in training and testing\n",
        "X,Y = shuffle(X,Y,random_state=1)\n",
        "train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size=0.20, random_state=42)\n",
        " \n",
        " \n",
        "#define and initialize the variables to work with the tensors\n",
        "learning_rate = 0.1\n",
        "training_epochs = 1000\n",
        " \n",
        "#Array to store cost obtained in each epoch\n",
        "cost_history = np.empty(shape=[1],dtype=float)\n",
        " \n",
        "n_dim = X.shape[1]\n",
        "n_class = 2\n",
        " \n",
        "x = tf.placeholder(tf.float32,[None,n_dim])\n",
        "W = tf.Variable(tf.zeros([n_dim,n_class]))\n",
        "b = tf.Variable(tf.zeros([n_class]))\n",
        " \n",
        "#initialize all variables.\n",
        "init = tf.global_variables_initializer()\n",
        " \n",
        "#define the cost function\n",
        "y_ = tf.placeholder(tf.float32,[None,n_class])\n",
        "y = tf.nn.softmax(tf.matmul(x, W)+ b)\n",
        "cost_function = tf.reduce_mean(-tf.reduce_sum((y_ * tf.log(y)),reduction_indices=[1]))\n",
        "training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
        " \n",
        "#initialize the session\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "cost_history = []\n",
        " \n",
        "#calculate the cost for each epoch\n",
        "for epoch in range(training_epochs):\n",
        "    sess.run(training_step,feed_dict={x:train_x,y_:train_y})\n",
        "    cost = sess.run(cost_function,feed_dict={x: train_x,y_: train_y})\n",
        "    cost_history = np.append(cost_history, cost)\n",
        "    print('epoch : ', epoch,  ' - ', 'cost: ', cost)\n",
        " \n",
        "pred_y = sess.run(y, feed_dict={x: test_x})\n",
        " \n",
        "#Calculate Accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(pred_y,1), tf.argmax(test_y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print(f'Accuracy:{sess.run(accuracy)}')\n",
        " \n",
        "plt.plot(range(len(cost_history)),cost_history)\n",
        "plt.axis([0,training_epochs,0,np.max(cost_history)])\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61\n",
            "epoch :  0  -  cost:  0.6890586\n",
            "epoch :  1  -  cost:  0.6861614\n",
            "epoch :  2  -  cost:  0.68367136\n",
            "epoch :  3  -  cost:  0.6813467\n",
            "epoch :  4  -  cost:  0.6791113\n",
            "epoch :  5  -  cost:  0.6769402\n",
            "epoch :  6  -  cost:  0.6748241\n",
            "epoch :  7  -  cost:  0.6727587\n",
            "epoch :  8  -  cost:  0.67074114\n",
            "epoch :  9  -  cost:  0.6687693\n",
            "epoch :  10  -  cost:  0.6668413\n",
            "epoch :  11  -  cost:  0.6649552\n",
            "epoch :  12  -  cost:  0.6631094\n",
            "epoch :  13  -  cost:  0.6613022\n",
            "epoch :  14  -  cost:  0.65953195\n",
            "epoch :  15  -  cost:  0.65779734\n",
            "epoch :  16  -  cost:  0.6560968\n",
            "epoch :  17  -  cost:  0.6544292\n",
            "epoch :  18  -  cost:  0.6527932\n",
            "epoch :  19  -  cost:  0.6511875\n",
            "epoch :  20  -  cost:  0.6496111\n",
            "epoch :  21  -  cost:  0.64806294\n",
            "epoch :  22  -  cost:  0.6465418\n",
            "epoch :  23  -  cost:  0.64504665\n",
            "epoch :  24  -  cost:  0.6435769\n",
            "epoch :  25  -  cost:  0.6421313\n",
            "epoch :  26  -  cost:  0.6407093\n",
            "epoch :  27  -  cost:  0.63930976\n",
            "epoch :  28  -  cost:  0.63793224\n",
            "epoch :  29  -  cost:  0.6365758\n",
            "epoch :  30  -  cost:  0.63523984\n",
            "epoch :  31  -  cost:  0.6339236\n",
            "epoch :  32  -  cost:  0.6326266\n",
            "epoch :  33  -  cost:  0.6313482\n",
            "epoch :  34  -  cost:  0.63008755\n",
            "epoch :  35  -  cost:  0.6288445\n",
            "epoch :  36  -  cost:  0.62761825\n",
            "epoch :  37  -  cost:  0.6264085\n",
            "epoch :  38  -  cost:  0.6252146\n",
            "epoch :  39  -  cost:  0.6240361\n",
            "epoch :  40  -  cost:  0.6228727\n",
            "epoch :  41  -  cost:  0.62172383\n",
            "epoch :  42  -  cost:  0.62058926\n",
            "epoch :  43  -  cost:  0.6194685\n",
            "epoch :  44  -  cost:  0.6183612\n",
            "epoch :  45  -  cost:  0.617267\n",
            "epoch :  46  -  cost:  0.6161856\n",
            "epoch :  47  -  cost:  0.6151166\n",
            "epoch :  48  -  cost:  0.6140598\n",
            "epoch :  49  -  cost:  0.6130148\n",
            "epoch :  50  -  cost:  0.61198145\n",
            "epoch :  51  -  cost:  0.61095935\n",
            "epoch :  52  -  cost:  0.60994834\n",
            "epoch :  53  -  cost:  0.60894805\n",
            "epoch :  54  -  cost:  0.6079583\n",
            "epoch :  55  -  cost:  0.606979\n",
            "epoch :  56  -  cost:  0.6060097\n",
            "epoch :  57  -  cost:  0.6050503\n",
            "epoch :  58  -  cost:  0.6041005\n",
            "epoch :  59  -  cost:  0.60316026\n",
            "epoch :  60  -  cost:  0.60222936\n",
            "epoch :  61  -  cost:  0.60130745\n",
            "epoch :  62  -  cost:  0.6003945\n",
            "epoch :  63  -  cost:  0.5994903\n",
            "epoch :  64  -  cost:  0.5985946\n",
            "epoch :  65  -  cost:  0.5977075\n",
            "epoch :  66  -  cost:  0.5968285\n",
            "epoch :  67  -  cost:  0.5959577\n",
            "epoch :  68  -  cost:  0.5950949\n",
            "epoch :  69  -  cost:  0.59423983\n",
            "epoch :  70  -  cost:  0.59339255\n",
            "epoch :  71  -  cost:  0.59255284\n",
            "epoch :  72  -  cost:  0.5917205\n",
            "epoch :  73  -  cost:  0.59089553\n",
            "epoch :  74  -  cost:  0.59007776\n",
            "epoch :  75  -  cost:  0.5892671\n",
            "epoch :  76  -  cost:  0.58846337\n",
            "epoch :  77  -  cost:  0.58766645\n",
            "epoch :  78  -  cost:  0.58687633\n",
            "epoch :  79  -  cost:  0.5860928\n",
            "epoch :  80  -  cost:  0.5853159\n",
            "epoch :  81  -  cost:  0.58454543\n",
            "epoch :  82  -  cost:  0.58378136\n",
            "epoch :  83  -  cost:  0.5830235\n",
            "epoch :  84  -  cost:  0.58227175\n",
            "epoch :  85  -  cost:  0.58152616\n",
            "epoch :  86  -  cost:  0.5807865\n",
            "epoch :  87  -  cost:  0.5800528\n",
            "epoch :  88  -  cost:  0.57932496\n",
            "epoch :  89  -  cost:  0.5786029\n",
            "epoch :  90  -  cost:  0.5778864\n",
            "epoch :  91  -  cost:  0.57717556\n",
            "epoch :  92  -  cost:  0.5764702\n",
            "epoch :  93  -  cost:  0.5757704\n",
            "epoch :  94  -  cost:  0.57507586\n",
            "epoch :  95  -  cost:  0.5743867\n",
            "epoch :  96  -  cost:  0.57370275\n",
            "epoch :  97  -  cost:  0.57302403\n",
            "epoch :  98  -  cost:  0.5723504\n",
            "epoch :  99  -  cost:  0.5716818\n",
            "epoch :  100  -  cost:  0.5710182\n",
            "epoch :  101  -  cost:  0.57035947\n",
            "epoch :  102  -  cost:  0.5697056\n",
            "epoch :  103  -  cost:  0.56905663\n",
            "epoch :  104  -  cost:  0.5684125\n",
            "epoch :  105  -  cost:  0.56777287\n",
            "epoch :  106  -  cost:  0.56713796\n",
            "epoch :  107  -  cost:  0.5665076\n",
            "epoch :  108  -  cost:  0.5658818\n",
            "epoch :  109  -  cost:  0.5652605\n",
            "epoch :  110  -  cost:  0.56464356\n",
            "epoch :  111  -  cost:  0.56403106\n",
            "epoch :  112  -  cost:  0.56342286\n",
            "epoch :  113  -  cost:  0.5628189\n",
            "epoch :  114  -  cost:  0.56221926\n",
            "epoch :  115  -  cost:  0.56162375\n",
            "epoch :  116  -  cost:  0.56103235\n",
            "epoch :  117  -  cost:  0.56044513\n",
            "epoch :  118  -  cost:  0.5598619\n",
            "epoch :  119  -  cost:  0.5592828\n",
            "epoch :  120  -  cost:  0.5587075\n",
            "epoch :  121  -  cost:  0.5581361\n",
            "epoch :  122  -  cost:  0.5575686\n",
            "epoch :  123  -  cost:  0.557005\n",
            "epoch :  124  -  cost:  0.55644524\n",
            "epoch :  125  -  cost:  0.5558891\n",
            "epoch :  126  -  cost:  0.55533683\n",
            "epoch :  127  -  cost:  0.5547881\n",
            "epoch :  128  -  cost:  0.5542431\n",
            "epoch :  129  -  cost:  0.55370164\n",
            "epoch :  130  -  cost:  0.55316377\n",
            "epoch :  131  -  cost:  0.5526294\n",
            "epoch :  132  -  cost:  0.5520986\n",
            "epoch :  133  -  cost:  0.5515712\n",
            "epoch :  134  -  cost:  0.5510472\n",
            "epoch :  135  -  cost:  0.5505266\n",
            "epoch :  136  -  cost:  0.5500095\n",
            "epoch :  137  -  cost:  0.5494956\n",
            "epoch :  138  -  cost:  0.5489849\n",
            "epoch :  139  -  cost:  0.5484776\n",
            "epoch :  140  -  cost:  0.54797345\n",
            "epoch :  141  -  cost:  0.5474725\n",
            "epoch :  142  -  cost:  0.5469747\n",
            "epoch :  143  -  cost:  0.54648\n",
            "epoch :  144  -  cost:  0.5459885\n",
            "epoch :  145  -  cost:  0.5455\n",
            "epoch :  146  -  cost:  0.54501456\n",
            "epoch :  147  -  cost:  0.5445321\n",
            "epoch :  148  -  cost:  0.54405266\n",
            "epoch :  149  -  cost:  0.5435761\n",
            "epoch :  150  -  cost:  0.5431026\n",
            "epoch :  151  -  cost:  0.5426319\n",
            "epoch :  152  -  cost:  0.5421641\n",
            "epoch :  153  -  cost:  0.54169923\n",
            "epoch :  154  -  cost:  0.541237\n",
            "epoch :  155  -  cost:  0.5407777\n",
            "epoch :  156  -  cost:  0.5403211\n",
            "epoch :  157  -  cost:  0.5398672\n",
            "epoch :  158  -  cost:  0.5394161\n",
            "epoch :  159  -  cost:  0.5389676\n",
            "epoch :  160  -  cost:  0.5385219\n",
            "epoch :  161  -  cost:  0.5380787\n",
            "epoch :  162  -  cost:  0.53763825\n",
            "epoch :  163  -  cost:  0.5372003\n",
            "epoch :  164  -  cost:  0.53676486\n",
            "epoch :  165  -  cost:  0.5363321\n",
            "epoch :  166  -  cost:  0.5359017\n",
            "epoch :  167  -  cost:  0.53547394\n",
            "epoch :  168  -  cost:  0.53504866\n",
            "epoch :  169  -  cost:  0.5346257\n",
            "epoch :  170  -  cost:  0.53420526\n",
            "epoch :  171  -  cost:  0.53378725\n",
            "epoch :  172  -  cost:  0.5333716\n",
            "epoch :  173  -  cost:  0.5329583\n",
            "epoch :  174  -  cost:  0.53254735\n",
            "epoch :  175  -  cost:  0.53213876\n",
            "epoch :  176  -  cost:  0.5317325\n",
            "epoch :  177  -  cost:  0.53132844\n",
            "epoch :  178  -  cost:  0.5309267\n",
            "epoch :  179  -  cost:  0.53052723\n",
            "epoch :  180  -  cost:  0.53012997\n",
            "epoch :  181  -  cost:  0.5297349\n",
            "epoch :  182  -  cost:  0.529342\n",
            "epoch :  183  -  cost:  0.5289513\n",
            "epoch :  184  -  cost:  0.5285627\n",
            "epoch :  185  -  cost:  0.5281763\n",
            "epoch :  186  -  cost:  0.527792\n",
            "epoch :  187  -  cost:  0.5274098\n",
            "epoch :  188  -  cost:  0.52702963\n",
            "epoch :  189  -  cost:  0.5266515\n",
            "epoch :  190  -  cost:  0.5262755\n",
            "epoch :  191  -  cost:  0.5259015\n",
            "epoch :  192  -  cost:  0.5255295\n",
            "epoch :  193  -  cost:  0.5251595\n",
            "epoch :  194  -  cost:  0.5247914\n",
            "epoch :  195  -  cost:  0.5244253\n",
            "epoch :  196  -  cost:  0.5240612\n",
            "epoch :  197  -  cost:  0.523699\n",
            "epoch :  198  -  cost:  0.5233387\n",
            "epoch :  199  -  cost:  0.5229803\n",
            "epoch :  200  -  cost:  0.5226238\n",
            "epoch :  201  -  cost:  0.52226907\n",
            "epoch :  202  -  cost:  0.5219163\n",
            "epoch :  203  -  cost:  0.52156526\n",
            "epoch :  204  -  cost:  0.5212161\n",
            "epoch :  205  -  cost:  0.5208687\n",
            "epoch :  206  -  cost:  0.52052313\n",
            "epoch :  207  -  cost:  0.52017933\n",
            "epoch :  208  -  cost:  0.51983726\n",
            "epoch :  209  -  cost:  0.5194969\n",
            "epoch :  210  -  cost:  0.51915836\n",
            "epoch :  211  -  cost:  0.51882154\n",
            "epoch :  212  -  cost:  0.5184863\n",
            "epoch :  213  -  cost:  0.5181529\n",
            "epoch :  214  -  cost:  0.5178211\n",
            "epoch :  215  -  cost:  0.517491\n",
            "epoch :  216  -  cost:  0.5171625\n",
            "epoch :  217  -  cost:  0.5168357\n",
            "epoch :  218  -  cost:  0.5165104\n",
            "epoch :  219  -  cost:  0.5161868\n",
            "epoch :  220  -  cost:  0.5158648\n",
            "epoch :  221  -  cost:  0.5155444\n",
            "epoch :  222  -  cost:  0.51522547\n",
            "epoch :  223  -  cost:  0.5149082\n",
            "epoch :  224  -  cost:  0.51459247\n",
            "epoch :  225  -  cost:  0.51427823\n",
            "epoch :  226  -  cost:  0.5139656\n",
            "epoch :  227  -  cost:  0.5136545\n",
            "epoch :  228  -  cost:  0.51334476\n",
            "epoch :  229  -  cost:  0.51303655\n",
            "epoch :  230  -  cost:  0.5127298\n",
            "epoch :  231  -  cost:  0.51242465\n",
            "epoch :  232  -  cost:  0.51212084\n",
            "epoch :  233  -  cost:  0.51181847\n",
            "epoch :  234  -  cost:  0.5115175\n",
            "epoch :  235  -  cost:  0.51121813\n",
            "epoch :  236  -  cost:  0.51092\n",
            "epoch :  237  -  cost:  0.51062334\n",
            "epoch :  238  -  cost:  0.51032805\n",
            "epoch :  239  -  cost:  0.5100342\n",
            "epoch :  240  -  cost:  0.50974166\n",
            "epoch :  241  -  cost:  0.50945044\n",
            "epoch :  242  -  cost:  0.5091606\n",
            "epoch :  243  -  cost:  0.5088721\n",
            "epoch :  244  -  cost:  0.508585\n",
            "epoch :  245  -  cost:  0.5082991\n",
            "epoch :  246  -  cost:  0.50801456\n",
            "epoch :  247  -  cost:  0.5077314\n",
            "epoch :  248  -  cost:  0.50744945\n",
            "epoch :  249  -  cost:  0.5071688\n",
            "epoch :  250  -  cost:  0.5068894\n",
            "epoch :  251  -  cost:  0.5066113\n",
            "epoch :  252  -  cost:  0.50633436\n",
            "epoch :  253  -  cost:  0.50605875\n",
            "epoch :  254  -  cost:  0.5057844\n",
            "epoch :  255  -  cost:  0.5055112\n",
            "epoch :  256  -  cost:  0.50523925\n",
            "epoch :  257  -  cost:  0.50496846\n",
            "epoch :  258  -  cost:  0.50469893\n",
            "epoch :  259  -  cost:  0.5044306\n",
            "epoch :  260  -  cost:  0.5041634\n",
            "epoch :  261  -  cost:  0.5038974\n",
            "epoch :  262  -  cost:  0.50363255\n",
            "epoch :  263  -  cost:  0.50336885\n",
            "epoch :  264  -  cost:  0.50310636\n",
            "epoch :  265  -  cost:  0.50284487\n",
            "epoch :  266  -  cost:  0.50258464\n",
            "epoch :  267  -  cost:  0.50232553\n",
            "epoch :  268  -  cost:  0.5020675\n",
            "epoch :  269  -  cost:  0.50181055\n",
            "epoch :  270  -  cost:  0.50155485\n",
            "epoch :  271  -  cost:  0.50130004\n",
            "epoch :  272  -  cost:  0.50104636\n",
            "epoch :  273  -  cost:  0.50079393\n",
            "epoch :  274  -  cost:  0.5005424\n",
            "epoch :  275  -  cost:  0.50029194\n",
            "epoch :  276  -  cost:  0.50004256\n",
            "epoch :  277  -  cost:  0.49979427\n",
            "epoch :  278  -  cost:  0.49954703\n",
            "epoch :  279  -  cost:  0.49930072\n",
            "epoch :  280  -  cost:  0.49905553\n",
            "epoch :  281  -  cost:  0.49881133\n",
            "epoch :  282  -  cost:  0.49856812\n",
            "epoch :  283  -  cost:  0.49832597\n",
            "epoch :  284  -  cost:  0.49808475\n",
            "epoch :  285  -  cost:  0.49784458\n",
            "epoch :  286  -  cost:  0.49760535\n",
            "epoch :  287  -  cost:  0.4973671\n",
            "epoch :  288  -  cost:  0.49712986\n",
            "epoch :  289  -  cost:  0.49689358\n",
            "epoch :  290  -  cost:  0.49665824\n",
            "epoch :  291  -  cost:  0.49642384\n",
            "epoch :  292  -  cost:  0.4961904\n",
            "epoch :  293  -  cost:  0.495958\n",
            "epoch :  294  -  cost:  0.49572638\n",
            "epoch :  295  -  cost:  0.4954958\n",
            "epoch :  296  -  cost:  0.49526602\n",
            "epoch :  297  -  cost:  0.4950373\n",
            "epoch :  298  -  cost:  0.49480933\n",
            "epoch :  299  -  cost:  0.49458236\n",
            "epoch :  300  -  cost:  0.49435627\n",
            "epoch :  301  -  cost:  0.4941311\n",
            "epoch :  302  -  cost:  0.4939068\n",
            "epoch :  303  -  cost:  0.49368337\n",
            "epoch :  304  -  cost:  0.49346074\n",
            "epoch :  305  -  cost:  0.49323907\n",
            "epoch :  306  -  cost:  0.49301827\n",
            "epoch :  307  -  cost:  0.4927982\n",
            "epoch :  308  -  cost:  0.4925792\n",
            "epoch :  309  -  cost:  0.49236095\n",
            "epoch :  310  -  cost:  0.49214342\n",
            "epoch :  311  -  cost:  0.49192685\n",
            "epoch :  312  -  cost:  0.4917111\n",
            "epoch :  313  -  cost:  0.49149615\n",
            "epoch :  314  -  cost:  0.49128202\n",
            "epoch :  315  -  cost:  0.49106875\n",
            "epoch :  316  -  cost:  0.49085623\n",
            "epoch :  317  -  cost:  0.4906445\n",
            "epoch :  318  -  cost:  0.4904336\n",
            "epoch :  319  -  cost:  0.4902235\n",
            "epoch :  320  -  cost:  0.49001423\n",
            "epoch :  321  -  cost:  0.48980564\n",
            "epoch :  322  -  cost:  0.48959795\n",
            "epoch :  323  -  cost:  0.48939088\n",
            "epoch :  324  -  cost:  0.4891847\n",
            "epoch :  325  -  cost:  0.48897922\n",
            "epoch :  326  -  cost:  0.48877463\n",
            "epoch :  327  -  cost:  0.48857066\n",
            "epoch :  328  -  cost:  0.48836744\n",
            "epoch :  329  -  cost:  0.48816505\n",
            "epoch :  330  -  cost:  0.48796326\n",
            "epoch :  331  -  cost:  0.4877623\n",
            "epoch :  332  -  cost:  0.4875621\n",
            "epoch :  333  -  cost:  0.48736262\n",
            "epoch :  334  -  cost:  0.4871638\n",
            "epoch :  335  -  cost:  0.48696575\n",
            "epoch :  336  -  cost:  0.48676842\n",
            "epoch :  337  -  cost:  0.48657176\n",
            "epoch :  338  -  cost:  0.48637584\n",
            "epoch :  339  -  cost:  0.48618054\n",
            "epoch :  340  -  cost:  0.48598605\n",
            "epoch :  341  -  cost:  0.48579222\n",
            "epoch :  342  -  cost:  0.48559904\n",
            "epoch :  343  -  cost:  0.48540655\n",
            "epoch :  344  -  cost:  0.48521474\n",
            "epoch :  345  -  cost:  0.48502365\n",
            "epoch :  346  -  cost:  0.48483324\n",
            "epoch :  347  -  cost:  0.48464337\n",
            "epoch :  348  -  cost:  0.4844543\n",
            "epoch :  349  -  cost:  0.48426586\n",
            "epoch :  350  -  cost:  0.48407805\n",
            "epoch :  351  -  cost:  0.4838909\n",
            "epoch :  352  -  cost:  0.48370436\n",
            "epoch :  353  -  cost:  0.4835185\n",
            "epoch :  354  -  cost:  0.48333332\n",
            "epoch :  355  -  cost:  0.48314875\n",
            "epoch :  356  -  cost:  0.48296475\n",
            "epoch :  357  -  cost:  0.48278153\n",
            "epoch :  358  -  cost:  0.48259878\n",
            "epoch :  359  -  cost:  0.48241675\n",
            "epoch :  360  -  cost:  0.4822353\n",
            "epoch :  361  -  cost:  0.48205447\n",
            "epoch :  362  -  cost:  0.48187423\n",
            "epoch :  363  -  cost:  0.48169458\n",
            "epoch :  364  -  cost:  0.4815156\n",
            "epoch :  365  -  cost:  0.48133722\n",
            "epoch :  366  -  cost:  0.48115933\n",
            "epoch :  367  -  cost:  0.48098218\n",
            "epoch :  368  -  cost:  0.48080555\n",
            "epoch :  369  -  cost:  0.48062953\n",
            "epoch :  370  -  cost:  0.48045406\n",
            "epoch :  371  -  cost:  0.48027918\n",
            "epoch :  372  -  cost:  0.48010486\n",
            "epoch :  373  -  cost:  0.47993115\n",
            "epoch :  374  -  cost:  0.47975796\n",
            "epoch :  375  -  cost:  0.4795854\n",
            "epoch :  376  -  cost:  0.4794134\n",
            "epoch :  377  -  cost:  0.47924194\n",
            "epoch :  378  -  cost:  0.479071\n",
            "epoch :  379  -  cost:  0.47890064\n",
            "epoch :  380  -  cost:  0.47873086\n",
            "epoch :  381  -  cost:  0.47856164\n",
            "epoch :  382  -  cost:  0.4783929\n",
            "epoch :  383  -  cost:  0.47822472\n",
            "epoch :  384  -  cost:  0.47805718\n",
            "epoch :  385  -  cost:  0.47789\n",
            "epoch :  386  -  cost:  0.4777235\n",
            "epoch :  387  -  cost:  0.47755748\n",
            "epoch :  388  -  cost:  0.477392\n",
            "epoch :  389  -  cost:  0.477227\n",
            "epoch :  390  -  cost:  0.4770626\n",
            "epoch :  391  -  cost:  0.4768987\n",
            "epoch :  392  -  cost:  0.47673523\n",
            "epoch :  393  -  cost:  0.4765723\n",
            "epoch :  394  -  cost:  0.47640997\n",
            "epoch :  395  -  cost:  0.4762481\n",
            "epoch :  396  -  cost:  0.4760867\n",
            "epoch :  397  -  cost:  0.4759259\n",
            "epoch :  398  -  cost:  0.4757655\n",
            "epoch :  399  -  cost:  0.47560564\n",
            "epoch :  400  -  cost:  0.47544625\n",
            "epoch :  401  -  cost:  0.47528738\n",
            "epoch :  402  -  cost:  0.47512892\n",
            "epoch :  403  -  cost:  0.474971\n",
            "epoch :  404  -  cost:  0.4748136\n",
            "epoch :  405  -  cost:  0.47465667\n",
            "epoch :  406  -  cost:  0.4745002\n",
            "epoch :  407  -  cost:  0.47434428\n",
            "epoch :  408  -  cost:  0.4741887\n",
            "epoch :  409  -  cost:  0.4740337\n",
            "epoch :  410  -  cost:  0.47387913\n",
            "epoch :  411  -  cost:  0.47372502\n",
            "epoch :  412  -  cost:  0.4735713\n",
            "epoch :  413  -  cost:  0.47341812\n",
            "epoch :  414  -  cost:  0.4732654\n",
            "epoch :  415  -  cost:  0.47311315\n",
            "epoch :  416  -  cost:  0.47296134\n",
            "epoch :  417  -  cost:  0.47281\n",
            "epoch :  418  -  cost:  0.47265902\n",
            "epoch :  419  -  cost:  0.4725086\n",
            "epoch :  420  -  cost:  0.47235852\n",
            "epoch :  421  -  cost:  0.47220904\n",
            "epoch :  422  -  cost:  0.47205982\n",
            "epoch :  423  -  cost:  0.47191116\n",
            "epoch :  424  -  cost:  0.47176284\n",
            "epoch :  425  -  cost:  0.47161505\n",
            "epoch :  426  -  cost:  0.47146767\n",
            "epoch :  427  -  cost:  0.4713207\n",
            "epoch :  428  -  cost:  0.47117412\n",
            "epoch :  429  -  cost:  0.471028\n",
            "epoch :  430  -  cost:  0.4708823\n",
            "epoch :  431  -  cost:  0.470737\n",
            "epoch :  432  -  cost:  0.4705922\n",
            "epoch :  433  -  cost:  0.4704477\n",
            "epoch :  434  -  cost:  0.4703037\n",
            "epoch :  435  -  cost:  0.4701601\n",
            "epoch :  436  -  cost:  0.4700169\n",
            "epoch :  437  -  cost:  0.46987405\n",
            "epoch :  438  -  cost:  0.46973175\n",
            "epoch :  439  -  cost:  0.4695897\n",
            "epoch :  440  -  cost:  0.46944812\n",
            "epoch :  441  -  cost:  0.4693069\n",
            "epoch :  442  -  cost:  0.46916616\n",
            "epoch :  443  -  cost:  0.46902576\n",
            "epoch :  444  -  cost:  0.46888575\n",
            "epoch :  445  -  cost:  0.46874613\n",
            "epoch :  446  -  cost:  0.46860695\n",
            "epoch :  447  -  cost:  0.46846807\n",
            "epoch :  448  -  cost:  0.4683296\n",
            "epoch :  449  -  cost:  0.46819156\n",
            "epoch :  450  -  cost:  0.46805388\n",
            "epoch :  451  -  cost:  0.46791658\n",
            "epoch :  452  -  cost:  0.46777964\n",
            "epoch :  453  -  cost:  0.46764314\n",
            "epoch :  454  -  cost:  0.467507\n",
            "epoch :  455  -  cost:  0.4673712\n",
            "epoch :  456  -  cost:  0.4672357\n",
            "epoch :  457  -  cost:  0.4671007\n",
            "epoch :  458  -  cost:  0.46696606\n",
            "epoch :  459  -  cost:  0.46683168\n",
            "epoch :  460  -  cost:  0.46669775\n",
            "epoch :  461  -  cost:  0.46656415\n",
            "epoch :  462  -  cost:  0.46643093\n",
            "epoch :  463  -  cost:  0.46629804\n",
            "epoch :  464  -  cost:  0.46616554\n",
            "epoch :  465  -  cost:  0.46603334\n",
            "epoch :  466  -  cost:  0.46590155\n",
            "epoch :  467  -  cost:  0.4657701\n",
            "epoch :  468  -  cost:  0.46563897\n",
            "epoch :  469  -  cost:  0.4655082\n",
            "epoch :  470  -  cost:  0.4653778\n",
            "epoch :  471  -  cost:  0.46524784\n",
            "epoch :  472  -  cost:  0.46511808\n",
            "epoch :  473  -  cost:  0.4649887\n",
            "epoch :  474  -  cost:  0.46485966\n",
            "epoch :  475  -  cost:  0.46473092\n",
            "epoch :  476  -  cost:  0.46460256\n",
            "epoch :  477  -  cost:  0.46447453\n",
            "epoch :  478  -  cost:  0.46434683\n",
            "epoch :  479  -  cost:  0.46421954\n",
            "epoch :  480  -  cost:  0.46409246\n",
            "epoch :  481  -  cost:  0.4639658\n",
            "epoch :  482  -  cost:  0.4638394\n",
            "epoch :  483  -  cost:  0.4637134\n",
            "epoch :  484  -  cost:  0.46358767\n",
            "epoch :  485  -  cost:  0.46346223\n",
            "epoch :  486  -  cost:  0.46333715\n",
            "epoch :  487  -  cost:  0.4632125\n",
            "epoch :  488  -  cost:  0.46308798\n",
            "epoch :  489  -  cost:  0.4629639\n",
            "epoch :  490  -  cost:  0.46284014\n",
            "epoch :  491  -  cost:  0.46271664\n",
            "epoch :  492  -  cost:  0.46259344\n",
            "epoch :  493  -  cost:  0.46247065\n",
            "epoch :  494  -  cost:  0.4623481\n",
            "epoch :  495  -  cost:  0.46222585\n",
            "epoch :  496  -  cost:  0.46210393\n",
            "epoch :  497  -  cost:  0.46198225\n",
            "epoch :  498  -  cost:  0.46186092\n",
            "epoch :  499  -  cost:  0.46173993\n",
            "epoch :  500  -  cost:  0.46161926\n",
            "epoch :  501  -  cost:  0.46149883\n",
            "epoch :  502  -  cost:  0.46137872\n",
            "epoch :  503  -  cost:  0.46125886\n",
            "epoch :  504  -  cost:  0.46113938\n",
            "epoch :  505  -  cost:  0.46102017\n",
            "epoch :  506  -  cost:  0.4609012\n",
            "epoch :  507  -  cost:  0.46078265\n",
            "epoch :  508  -  cost:  0.46066427\n",
            "epoch :  509  -  cost:  0.4605462\n",
            "epoch :  510  -  cost:  0.46042845\n",
            "epoch :  511  -  cost:  0.46031097\n",
            "epoch :  512  -  cost:  0.46019378\n",
            "epoch :  513  -  cost:  0.4600769\n",
            "epoch :  514  -  cost:  0.45996028\n",
            "epoch :  515  -  cost:  0.45984396\n",
            "epoch :  516  -  cost:  0.45972785\n",
            "epoch :  517  -  cost:  0.45961207\n",
            "epoch :  518  -  cost:  0.45949662\n",
            "epoch :  519  -  cost:  0.45938137\n",
            "epoch :  520  -  cost:  0.45926648\n",
            "epoch :  521  -  cost:  0.4591518\n",
            "epoch :  522  -  cost:  0.45903742\n",
            "epoch :  523  -  cost:  0.45892325\n",
            "epoch :  524  -  cost:  0.45880947\n",
            "epoch :  525  -  cost:  0.4586959\n",
            "epoch :  526  -  cost:  0.45858255\n",
            "epoch :  527  -  cost:  0.45846954\n",
            "epoch :  528  -  cost:  0.45835677\n",
            "epoch :  529  -  cost:  0.45824426\n",
            "epoch :  530  -  cost:  0.458132\n",
            "epoch :  531  -  cost:  0.45802006\n",
            "epoch :  532  -  cost:  0.4579084\n",
            "epoch :  533  -  cost:  0.45779693\n",
            "epoch :  534  -  cost:  0.45768568\n",
            "epoch :  535  -  cost:  0.45757475\n",
            "epoch :  536  -  cost:  0.45746413\n",
            "epoch :  537  -  cost:  0.4573537\n",
            "epoch :  538  -  cost:  0.45724353\n",
            "epoch :  539  -  cost:  0.45713362\n",
            "epoch :  540  -  cost:  0.45702395\n",
            "epoch :  541  -  cost:  0.4569146\n",
            "epoch :  542  -  cost:  0.45680547\n",
            "epoch :  543  -  cost:  0.45669657\n",
            "epoch :  544  -  cost:  0.4565879\n",
            "epoch :  545  -  cost:  0.45647952\n",
            "epoch :  546  -  cost:  0.45637137\n",
            "epoch :  547  -  cost:  0.45626345\n",
            "epoch :  548  -  cost:  0.45615587\n",
            "epoch :  549  -  cost:  0.4560484\n",
            "epoch :  550  -  cost:  0.45594123\n",
            "epoch :  551  -  cost:  0.4558343\n",
            "epoch :  552  -  cost:  0.45572764\n",
            "epoch :  553  -  cost:  0.45562124\n",
            "epoch :  554  -  cost:  0.455515\n",
            "epoch :  555  -  cost:  0.45540902\n",
            "epoch :  556  -  cost:  0.45530334\n",
            "epoch :  557  -  cost:  0.45519778\n",
            "epoch :  558  -  cost:  0.4550926\n",
            "epoch :  559  -  cost:  0.45498756\n",
            "epoch :  560  -  cost:  0.4548828\n",
            "epoch :  561  -  cost:  0.45477822\n",
            "epoch :  562  -  cost:  0.45467395\n",
            "epoch :  563  -  cost:  0.45456988\n",
            "epoch :  564  -  cost:  0.45446596\n",
            "epoch :  565  -  cost:  0.45436236\n",
            "epoch :  566  -  cost:  0.45425895\n",
            "epoch :  567  -  cost:  0.45415574\n",
            "epoch :  568  -  cost:  0.45405287\n",
            "epoch :  569  -  cost:  0.45395008\n",
            "epoch :  570  -  cost:  0.45384762\n",
            "epoch :  571  -  cost:  0.4537453\n",
            "epoch :  572  -  cost:  0.45364323\n",
            "epoch :  573  -  cost:  0.45354146\n",
            "epoch :  574  -  cost:  0.4534398\n",
            "epoch :  575  -  cost:  0.45333847\n",
            "epoch :  576  -  cost:  0.45323727\n",
            "epoch :  577  -  cost:  0.45313638\n",
            "epoch :  578  -  cost:  0.45303565\n",
            "epoch :  579  -  cost:  0.4529351\n",
            "epoch :  580  -  cost:  0.45283484\n",
            "epoch :  581  -  cost:  0.45273474\n",
            "epoch :  582  -  cost:  0.45263487\n",
            "epoch :  583  -  cost:  0.45253527\n",
            "epoch :  584  -  cost:  0.45243573\n",
            "epoch :  585  -  cost:  0.4523365\n",
            "epoch :  586  -  cost:  0.4522375\n",
            "epoch :  587  -  cost:  0.45213872\n",
            "epoch :  588  -  cost:  0.4520401\n",
            "epoch :  589  -  cost:  0.4519416\n",
            "epoch :  590  -  cost:  0.4518435\n",
            "epoch :  591  -  cost:  0.45174557\n",
            "epoch :  592  -  cost:  0.45164776\n",
            "epoch :  593  -  cost:  0.4515502\n",
            "epoch :  594  -  cost:  0.45145282\n",
            "epoch :  595  -  cost:  0.45135564\n",
            "epoch :  596  -  cost:  0.45125872\n",
            "epoch :  597  -  cost:  0.45116198\n",
            "epoch :  598  -  cost:  0.4510654\n",
            "epoch :  599  -  cost:  0.45096904\n",
            "epoch :  600  -  cost:  0.4508729\n",
            "epoch :  601  -  cost:  0.4507769\n",
            "epoch :  602  -  cost:  0.45068118\n",
            "epoch :  603  -  cost:  0.4505856\n",
            "epoch :  604  -  cost:  0.45049027\n",
            "epoch :  605  -  cost:  0.4503951\n",
            "epoch :  606  -  cost:  0.45030013\n",
            "epoch :  607  -  cost:  0.45020536\n",
            "epoch :  608  -  cost:  0.4501107\n",
            "epoch :  609  -  cost:  0.45001632\n",
            "epoch :  610  -  cost:  0.44992214\n",
            "epoch :  611  -  cost:  0.44982818\n",
            "epoch :  612  -  cost:  0.4497343\n",
            "epoch :  613  -  cost:  0.44964072\n",
            "epoch :  614  -  cost:  0.44954723\n",
            "epoch :  615  -  cost:  0.449454\n",
            "epoch :  616  -  cost:  0.44936088\n",
            "epoch :  617  -  cost:  0.44926807\n",
            "epoch :  618  -  cost:  0.44917542\n",
            "epoch :  619  -  cost:  0.44908294\n",
            "epoch :  620  -  cost:  0.44899064\n",
            "epoch :  621  -  cost:  0.44889846\n",
            "epoch :  622  -  cost:  0.44880652\n",
            "epoch :  623  -  cost:  0.4487148\n",
            "epoch :  624  -  cost:  0.44862315\n",
            "epoch :  625  -  cost:  0.44853178\n",
            "epoch :  626  -  cost:  0.44844055\n",
            "epoch :  627  -  cost:  0.44834957\n",
            "epoch :  628  -  cost:  0.44825864\n",
            "epoch :  629  -  cost:  0.44816798\n",
            "epoch :  630  -  cost:  0.4480775\n",
            "epoch :  631  -  cost:  0.44798714\n",
            "epoch :  632  -  cost:  0.44789705\n",
            "epoch :  633  -  cost:  0.44780704\n",
            "epoch :  634  -  cost:  0.44771725\n",
            "epoch :  635  -  cost:  0.44762763\n",
            "epoch :  636  -  cost:  0.44753817\n",
            "epoch :  637  -  cost:  0.4474489\n",
            "epoch :  638  -  cost:  0.4473598\n",
            "epoch :  639  -  cost:  0.4472709\n",
            "epoch :  640  -  cost:  0.44718212\n",
            "epoch :  641  -  cost:  0.44709358\n",
            "epoch :  642  -  cost:  0.44700515\n",
            "epoch :  643  -  cost:  0.44691694\n",
            "epoch :  644  -  cost:  0.44682884\n",
            "epoch :  645  -  cost:  0.44674096\n",
            "epoch :  646  -  cost:  0.4466532\n",
            "epoch :  647  -  cost:  0.44656572\n",
            "epoch :  648  -  cost:  0.44647828\n",
            "epoch :  649  -  cost:  0.44639102\n",
            "epoch :  650  -  cost:  0.446304\n",
            "epoch :  651  -  cost:  0.44621712\n",
            "epoch :  652  -  cost:  0.44613034\n",
            "epoch :  653  -  cost:  0.44604376\n",
            "epoch :  654  -  cost:  0.4459574\n",
            "epoch :  655  -  cost:  0.44587114\n",
            "epoch :  656  -  cost:  0.44578505\n",
            "epoch :  657  -  cost:  0.44569916\n",
            "epoch :  658  -  cost:  0.44561338\n",
            "epoch :  659  -  cost:  0.4455278\n",
            "epoch :  660  -  cost:  0.44544238\n",
            "epoch :  661  -  cost:  0.4453571\n",
            "epoch :  662  -  cost:  0.44527203\n",
            "epoch :  663  -  cost:  0.445187\n",
            "epoch :  664  -  cost:  0.4451022\n",
            "epoch :  665  -  cost:  0.44501758\n",
            "epoch :  666  -  cost:  0.44493315\n",
            "epoch :  667  -  cost:  0.44484878\n",
            "epoch :  668  -  cost:  0.44476467\n",
            "epoch :  669  -  cost:  0.4446806\n",
            "epoch :  670  -  cost:  0.44459674\n",
            "epoch :  671  -  cost:  0.44451302\n",
            "epoch :  672  -  cost:  0.44442952\n",
            "epoch :  673  -  cost:  0.44434607\n",
            "epoch :  674  -  cost:  0.4442628\n",
            "epoch :  675  -  cost:  0.4441798\n",
            "epoch :  676  -  cost:  0.44409683\n",
            "epoch :  677  -  cost:  0.444014\n",
            "epoch :  678  -  cost:  0.4439314\n",
            "epoch :  679  -  cost:  0.4438489\n",
            "epoch :  680  -  cost:  0.44376656\n",
            "epoch :  681  -  cost:  0.44368434\n",
            "epoch :  682  -  cost:  0.44360235\n",
            "epoch :  683  -  cost:  0.44352037\n",
            "epoch :  684  -  cost:  0.4434387\n",
            "epoch :  685  -  cost:  0.4433571\n",
            "epoch :  686  -  cost:  0.44327563\n",
            "epoch :  687  -  cost:  0.44319436\n",
            "epoch :  688  -  cost:  0.4431132\n",
            "epoch :  689  -  cost:  0.44303215\n",
            "epoch :  690  -  cost:  0.44295132\n",
            "epoch :  691  -  cost:  0.44287053\n",
            "epoch :  692  -  cost:  0.44279\n",
            "epoch :  693  -  cost:  0.44270954\n",
            "epoch :  694  -  cost:  0.44262928\n",
            "epoch :  695  -  cost:  0.44254908\n",
            "epoch :  696  -  cost:  0.4424691\n",
            "epoch :  697  -  cost:  0.4423892\n",
            "epoch :  698  -  cost:  0.44230947\n",
            "epoch :  699  -  cost:  0.44222987\n",
            "epoch :  700  -  cost:  0.4421504\n",
            "epoch :  701  -  cost:  0.4420711\n",
            "epoch :  702  -  cost:  0.44199196\n",
            "epoch :  703  -  cost:  0.44191295\n",
            "epoch :  704  -  cost:  0.441834\n",
            "epoch :  705  -  cost:  0.4417552\n",
            "epoch :  706  -  cost:  0.44167665\n",
            "epoch :  707  -  cost:  0.44159818\n",
            "epoch :  708  -  cost:  0.44151992\n",
            "epoch :  709  -  cost:  0.44144163\n",
            "epoch :  710  -  cost:  0.4413635\n",
            "epoch :  711  -  cost:  0.4412856\n",
            "epoch :  712  -  cost:  0.4412078\n",
            "epoch :  713  -  cost:  0.44113007\n",
            "epoch :  714  -  cost:  0.44105262\n",
            "epoch :  715  -  cost:  0.44097513\n",
            "epoch :  716  -  cost:  0.4408979\n",
            "epoch :  717  -  cost:  0.44082078\n",
            "epoch :  718  -  cost:  0.44074377\n",
            "epoch :  719  -  cost:  0.44066685\n",
            "epoch :  720  -  cost:  0.44059014\n",
            "epoch :  721  -  cost:  0.44051355\n",
            "epoch :  722  -  cost:  0.44043708\n",
            "epoch :  723  -  cost:  0.4403607\n",
            "epoch :  724  -  cost:  0.4402845\n",
            "epoch :  725  -  cost:  0.44020838\n",
            "epoch :  726  -  cost:  0.4401324\n",
            "epoch :  727  -  cost:  0.4400566\n",
            "epoch :  728  -  cost:  0.4399809\n",
            "epoch :  729  -  cost:  0.4399053\n",
            "epoch :  730  -  cost:  0.4398299\n",
            "epoch :  731  -  cost:  0.43975446\n",
            "epoch :  732  -  cost:  0.43967927\n",
            "epoch :  733  -  cost:  0.43960422\n",
            "epoch :  734  -  cost:  0.43952927\n",
            "epoch :  735  -  cost:  0.4394544\n",
            "epoch :  736  -  cost:  0.43937975\n",
            "epoch :  737  -  cost:  0.43930516\n",
            "epoch :  738  -  cost:  0.4392307\n",
            "epoch :  739  -  cost:  0.43915644\n",
            "epoch :  740  -  cost:  0.4390822\n",
            "epoch :  741  -  cost:  0.43900812\n",
            "epoch :  742  -  cost:  0.43893415\n",
            "epoch :  743  -  cost:  0.43886036\n",
            "epoch :  744  -  cost:  0.4387866\n",
            "epoch :  745  -  cost:  0.43871298\n",
            "epoch :  746  -  cost:  0.43863955\n",
            "epoch :  747  -  cost:  0.43856612\n",
            "epoch :  748  -  cost:  0.43849292\n",
            "epoch :  749  -  cost:  0.43841982\n",
            "epoch :  750  -  cost:  0.43834677\n",
            "epoch :  751  -  cost:  0.43827394\n",
            "epoch :  752  -  cost:  0.43820122\n",
            "epoch :  753  -  cost:  0.43812853\n",
            "epoch :  754  -  cost:  0.43805608\n",
            "epoch :  755  -  cost:  0.43798357\n",
            "epoch :  756  -  cost:  0.43791133\n",
            "epoch :  757  -  cost:  0.43783915\n",
            "epoch :  758  -  cost:  0.4377671\n",
            "epoch :  759  -  cost:  0.43769518\n",
            "epoch :  760  -  cost:  0.43762332\n",
            "epoch :  761  -  cost:  0.43755165\n",
            "epoch :  762  -  cost:  0.43748006\n",
            "epoch :  763  -  cost:  0.43740854\n",
            "epoch :  764  -  cost:  0.4373372\n",
            "epoch :  765  -  cost:  0.437266\n",
            "epoch :  766  -  cost:  0.43719482\n",
            "epoch :  767  -  cost:  0.43712375\n",
            "epoch :  768  -  cost:  0.43705288\n",
            "epoch :  769  -  cost:  0.43698207\n",
            "epoch :  770  -  cost:  0.43691143\n",
            "epoch :  771  -  cost:  0.43684083\n",
            "epoch :  772  -  cost:  0.43677035\n",
            "epoch :  773  -  cost:  0.4367\n",
            "epoch :  774  -  cost:  0.43662977\n",
            "epoch :  775  -  cost:  0.43655968\n",
            "epoch :  776  -  cost:  0.43648964\n",
            "epoch :  777  -  cost:  0.43641967\n",
            "epoch :  778  -  cost:  0.43634996\n",
            "epoch :  779  -  cost:  0.43628022\n",
            "epoch :  780  -  cost:  0.4362107\n",
            "epoch :  781  -  cost:  0.43614122\n",
            "epoch :  782  -  cost:  0.4360718\n",
            "epoch :  783  -  cost:  0.43600252\n",
            "epoch :  784  -  cost:  0.43593338\n",
            "epoch :  785  -  cost:  0.43586436\n",
            "epoch :  786  -  cost:  0.4357954\n",
            "epoch :  787  -  cost:  0.43572664\n",
            "epoch :  788  -  cost:  0.4356579\n",
            "epoch :  789  -  cost:  0.43558928\n",
            "epoch :  790  -  cost:  0.4355208\n",
            "epoch :  791  -  cost:  0.43545237\n",
            "epoch :  792  -  cost:  0.43538412\n",
            "epoch :  793  -  cost:  0.4353159\n",
            "epoch :  794  -  cost:  0.4352478\n",
            "epoch :  795  -  cost:  0.4351798\n",
            "epoch :  796  -  cost:  0.4351119\n",
            "epoch :  797  -  cost:  0.43504417\n",
            "epoch :  798  -  cost:  0.43497652\n",
            "epoch :  799  -  cost:  0.43490896\n",
            "epoch :  800  -  cost:  0.4348415\n",
            "epoch :  801  -  cost:  0.4347741\n",
            "epoch :  802  -  cost:  0.4347068\n",
            "epoch :  803  -  cost:  0.43463963\n",
            "epoch :  804  -  cost:  0.43457264\n",
            "epoch :  805  -  cost:  0.43450567\n",
            "epoch :  806  -  cost:  0.4344388\n",
            "epoch :  807  -  cost:  0.43437204\n",
            "epoch :  808  -  cost:  0.4343054\n",
            "epoch :  809  -  cost:  0.43423888\n",
            "epoch :  810  -  cost:  0.43417242\n",
            "epoch :  811  -  cost:  0.43410602\n",
            "epoch :  812  -  cost:  0.43403977\n",
            "epoch :  813  -  cost:  0.4339736\n",
            "epoch :  814  -  cost:  0.43390757\n",
            "epoch :  815  -  cost:  0.43384165\n",
            "epoch :  816  -  cost:  0.43377575\n",
            "epoch :  817  -  cost:  0.43371004\n",
            "epoch :  818  -  cost:  0.4336443\n",
            "epoch :  819  -  cost:  0.43357876\n",
            "epoch :  820  -  cost:  0.43351325\n",
            "epoch :  821  -  cost:  0.43344793\n",
            "epoch :  822  -  cost:  0.43338266\n",
            "epoch :  823  -  cost:  0.4333174\n",
            "epoch :  824  -  cost:  0.43325233\n",
            "epoch :  825  -  cost:  0.43318737\n",
            "epoch :  826  -  cost:  0.4331225\n",
            "epoch :  827  -  cost:  0.4330577\n",
            "epoch :  828  -  cost:  0.43299294\n",
            "epoch :  829  -  cost:  0.43292832\n",
            "epoch :  830  -  cost:  0.43286386\n",
            "epoch :  831  -  cost:  0.43279946\n",
            "epoch :  832  -  cost:  0.4327351\n",
            "epoch :  833  -  cost:  0.43267086\n",
            "epoch :  834  -  cost:  0.4326067\n",
            "epoch :  835  -  cost:  0.4325427\n",
            "epoch :  836  -  cost:  0.4324787\n",
            "epoch :  837  -  cost:  0.4324149\n",
            "epoch :  838  -  cost:  0.43235108\n",
            "epoch :  839  -  cost:  0.43228742\n",
            "epoch :  840  -  cost:  0.43222383\n",
            "epoch :  841  -  cost:  0.43216035\n",
            "epoch :  842  -  cost:  0.4320969\n",
            "epoch :  843  -  cost:  0.4320336\n",
            "epoch :  844  -  cost:  0.43197045\n",
            "epoch :  845  -  cost:  0.4319073\n",
            "epoch :  846  -  cost:  0.43184426\n",
            "epoch :  847  -  cost:  0.43178123\n",
            "epoch :  848  -  cost:  0.43171844\n",
            "epoch :  849  -  cost:  0.4316557\n",
            "epoch :  850  -  cost:  0.43159294\n",
            "epoch :  851  -  cost:  0.43153036\n",
            "epoch :  852  -  cost:  0.4314679\n",
            "epoch :  853  -  cost:  0.4314055\n",
            "epoch :  854  -  cost:  0.43134314\n",
            "epoch :  855  -  cost:  0.43128088\n",
            "epoch :  856  -  cost:  0.43121874\n",
            "epoch :  857  -  cost:  0.4311567\n",
            "epoch :  858  -  cost:  0.43109474\n",
            "epoch :  859  -  cost:  0.4310328\n",
            "epoch :  860  -  cost:  0.43097106\n",
            "epoch :  861  -  cost:  0.43090937\n",
            "epoch :  862  -  cost:  0.43084773\n",
            "epoch :  863  -  cost:  0.43078613\n",
            "epoch :  864  -  cost:  0.43072468\n",
            "epoch :  865  -  cost:  0.43066332\n",
            "epoch :  866  -  cost:  0.43060204\n",
            "epoch :  867  -  cost:  0.43054083\n",
            "epoch :  868  -  cost:  0.43047976\n",
            "epoch :  869  -  cost:  0.43041876\n",
            "epoch :  870  -  cost:  0.4303577\n",
            "epoch :  871  -  cost:  0.43029687\n",
            "epoch :  872  -  cost:  0.43023613\n",
            "epoch :  873  -  cost:  0.43017545\n",
            "epoch :  874  -  cost:  0.43011484\n",
            "epoch :  875  -  cost:  0.43005428\n",
            "epoch :  876  -  cost:  0.42999387\n",
            "epoch :  877  -  cost:  0.4299335\n",
            "epoch :  878  -  cost:  0.42987323\n",
            "epoch :  879  -  cost:  0.4298131\n",
            "epoch :  880  -  cost:  0.4297529\n",
            "epoch :  881  -  cost:  0.42969292\n",
            "epoch :  882  -  cost:  0.429633\n",
            "epoch :  883  -  cost:  0.42957312\n",
            "epoch :  884  -  cost:  0.4295133\n",
            "epoch :  885  -  cost:  0.4294536\n",
            "epoch :  886  -  cost:  0.42939404\n",
            "epoch :  887  -  cost:  0.42933446\n",
            "epoch :  888  -  cost:  0.42927504\n",
            "epoch :  889  -  cost:  0.42921564\n",
            "epoch :  890  -  cost:  0.42915636\n",
            "epoch :  891  -  cost:  0.42909712\n",
            "epoch :  892  -  cost:  0.429038\n",
            "epoch :  893  -  cost:  0.42897895\n",
            "epoch :  894  -  cost:  0.42891997\n",
            "epoch :  895  -  cost:  0.42886102\n",
            "epoch :  896  -  cost:  0.42880222\n",
            "epoch :  897  -  cost:  0.42874348\n",
            "epoch :  898  -  cost:  0.4286848\n",
            "epoch :  899  -  cost:  0.42862627\n",
            "epoch :  900  -  cost:  0.4285678\n",
            "epoch :  901  -  cost:  0.42850932\n",
            "epoch :  902  -  cost:  0.42845103\n",
            "epoch :  903  -  cost:  0.4283927\n",
            "epoch :  904  -  cost:  0.4283345\n",
            "epoch :  905  -  cost:  0.4282764\n",
            "epoch :  906  -  cost:  0.42821836\n",
            "epoch :  907  -  cost:  0.42816043\n",
            "epoch :  908  -  cost:  0.42810252\n",
            "epoch :  909  -  cost:  0.42804468\n",
            "epoch :  910  -  cost:  0.427987\n",
            "epoch :  911  -  cost:  0.4279293\n",
            "epoch :  912  -  cost:  0.42787176\n",
            "epoch :  913  -  cost:  0.42781428\n",
            "epoch :  914  -  cost:  0.4277568\n",
            "epoch :  915  -  cost:  0.42769948\n",
            "epoch :  916  -  cost:  0.42764223\n",
            "epoch :  917  -  cost:  0.42758504\n",
            "epoch :  918  -  cost:  0.42752787\n",
            "epoch :  919  -  cost:  0.4274708\n",
            "epoch :  920  -  cost:  0.42741385\n",
            "epoch :  921  -  cost:  0.42735687\n",
            "epoch :  922  -  cost:  0.4273001\n",
            "epoch :  923  -  cost:  0.42724326\n",
            "epoch :  924  -  cost:  0.42718664\n",
            "epoch :  925  -  cost:  0.42713004\n",
            "epoch :  926  -  cost:  0.42707348\n",
            "epoch :  927  -  cost:  0.42701697\n",
            "epoch :  928  -  cost:  0.42696065\n",
            "epoch :  929  -  cost:  0.42690438\n",
            "epoch :  930  -  cost:  0.42684808\n",
            "epoch :  931  -  cost:  0.42679194\n",
            "epoch :  932  -  cost:  0.4267358\n",
            "epoch :  933  -  cost:  0.42667982\n",
            "epoch :  934  -  cost:  0.42662382\n",
            "epoch :  935  -  cost:  0.42656794\n",
            "epoch :  936  -  cost:  0.4265121\n",
            "epoch :  937  -  cost:  0.4264564\n",
            "epoch :  938  -  cost:  0.42640072\n",
            "epoch :  939  -  cost:  0.42634514\n",
            "epoch :  940  -  cost:  0.4262896\n",
            "epoch :  941  -  cost:  0.42623416\n",
            "epoch :  942  -  cost:  0.42617875\n",
            "epoch :  943  -  cost:  0.4261234\n",
            "epoch :  944  -  cost:  0.42606816\n",
            "epoch :  945  -  cost:  0.42601305\n",
            "epoch :  946  -  cost:  0.42595792\n",
            "epoch :  947  -  cost:  0.4259028\n",
            "epoch :  948  -  cost:  0.4258479\n",
            "epoch :  949  -  cost:  0.425793\n",
            "epoch :  950  -  cost:  0.42573822\n",
            "epoch :  951  -  cost:  0.42568335\n",
            "epoch :  952  -  cost:  0.42562872\n",
            "epoch :  953  -  cost:  0.4255741\n",
            "epoch :  954  -  cost:  0.42551953\n",
            "epoch :  955  -  cost:  0.42546508\n",
            "epoch :  956  -  cost:  0.4254106\n",
            "epoch :  957  -  cost:  0.42535627\n",
            "epoch :  958  -  cost:  0.425302\n",
            "epoch :  959  -  cost:  0.4252478\n",
            "epoch :  960  -  cost:  0.4251936\n",
            "epoch :  961  -  cost:  0.4251396\n",
            "epoch :  962  -  cost:  0.42508554\n",
            "epoch :  963  -  cost:  0.42503163\n",
            "epoch :  964  -  cost:  0.42497775\n",
            "epoch :  965  -  cost:  0.42492393\n",
            "epoch :  966  -  cost:  0.42487016\n",
            "epoch :  967  -  cost:  0.42481652\n",
            "epoch :  968  -  cost:  0.42476287\n",
            "epoch :  969  -  cost:  0.42470938\n",
            "epoch :  970  -  cost:  0.42465585\n",
            "epoch :  971  -  cost:  0.42460245\n",
            "epoch :  972  -  cost:  0.42454913\n",
            "epoch :  973  -  cost:  0.42449582\n",
            "epoch :  974  -  cost:  0.42444256\n",
            "epoch :  975  -  cost:  0.42438942\n",
            "epoch :  976  -  cost:  0.42433634\n",
            "epoch :  977  -  cost:  0.4242833\n",
            "epoch :  978  -  cost:  0.4242304\n",
            "epoch :  979  -  cost:  0.4241775\n",
            "epoch :  980  -  cost:  0.42412466\n",
            "epoch :  981  -  cost:  0.42407185\n",
            "epoch :  982  -  cost:  0.4240192\n",
            "epoch :  983  -  cost:  0.42396653\n",
            "epoch :  984  -  cost:  0.423914\n",
            "epoch :  985  -  cost:  0.42386147\n",
            "epoch :  986  -  cost:  0.423809\n",
            "epoch :  987  -  cost:  0.42375663\n",
            "epoch :  988  -  cost:  0.4237043\n",
            "epoch :  989  -  cost:  0.423652\n",
            "epoch :  990  -  cost:  0.4235999\n",
            "epoch :  991  -  cost:  0.42354774\n",
            "epoch :  992  -  cost:  0.42349568\n",
            "epoch :  993  -  cost:  0.42344365\n",
            "epoch :  994  -  cost:  0.42339173\n",
            "epoch :  995  -  cost:  0.42333984\n",
            "epoch :  996  -  cost:  0.42328802\n",
            "epoch :  997  -  cost:  0.42323622\n",
            "epoch :  998  -  cost:  0.4231845\n",
            "epoch :  999  -  cost:  0.42313287\n",
            "Accuracy:0.8333333134651184\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrE0ZPiW3KRH",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnlS3tss3Mhu",
        "colab_type": "text"
      },
      "source": [
        "In this blog on Perceptron Learning Algorithm, you learned what is a perceptron and how to implement it using TensorFlow library. You also understood how a perceptron can be used as a linear classifier and I demonstrated how to we can use this fact to implement AND Gate using a perceptron. At last, I took a one step ahead and applied perceptron to solve a real time use case where I classified SONAR data set to detect the difference between Rock and Mine. Now, in the next blog I will talk about limitations of a single layer perceptron and how you can form a multi-layer perceptron or a neural network to deal with more complex problems. There, you will also learn about how to build a multi-layer neural network using TensorFlow from scratch."
      ]
    }
  ]
}