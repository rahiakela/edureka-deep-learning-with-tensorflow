{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "module-3-perceptron-learning-algorithm.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPR1vKvRuLm8aZ7OUneb91N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/edureka-deep-learning-with-tensorflow/blob/module-3-deep-dive-into-neural-networks-with-tensorFlow/module_3_perceptron_learning_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X87MA0ZfoAA",
        "colab_type": "text"
      },
      "source": [
        "# Perceptron Learning Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV0J4p1wfo39",
        "colab_type": "text"
      },
      "source": [
        "As you know a perceptron serves as a basic building block for creating a deep neural network therefore, it is quite obvious that we should begin our journey of mastering Deep Learning with perceptron and learn how to implement it using TensorFlow to solve different problems. \n",
        "\n",
        "Following are the topics that will be covered in this blog on Perceptron Learning Algorithm:\n",
        "\n",
        "* Perceptron as a Linear Classifier\n",
        "* Implementation of a Perceptron using TensorFlow Library\n",
        "* SONAR Data Classification Using a Single Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsWX2LjigNcN",
        "colab_type": "text"
      },
      "source": [
        "## Types of Classification Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCK8qKpqgOcn",
        "colab_type": "text"
      },
      "source": [
        "One can categorize all kinds of classification problems that can be solved using neural networks into two broad categories:\n",
        "* Linearly Separable Problems\n",
        "* Non-Linearly Separable Problems\n",
        "\n",
        "Basically, a problem is said to be linearly separable if you can classify the data set into two categories or classes using a single line. For example, separating cats from a group of cats and dogs. On the contrary, in case of a non-linearly separable problems, the data set contains multiple classes and requires non-linear line for separating them into their respective classes. For example, classification of handwritten digits. Let us visualize the difference between the two by plotting the graph of a linearly separable problem and non-linearly problem data set:\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/07/Linear-528x264.jpg?raw=1' width='800'/>\n",
        "\n",
        "Since, you all are familiar with AND Gates, I will be using it as an example to explain how a perceptron works as a linear classifier.\n",
        "\n",
        "**Note**: As you move onto much more complex problems such as Image Recognition, which I covered briefly in the previous blog, the relationship in the data that you want to capture becomes highly non-linear and therefore, requires a network which consists of multiple artificial neurons, called as artificial neural network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l38AVIwCgt7O",
        "colab_type": "text"
      },
      "source": [
        "## Perceptron as AND Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw_i1kKjgxfj",
        "colab_type": "text"
      },
      "source": [
        "As you know that AND gate produces an output as 1 if both the inputs are 1 and 0 in all other cases. Therefore, a perceptron can be used as a separator or a decision line that divides the input set of AND Gate, into two classes:\n",
        "\n",
        "* **Class 1**: Inputs having output as 0 that lies below the decision line.\n",
        "* **Class 2**: Inputs having output as 1 that lies above the decision line or separator. \n",
        "\n",
        "The below diagram shows the above idea of classifying the inputs of AND Gate using a perceptron:\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/07/AND-Gate-Classifier-Deep-Learning-Tutorial-Edureka-528x194.png?raw=1' width='800'/>\n",
        "\n",
        "Till now, you understood that a linear perceptron can be used to classify the input data set into two classes. But, how does it actually classify the data? \n",
        "\n",
        "Mathematically, one can represent a perceptron as a function of weights, inputs and bias (vertical offset):\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/06/Transfer-Function-Deep-Learning-Tutorial-Edureka-300x152.png?raw=1' width='800'/>\n",
        "\n",
        "* Each of the input received by the perceptron has been weighted based on the amount of its contribution for obtaining the final output. \n",
        "* Bias allows us to shift the decision line so that it can best separate the inputs into two classes.\n",
        "\n",
        "Enough of the theory, let us look at the first example of this blog on Perceptron Learning Algorithm where I will implement AND Gate using a perceptron from scratch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e47qA-LzhaQ0",
        "colab_type": "text"
      },
      "source": [
        "## Perceptron Learning Algorithm: Implementation of AND Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2NY31fRhbuV",
        "colab_type": "text"
      },
      "source": [
        "### 1. Import all the required library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdZ1hlxIhw9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "f5f1b074-231e-42e3-ab97-a9c7f4f7de91"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p9bgabRh1Ez",
        "colab_type": "text"
      },
      "source": [
        "### Define Vector Variables for Input and Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OraQuPBh4id",
        "colab_type": "text"
      },
      "source": [
        "Now, I will create variables for storing the input, output and bias for my perceptron:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beBxtabgh7BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input1, input2 and bias\n",
        "train_in = [\n",
        "   [1., 1., 1],\n",
        "   [1., 0, 1],\n",
        "   [0, 1., 1],\n",
        "   [0, 0, 1]         \n",
        "]\n",
        "\n",
        "# target\n",
        "train_out = [[1.], [0], [0], [0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiYRxW21iXNx",
        "colab_type": "text"
      },
      "source": [
        "### 3. Define Weight Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HT8JTLbiYIz",
        "colab_type": "text"
      },
      "source": [
        "Now, I need to define the weight variable and assign some random values to it initially. Since, I have three inputs over here (input 1, input 2 & bias), I will require 3 weight values for each input. So, I will define a tensor variable of shape 3×1 for our weights that will be initialized with random values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLrI59cwiWS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weight variable initialized with random values using random_normal()\n",
        "w = tf.Variable(tf.random_normal([3, 1], seed=12))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihBZ3xhgivUs",
        "colab_type": "text"
      },
      "source": [
        "### 4. Define placeholders for Input and Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euygNDrwiw8f",
        "colab_type": "text"
      },
      "source": [
        "In TensorFlow, you can specify placeholders that can accept external inputs on the runtime. So, I will define two placeholders –  x for input and y for output. Later on, you will understand how to feed inputs to a placeholder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2dkHF0Nisg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Placeholder for input and output\n",
        "x = tf.placeholder(tf.float32, [None, 3])\n",
        "y = tf.placeholder(tf.float32, [None, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CRIPl6zjGwr",
        "colab_type": "text"
      },
      "source": [
        "### 5. Calculate Output and Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ekemDTjKnC",
        "colab_type": "text"
      },
      "source": [
        "As discussed earlier, the input received by a perceptron is first multiplied by the respective weights and then, all these weighted inputs are summed together. This summed value is then fed to activation for obtaining the final result as shown in the image below followed by the the code:\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/07/AND-Gate-Perceptron-Perceptron-Learning-Algorithm-Edureka-528x207.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42FvadTqje1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate output\n",
        "output = tf.nn.relu(tf.matmul(x, w))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWmTCpX2jqgc",
        "colab_type": "text"
      },
      "source": [
        "### 6. Calculate the Cost or Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqXyxiCSjrf4",
        "colab_type": "text"
      },
      "source": [
        "Now, I need to calculate the error value w.r.t perceptron output and the desired output. Generally, this error is calculated as Mean Squared Error which is nothing but the square of difference of perceptron output and desired output as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KtUfM24joYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mean Squared Loss or Error\n",
        "loss = tf.reduce_sum(tf.square(output - y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDY6ep0Aj7At",
        "colab_type": "text"
      },
      "source": [
        "### 7. Minimize Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk8x1Qqcj-qH",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow provides optimizers that slowly change each variable (weight and bias) in order to minimize the loss in successive iterations. The simplest optimizer is gradient descent which I will be using in this case. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpqcJPVVkEHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Minimize loss using GradientDescentOptimizer with a learning rate of 0.01\n",
        "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
        "train = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5Q1mJlokUV3",
        "colab_type": "text"
      },
      "source": [
        "### 8. Initialize all the variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya66I47IkVRd",
        "colab_type": "text"
      },
      "source": [
        "Variables are not initialized when you call tf.Variable. So, I need to explicitly initialize all the variables in a TensorFlow program using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ0VBZCmkRo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize all the global variables\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eomg17bdknyW",
        "colab_type": "text"
      },
      "source": [
        "### 9. Training Perceptron in Iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvC7MemGkoxW",
        "colab_type": "text"
      },
      "source": [
        "Now, I need to train our perceptron i.e. update values of weights and bias in successive iteration to minimize the error or loss. Here, I will train our perceptron in 1000 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBiykzKDklOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "4abcd348-5d0c-4ff4-a0fb-10d67f3aaf11"
      },
      "source": [
        "# Compute output and cost w.r.t to input vector\n",
        "for i in range(10):\n",
        "  sess.run(train, {x: train_in, y: train_out})\n",
        "  cost = sess.run(loss, feed_dict={x: train_in, y: train_out})\n",
        "  print(f'Epoch-- {str(i)} -- loss -- {str(cost)}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch-- 0 -- loss -- 1.0213106\n",
            "Epoch-- 1 -- loss -- 1.0033305\n",
            "Epoch-- 2 -- loss -- 0.9856898\n",
            "Epoch-- 3 -- loss -- 0.96837854\n",
            "Epoch-- 4 -- loss -- 0.95138687\n",
            "Epoch-- 5 -- loss -- 0.93470633\n",
            "Epoch-- 6 -- loss -- 0.9183289\n",
            "Epoch-- 7 -- loss -- 0.90224737\n",
            "Epoch-- 8 -- loss -- 0.8864547\n",
            "Epoch-- 9 -- loss -- 0.8709445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C3b5uEylSbn",
        "colab_type": "text"
      },
      "source": [
        "In  the above code, you can observe how I am feeding train_in (input set of AND Gate) and train_out (output set of AND gate) to placeholders x and y respectively using feed_dict for calculating the cost or error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooz07MrGl2Ko",
        "colab_type": "text"
      },
      "source": [
        "## Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he9bvf-el3w8",
        "colab_type": "text"
      },
      "source": [
        "As discussed earlier, the activation function is applied to the output of a perceptron as shown in the image below:\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/06/Activation-Function-Deep-Learning-Tutorial-Edureka-300x121.png?raw=1' width='800'/>\n",
        "\n",
        "In the previous example, I have shown you how to use a linear perceptron with relu activation function for performing linear classification on the input set of AND Gate. But, what if the classification that you wish to perform is non-linear in nature. In that case, you will be using one of the non-linear activation functions. Some of the prominent non-linear activation functions have been shown below:\n",
        "\n",
        "<img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2017/06/Activation-Functions-Deep-Learning-Tutorial-Edureka-528x177.png?raw=1' width='800'/>\n",
        "\n",
        "TensorFlow library provides built-in functions for applying activation functions. The built-in functions w.r.t. above stated activation functions are listed below:\n",
        "\n",
        "* **tf.sigmoid(x, name=None)**\n",
        "  * Computes sigmoid of x element-wise\n",
        "  * For an element x, sigmoid is calculated as –  y = 1 / (1 + exp(-x))\n",
        "* **tf.nn.relu(features, name=None)**\n",
        "  * Computes rectified linear as – max(features, 0)\n",
        "* **tf.tanh(x, name=None)**\n",
        "  * Computes hyperbolic tangent of x element wise\n",
        "\n",
        "So far, you have learned how a perceptron works and how you can program it using TensorFlow. So, it’s time to move ahead and apply our understanding of a perceptron to solve an interesting use case on SONAR Data Classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5F-B1XMmqmq",
        "colab_type": "text"
      },
      "source": [
        "## SONAR Data Classification Using Single Layer Perceptrons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b60csAxmuQc",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}